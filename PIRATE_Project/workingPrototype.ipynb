{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import here\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "from getDocument import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer, BertModel\n",
    "\n",
    "model = BertModel.from_pretrained(\"allenai/scibert_scivocab_uncased\", is_decoder=True) #add_cross_attention\n",
    "tokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased', padding=True, truncation=True) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is an example of running Torch.nn.CosineSimilarity() on single sentences. It works somewhat as text1 and text2 have a higher similarity than 1/3 and 2/3.  Torch.nn.CosineSimilarity(dim=2) returns a vector of size [1, 14] which I then sum to get the similarity *THIS MAY NOT BE THE CORRECT WAY TO DO IT* but right now I don't have any better guidance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test of sentences to gather understanding of what cosine similarity is\n",
    "# https://pytorch.org/docs/stable/generated/torch.nn.CosineSimilarity.html\n",
    "\n",
    "text1 = \"No one was allowed to interfere with the work.\"\n",
    "text2 = \"The job was done with no interaction.\"\n",
    "text3 = \"Walking in the park is fun when it's warm.\"\n",
    "\n",
    "MAX_LENGTH = 18\n",
    "\n",
    "# Need to do the padding buisness because nn.CosineSimilarity requires tensors of equal length\n",
    "encoded_input1 = tokenizer(text1, return_tensors='pt', max_length=MAX_LENGTH, padding='max_length')\n",
    "encoded_input2 = tokenizer(text2, return_tensors='pt', max_length=MAX_LENGTH, padding='max_length')\n",
    "encoded_input3 = tokenizer(text3, return_tensors='pt', max_length=MAX_LENGTH, padding='max_length')\n",
    "\n",
    "output1 = model(**encoded_input1, return_dict=False)\n",
    "output2 = model(**encoded_input2, return_dict=False)\n",
    "output3 = model(**encoded_input3, return_dict=False)\n",
    "\n",
    "print(f\"encoded_input1: {encoded_input1['input_ids']}\")\n",
    "print(f\"encoded_input2: {encoded_input2['input_ids']}\")\n",
    "print(f\"encoded_input3: {encoded_input3['input_ids']}\")\n",
    "print(f\"decode of encoded_input1: {tokenizer.decode(encoded_input1['input_ids'][0])}\")\n",
    "print(f\"decode of encoded_input2: {tokenizer.decode(encoded_input2['input_ids'][0])}\")\n",
    "print(f\"decode of encoded_inpu32: {tokenizer.decode(encoded_input3['input_ids'][0])}\")\n",
    "\n",
    "# print(output1[0].shape)\n",
    "# print(output2[0].shape)\n",
    "# print(output3[0].shape)\n",
    "\n",
    "# cosine_scores = util.cos_sim(encoded_input1[\"input_ids\"], encoded_input2[\"input_ids\"])\n",
    "cosine_scores_DIM2 = torch.nn.CosineSimilarity(dim=2)\n",
    "# cosine_scores2 = torch.nn.CosineSimilarity(dim=1)\n",
    "# cosine_scores3 = torch.nn.CosineSimilarity(dim=2)\n",
    "\n",
    "score1 = cosine_scores_DIM2(output1[0], output2[0])\n",
    "score2 = cosine_scores_DIM2(output1[0], output3[0])\n",
    "score3 = cosine_scores_DIM2(output2[0], output3[0])\n",
    "# score4 = cosine_scores4(output1[0], output2[0])\n",
    "\n",
    "# print(f\"score1.shape: {score1.shape}\")\n",
    "# print(f\"score2.shape: {score2.shape}\")\n",
    "# print(f\"score3.shape: {score3.shape}\")\n",
    "\n",
    "# print(f\"sum of score3: {score3[0].sum().item()} and score3[0].size()[0]: {score3[0].size()[0]}\")\n",
    "print(f\"Sentence 1 & 2 cosine similarity score: {score1.mean().item()}\")\n",
    "print(f\"Sentence 1 & 3 cosine similarity score: {score2.mean().item()}\")\n",
    "print(f\"Sentence 2 & 3 cosine similarity score: {score3.mean().item()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With max_length=14 the scores are\n",
    "1/2 -> 0.582\n",
    "1/3 -> 0.475\n",
    "2/3 -> 0.490\n",
    "\n",
    "With max_length = 18 the scores are\n",
    "1/2 -> 0.656\n",
    "1/3 -> 0.573\n",
    "2/3 -> 0.588\n",
    "\n",
    "seeming to suggest that unnecessary padding will hurt the accuracy (as expected)\n",
    "_____________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph1 = \"Pre-trained text encoders have rapidly advanced the state of the art on many NLPtasks. We focus on one such model, BERT, and aim to quantify where linguistic information is captured within the network. We find that the model represents the steps of the traditional NLP pipeline in an interpretable and localizable way, and that the regions responsible for each step appear in the expected sequence: POS tagging, parsing, NER, semantic roles, then coreference. Qualitative analysis reveals that the model can and often does adjust this pipeline dynamically, revising lower-level decisions on the basis of disambiguating information from higher-level representations.\"\n",
    "paragraph2 = \"We present a set of experiments to demonstrate that deep recurrent neural networks (RNNs) learn internal representations that capture soft hierarchical notions of syntax from highly varied supervision. We consider four syntax tasks at different depths of the parse tree; for each word, we predict its part of speech as well as the first (parent), second (grandparent) and third level (great-grandparent) constituent labels that appear above it. These predictions are made from representations produced at different depths in networks that are pretrained with one of four objectives: dependency parsing, semantic role labeling, machine translation, or language modeling. In every case, we find a correspondence between network depth and syntactic depth, suggesting that a soft syntactic hierarchy emerges. This effect is robust across all conditions, indicating that the models encode significant amounts of syntax even in the absence of an explicit syntactic training supervision.\"\n",
    "paragraph3 = \"Language  technology  has  become  pervasive  in  everyday  life. Neural  networks  are  a key component  in  this technology  thanks  to  their ability  to model  large  amounts  of data.  Contrary  to traditional  systems,  models  based  on  deep  neural  networks  (a.k.a. deep  learning) can  be  trained  in  an  end-to-end  fashion  on  input-output  pairs,  such  as  a  sentence  in  one language  and  its translation  in  another  language,  or  a  speech  utterance  and  its  transcription.  The end-to-end  training  paradigm simplifies  the  engineering process  while  giving  the model  flexibility  to  optimize  for  the  desired  task. This,  however,  often  comes  at  the  expense  of model  interpretability:  understanding  the role  of different parts  of the  deep  neural network  is  difficult,  and  such  models  are  sometimes  perceived  as  'black-box',  hindering research  efforts  and  limiting their  utility to  society.\"\n",
    "\n",
    "MAX_LENGTH = 175\n",
    "\n",
    "# Need to do the padding buisness because nn.CosineSimilarity requires tensors of equal length\n",
    "encoded_long_input1 = tokenizer(paragraph1, return_tensors='pt', max_length=MAX_LENGTH, padding='max_length')\n",
    "encoded_long_input2 = tokenizer(paragraph2, return_tensors='pt', max_length=MAX_LENGTH, padding='max_length')\n",
    "encoded_long_input3 = tokenizer(paragraph3, return_tensors='pt', max_length=MAX_LENGTH, padding='max_length')\n",
    "\n",
    "output_long_1 = model(**encoded_long_input1, return_dict=False)\n",
    "output_long_2 = model(**encoded_long_input2, return_dict=False)\n",
    "output_long_3 = model(**encoded_long_input3, return_dict=False)\n",
    "\n",
    "# print(f\"encoded_input1: {encoded_long_input1['input_ids']}\")\n",
    "# print(f\"encoded_input2: {encoded_long_input2['input_ids']}\")\n",
    "# print(f\"encoded_input3: {encoded_long_input3['input_ids']}\")\n",
    "print(f\"decode of encoded_input1: {tokenizer.decode(encoded_long_input1['input_ids'][0])}\")\n",
    "print(f\"decode of encoded_input2: {tokenizer.decode(encoded_long_input2['input_ids'][0])}\")\n",
    "print(f\"decode of encoded_inpu32: {tokenizer.decode(encoded_long_input3['input_ids'][0])}\")\n",
    "\n",
    "print(output_long_1[0].shape)\n",
    "print(output_long_2[0].shape)\n",
    "print(output_long_3[0].shape)\n",
    "\n",
    "cosine_scores_DIM2 = torch.nn.CosineSimilarity(dim=0)\n",
    "\n",
    "score_long_1 = cosine_scores_DIM2(output_long_1[0], output_long_2[0])\n",
    "score_long_2 = cosine_scores_DIM2(output_long_1[0], output_long_3[0])\n",
    "score_long_3 = cosine_scores_DIM2(output_long_2[0], output_long_3[0])\n",
    "\n",
    "print(f\"Sentence 1 & 2 cosine similarity score: {score_long_1.mean().item()}\")\n",
    "print(f\"Sentence 1 & 3 cosine similarity score: {score_long_2.mean().item()}\")\n",
    "print(f\"Sentence 2 & 3 cosine similarity score: {score_long_3.mean().item()}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text came from the BERT&ClassicNLP folder with 1: main paper, 2: source1, 3: source2\n",
    "using their abstracts, source 3 is supposed to be a semi-black sheep source as it's a thesis abstract and not a normal academic paper one\n",
    "\n",
    "With max_length=175 & dim=2:\n",
    "Sentence 1 & 2 cosine similarity score: 0.296\n",
    "Sentence 1 & 3 cosine similarity score: 0.364\n",
    "Sentence 2 & 3 cosine similarity score: 0.319\n",
    "\n",
    "With max_length=175 & dim=1:\n",
    "Sentence 1 & 2 cosine similarity score: 0.224\n",
    "Sentence 1 & 3 cosine similarity score: 0.248\n",
    "Sentence 2 & 3 cosine similarity score: 0.237\n",
    "\n",
    "With max_length=175 & dim=0:\n",
    "Sentence 1 & 2 cosine similarity score: 0.191\n",
    "Sentence 1 & 3 cosine similarity score: 0.219\n",
    "Sentence 2 & 3 cosine similarity score: 0.201\n",
    "\n",
    "______________________________________________________________________________________________________________________"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the tuple of SPECTER embeddings of a main paper and all of its references\n",
    "\n",
    "Both indices of the tuple are lists, 0 is embeddings and 1 is references\n",
    "\n",
    "The 0 index of each list is of the main paper\n",
    "\n",
    "The each embedding is just a list of numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tupleOfResults = get_paper_and_references_embedding_and_titles_from_query(\"BERT Rediscovers the Classical NLP Pipeline\")\n",
    "\n",
    "embeddings = tupleOfResults[0]\n",
    "titles = tupleOfResults[1]\n",
    "\n",
    "print(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\zebzi\\Documents\\School\\Senior_Year\\CSCI_5541\\FinalProject\\workingPrototype.ipynb Cell 10\u001b[0m in \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/zebzi/Documents/School/Senior_Year/CSCI_5541/FinalProject/workingPrototype.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Test the Cosine Similarity by first creating tensors and comparing the results\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/zebzi/Documents/School/Senior_Year/CSCI_5541/FinalProject/workingPrototype.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m newCosineSimilarity \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mCosineSimilarity(dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/zebzi/Documents/School/Senior_Year/CSCI_5541/FinalProject/workingPrototype.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m mainPaper \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(embeddings[\u001b[39m0\u001b[39m])\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/zebzi/Documents/School/Senior_Year/CSCI_5541/FinalProject/workingPrototype.ipynb#X12sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMain paper title: \u001b[39m\u001b[39m{\u001b[39;00mtitles[\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/zebzi/Documents/School/Senior_Year/CSCI_5541/FinalProject/workingPrototype.ipynb#X12sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39mlen\u001b[39m(embeddings)):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'embeddings' is not defined"
     ]
    }
   ],
   "source": [
    "# Test the Cosine Similarity by first creating tensors and comparing the results\n",
    "newCosineSimilarity = torch.nn.CosineSimilarity(dim=0)\n",
    "mainPaper = torch.tensor(embeddings[0])\n",
    "print(f\"Main paper title: {titles[0]}\")\n",
    "for i in range(1, len(embeddings)):\n",
    "    referencePaper = torch.tensor(embeddings[i])\n",
    "    print(f\"Main Paper with embeddings[{i}]: {newCosineSimilarity(mainPaper, referencePaper):.3f} and the reference paper: {titles[i]}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With dim=0: (Which is the only allowed cosine similarity )\n",
    "Main Paper with embeddings[1]: 0.7507858276367188\n",
    "Main Paper with embeddings[2]: 0.7007443308830261\n",
    "Main Paper with embeddings[3]: 0.734921395778656\n",
    "Main Paper with embeddings[4]: 0.7470535039901733\n",
    "Main Paper with embeddings[5]: 0.7582955956459045\n",
    "Main Paper with embeddings[6]: 0.6695845127105713\n",
    "Main Paper with embeddings[7]: 0.7345977425575256\n",
    "Main Paper with embeddings[8]: 0.6843916773796082\n",
    "Main Paper with embeddings[9]: 0.7346400618553162\n",
    "Main Paper with embeddings[10]: 0.7553737759590149\n",
    "Main Paper with embeddings[11]: 0.6457442045211792\n",
    "Main Paper with embeddings[12]: 0.7245481610298157\n",
    "Main Paper with embeddings[13]: 0.6589515209197998\n",
    "Main Paper with embeddings[14]: 0.6954355835914612\n",
    "Main Paper with embeddings[15]: 0.6530366539955139\n",
    "Main Paper with embeddings[16]: 0.5189672708511353\n",
    "Main Paper with embeddings[17]: 0.6884840726852417\n",
    "Main Paper with embeddings[18]: 0.616716206073761\n",
    "Main Paper with embeddings[19]: 0.644980251789093\n",
    "Main Paper with embeddings[20]: 0.6220344305038452"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________________________________________________________________________________________________________________________________________\n",
    "Create a reference paper corpus and then do cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "defaultPaper = get_paper_and_references_embedding_and_titles_from_query(\"BERT Rediscovers the Classical NLP Pipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary for the purpose of not repreating titles\n",
    "title_embedding_dict = {}\n",
    "\n",
    "for embedding, title in zip(defaultPaper[0], defaultPaper[1]):\n",
    "    subPaper = get_paper_and_references_embedding_and_titles_from_query(title)\n",
    "    # First depth, all of the main paper's references\n",
    "    if title not in title_embedding_dict:\n",
    "        title_embedding_dict[title] = embedding\n",
    "\n",
    "    # Second depth, all of the referenced paper's references\n",
    "    for sub_embedding, sub_title in zip(subPaper[0], subPaper[1]):\n",
    "        if sub_title not in title_embedding_dict:\n",
    "            title_embedding_dict[sub_title] = sub_embedding\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to return just the first value of the list to be used in list.sort()\n",
    "def getKey(listOfCosAndTitle):\n",
    "    return listOfCosAndTitle[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "836\n"
     ]
    }
   ],
   "source": [
    "print(len(title_embedding_dict))\n",
    "# Redefine cos similarity here for ease of use\n",
    "# also redefine 'main' paper\n",
    "newCosineSimilarity = torch.nn.CosineSimilarity(dim=0)\n",
    "mainPaperEmbedding = torch.tensor(defaultPaper[0][0])\n",
    "\n",
    "similarityScoresWithTitle = []\n",
    "# Get the cos similarity between the main paper and all of the subpapers\n",
    "for key in title_embedding_dict.keys():\n",
    "    compared_paper = torch.tensor(title_embedding_dict[key])\n",
    "    similarityScoresWithTitle.append([newCosineSimilarity(mainPaperEmbedding, compared_paper), key])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cos similarity score: 1.000 and it's \u001b[32mcited in     \u001b[0mthe main paper with title: BERT Rediscovers the Classical NLP Pipeline\n",
      "Cos similarity score: 0.800 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Semi-supervised sequence tagging with bidirectional language models\n",
      "Cos similarity score: 0.790 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: End-to-end Neural Coreference Resolution\n",
      "Cos similarity score: 0.782 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Inference is Everything: Recasting Semantic Resources into a Unified Evaluation Framework\n",
      "Cos similarity score: 0.781 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: LSTM CCG Parsing    \n",
      "Cos similarity score: 0.781 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Constituency Parsing with a Self-Attentive Encoder\n",
      "Cos similarity score: 0.778 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond\n",
      "Cos similarity score: 0.778 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: A Joint Many-Task Model: Growing a Neural Network for Multiple NLP Tasks\n",
      "Cos similarity score: 0.776 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: context2vec: Learning Generic Context Embedding with Bidirectional LSTM\n",
      "Cos similarity score: 0.773 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Tailoring Continuous Word Representations for Dependency Parsing\n",
      "Cos similarity score: 0.773 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Universal Language Model Fine-tuning for Text Classification\n",
      "Cos similarity score: 0.772 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Bottom-Up Abstractive Summarization\n",
      "Cos similarity score: 0.771 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Learning Global Features for Coreference Resolution\n",
      "Cos similarity score: 0.769 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Deep Semantic Role Labeling: What Works and What’s Next\n",
      "Cos similarity score: 0.765 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Semantic Parsing via Paraphrasing\n",
      "Cos similarity score: 0.758 and it's \u001b[32mcited in     \u001b[0mthe main paper with title: Dissecting Contextual Word Embeddings: Architecture and Representation\n",
      "Cos similarity score: 0.755 and it's \u001b[32mcited in     \u001b[0mthe main paper with title: Deep Contextualized Word Representations\n",
      "Cos similarity score: 0.752 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Neural Sequence Learning Models for Word Sense Disambiguation\n",
      "Cos similarity score: 0.752 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Learning Distributed Representations of Sentences from Unlabelled Data\n",
      "Cos similarity score: 0.752 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Semantic Tagging with Deep Residual Networks\n",
      "Cos similarity score: 0.751 and it's \u001b[32mcited in     \u001b[0mthe main paper with title: What do you learn from context? Probing for sentence structure in contextualized word representations\n",
      "Cos similarity score: 0.750 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Deep Semantic Role Labeling with Self-Attention\n",
      "Cos similarity score: 0.749 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: AllenNLP: A Deep Semantic Natural Language Processing Platform\n",
      "Cos similarity score: 0.748 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: On the Evaluation of Semantic Phenomena in Neural Machine Translation Using Natural Language Inference\n",
      "Cos similarity score: 0.748 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Character-Level Question Answering with Attention\n",
      "Cos similarity score: 0.747 and it's \u001b[32mcited in     \u001b[0mthe main paper with title: Language Models are Unsupervised Multitask Learners\n",
      "Cos similarity score: 0.746 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: A large annotated corpus for learning natural language inference\n",
      "Cos similarity score: 0.745 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Embeddings for Word Sense Disambiguation: An Evaluation Study\n",
      "Cos similarity score: 0.744 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: A Neural Network for Factoid Question Answering over Paragraphs\n",
      "Cos similarity score: 0.740 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Jointly Predicting Predicates and Arguments in Neural Semantic Role Labeling\n",
      "Cos similarity score: 0.740 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Story Cloze Task: UW NLP System\n",
      "Cos similarity score: 0.739 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Exploring the Syntactic Abilities of RNNs with Multi-task Learning\n",
      "Cos similarity score: 0.739 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Skip-Thought Vectors\n",
      "Cos similarity score: 0.739 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding\n",
      "Cos similarity score: 0.738 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Bidirectional Attention Flow for Machine Comprehension\n",
      "Cos similarity score: 0.738 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Frame-Semantic Parsing with Softmax-Margin Segmental RNNs and a Syntactic Scaffold\n",
      "Cos similarity score: 0.738 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Efficient Contextualized Representation: Language Model Pruning for Sequence Labeling\n",
      "Cos similarity score: 0.737 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Supervised Learning of Universal Sentence Representations from Natural Language Inference Data\n",
      "Cos similarity score: 0.737 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: The LAMBADA dataset: Word prediction requiring a broad discourse context\n",
      "Cos similarity score: 0.736 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: SemEval-2007 Task 04: Classification of Semantic Relations between Nominals\n",
      "Cos similarity score: 0.735 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: LSTMs Can Learn Syntax-Sensitive Dependencies Well, But Modeling Structure Makes Them Better\n",
      "Cos similarity score: 0.735 and it's \u001b[32mcited in     \u001b[0mthe main paper with title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\n",
      "Cos similarity score: 0.735 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Learned in Translation: Contextualized Word Vectors\n",
      "Cos similarity score: 0.735 and it's \u001b[32mcited in     \u001b[0mthe main paper with title: Collecting Diverse Natural Language Inference Problems for Sentence Representation Evaluation\n",
      "Cos similarity score: 0.735 and it's \u001b[32mcited in     \u001b[0mthe main paper with title: Deep RNNs Encode Soft Hierarchical Syntax\n",
      "Cos similarity score: 0.735 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Extending a Parser to Distant Domains Using a Few Dozen Partially Annotated Examples\n",
      "Cos similarity score: 0.733 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Recognizing textual entailment: Rational, evaluation and approaches\n",
      "Cos similarity score: 0.733 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Neural-Davidsonian Semantic Proto-role Labeling\n",
      "Cos similarity score: 0.732 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF\n",
      "Cos similarity score: 0.731 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Deep Biaffine Attention for Neural Dependency Parsing\n",
      "Cos similarity score: 0.731 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Entity Tracking Improves Cloze-style Reading Comprehension\n",
      "Cos similarity score: 0.730 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: What Syntax Can Contribute in the Entailment Task\n",
      "Cos similarity score: 0.730 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Fast and Accurate Shift-Reduce Constituent Parsing\n",
      "Cos similarity score: 0.730 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Get To The Point: Summarization with Pointer-Generator Networks\n",
      "Cos similarity score: 0.729 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Neural Machine Translation with Source-Side Latent Graph Parsing\n",
      "Cos similarity score: 0.729 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Multilingual Language Processing From Bytes\n",
      "Cos similarity score: 0.728 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Natural Language Processing (Almost) from Scratch\n",
      "Cos similarity score: 0.728 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Word Sense Disambiguation: A Unified Evaluation Framework and Empirical Comparison\n",
      "Cos similarity score: 0.727 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Word Representations: A Simple and General Method for Semi-Supervised Learning\n",
      "Cos similarity score: 0.727 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Assessing the Ability of LSTMs to Learn Syntax-Sensitive Dependencies\n",
      "Cos similarity score: 0.727 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Hypothesis Only Baselines in Natural Language Inference\n",
      "Cos similarity score: 0.727 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Language Modeling Teaches You More than Translation Does: Lessons Learned Through Auxiliary Syntactic Task Analysis\n",
      "Cos similarity score: 0.727 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection\n",
      "Cos similarity score: 0.727 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: A Deep Reinforced Model for Abstractive Summarization\n",
      "Cos similarity score: 0.726 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Resolving Complex Cases of Definite Pronouns: The Winograd Schema Challenge\n",
      "Cos similarity score: 0.726 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Recursive deep learning for natural language processing and computer vision\n",
      "Cos similarity score: 0.725 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: On the State of the Art of Evaluation in Neural Language Models\n",
      "Cos similarity score: 0.725 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: End-to-end learning of semantic role labeling using recurrent neural networks\n",
      "Cos similarity score: 0.725 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Visualizing and Understanding Neural Models in NLP\n",
      "Cos similarity score: 0.725 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: A Hierarchical Neural Autoencoder for Paragraphs and Documents\n",
      "Cos similarity score: 0.725 and it's \u001b[32mcited in     \u001b[0mthe main paper with title: Improving Language Understanding by Generative Pre-Training\n",
      "Cos similarity score: 0.725 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference\n",
      "Cos similarity score: 0.724 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Feature-Rich Part-of-Speech Tagging with a Cyclic Dependency Network\n",
      "Cos similarity score: 0.724 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: What’s Going On in Neural Constituency Parsers? An Analysis\n",
      "Cos similarity score: 0.724 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Enhanced LSTM for Natural Language Inference\n",
      "Cos similarity score: 0.724 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Towards Robust Linguistic Analysis using OntoNotes\n",
      "Cos similarity score: 0.724 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Pointer Sentinel Mixture Models\n",
      "Cos similarity score: 0.723 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: XNLI: Evaluating Cross-lingual Sentence Representations\n",
      "Cos similarity score: 0.723 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Language Modeling with Gated Convolutional Networks\n",
      "Cos similarity score: 0.722 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: A Simple Method for Commonsense Reasoning\n",
      "Cos similarity score: 0.722 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Stress Test Evaluation for Natural Language Inference\n",
      "Cos similarity score: 0.721 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Higher-Order Coreference Resolution with Coarse-to-Fine Inference\n",
      "Cos similarity score: 0.721 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Syntactic Contributions in the Entailment Task: an implementation\n",
      "Cos similarity score: 0.721 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Investigating the Usefulness of Generalized Word Representations in SMT\n",
      "Cos similarity score: 0.721 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: A Convolutional Neural Network for Modelling Sentences\n",
      "Cos similarity score: 0.721 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Analyzing and Integrating Dependency Parsers\n",
      "Cos similarity score: 0.720 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Generating Entailment Rules from FrameNet\n",
      "Cos similarity score: 0.720 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Unsupervised Pretraining for Sequence to Sequence Learning\n",
      "Cos similarity score: 0.720 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Learning Character-level Representations for Part-of-Speech Tagging\n",
      "Cos similarity score: 0.720 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Evaluating Compositionality in Sentence Embeddings\n",
      "Cos similarity score: 0.720 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: SentEval: An Evaluation Toolkit for Universal Sentence Representations\n",
      "Cos similarity score: 0.719 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Target-side Word Segmentation Strategies for Neural Machine Translation\n",
      "Cos similarity score: 0.717 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Classification of semantic relations between nominals\n",
      "Cos similarity score: 0.717 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: What do Neural Machine Translation Models Learn about Morphology?\n",
      "Cos similarity score: 0.717 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Learning and Evaluating General Linguistic Intelligence\n",
      "Cos similarity score: 0.716 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Handling Homographs in Neural Machine Translation\n",
      "Cos similarity score: 0.714 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: SQuAD: 100,000+ Questions for Machine Comprehension of Text\n",
      "Cos similarity score: 0.714 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Generating Wikipedia by Summarizing Long Sequences\n",
      "Cos similarity score: 0.714 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Relation Alignment for Textual Entailment Recognition\n",
      "Cos similarity score: 0.714 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Neural Attention for Learning to Rank Questions in Community Question Answering\n",
      "Cos similarity score: 0.713 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Wizard of Wikipedia: Knowledge-Powered Conversational agents\n",
      "Cos similarity score: 0.713 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Learning Accurate, Compact, and Interpretable Tree Annotation\n",
      "Cos similarity score: 0.713 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Natural Language Multitasking: Analyzing and Improving Syntactic Saliency of Hidden Representations\n",
      "Cos similarity score: 0.713 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Regularizing and Optimizing LSTM Language Models\n",
      "Cos similarity score: 0.713 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: CoNLL-2012 Shared Task: Modeling Multilingual Unrestricted Coreference in OntoNotes\n",
      "Cos similarity score: 0.713 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: An approach using Named Entities for Recognizing Textual Entailment\n",
      "Cos similarity score: 0.712 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: A Neural Conversational Model\n",
      "Cos similarity score: 0.712 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Character-Level Language Modeling with Deeper Self-Attention\n",
      "Cos similarity score: 0.712 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Efficient Non-parametric Estimation of Multiple Embeddings per Word in Vector Space\n",
      "Cos similarity score: 0.712 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Predicting Target Language CCG Supertags Improves Neural Machine Translation\n",
      "Cos similarity score: 0.711 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Trimming and Improving Skip-thought Vectors\n",
      "Cos similarity score: 0.711 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Easy Victories and Uphill Battles in Coreference Resolution\n",
      "Cos similarity score: 0.711 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Breaking NLI Systems with Sentences that Require Simple Lexical Inferences\n",
      "Cos similarity score: 0.710 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: EuroSense: Automatic Harvesting of Multilingual Sense Annotations from Parallel Text\n",
      "Cos similarity score: 0.709 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Tree-to-Sequence Attentional Neural Machine Translation\n",
      "Cos similarity score: 0.709 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: End-To-End Memory Networks\n",
      "Cos similarity score: 0.708 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Semantic Roles for SMT: A Hybrid Two-Pass Model\n",
      "Cos similarity score: 0.708 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Annotation Artifacts in Natural Language Inference Data\n",
      "Cos similarity score: 0.708 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Maximum entropy models for natural language ambiguity resolution\n",
      "Cos similarity score: 0.707 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: An NLP Curator (or: How I Learned to Stop Worrying and Love NLP Pipelines)\n",
      "Cos similarity score: 0.707 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Dialog-based Language Learning\n",
      "Cos similarity score: 0.706 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Exploring the Limits of Language Modeling\n",
      "Cos similarity score: 0.705 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Scalable Inference and Training of Context-Rich Syntactic Translation Models\n",
      "Cos similarity score: 0.705 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Self-Training PCFG Grammars with Latent Annotations Across Languages\n",
      "Cos similarity score: 0.704 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Deep Reinforcement Learning for Mention-Ranking Coreference Models\n",
      "Cos similarity score: 0.704 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Automatic Acquisition of Hyponyms from Large Text Corpora\n",
      "Cos similarity score: 0.704 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Parser Showdown at the Wall Street Corral: An Empirical Investigation of Error Types in Parser Output\n",
      "Cos similarity score: 0.704 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Rationalizing Neural Predictions\n",
      "Cos similarity score: 0.703 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Multilingual Dependency Analysis with a Two-Stage Discriminative Parser\n",
      "Cos similarity score: 0.703 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Overview of the 2012 Shared Task on Parsing the Web\n",
      "Cos similarity score: 0.701 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Self-Adaptive Hierarchical Sentence Model\n",
      "Cos similarity score: 0.701 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Reading Wikipedia to Answer Open-Domain Questions\n",
      "Cos similarity score: 0.701 and it's \u001b[32mcited in     \u001b[0mthe main paper with title: Multi-Task Deep Neural Networks for Natural Language Understanding\n",
      "Cos similarity score: 0.701 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Large-Scale Syntactic Language Modeling with Treelets\n",
      "Cos similarity score: 0.700 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Advances in Pre-Training Distributed Word Representations\n",
      "Cos similarity score: 0.700 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Don’t count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors\n",
      "Cos similarity score: 0.700 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank\n",
      "Cos similarity score: 0.699 and it's \u001b[32mcited in     \u001b[0mthe main paper with title: Semantic Proto-Role Labeling\n",
      "Cos similarity score: 0.699 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Visualizing and Understanding Neural Machine Translation\n",
      "Cos similarity score: 0.699 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: How Robust Are Character-Based Word Embeddings in Tagging and MT Against Wrod Scramlbing or Randdm Nouse?\n",
      "Cos similarity score: 0.698 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Zero-Shot Cross-lingual Classification Using Multilingual Neural Machine Translation\n",
      "Cos similarity score: 0.698 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: A Compositional and Interpretable Semantic Space\n",
      "Cos similarity score: 0.698 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Improving SMT quality with morpho-syntactic analysis\n",
      "Cos similarity score: 0.698 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: FrameNet+: Fast Paraphrastic Tripling of FrameNet\n",
      "Cos similarity score: 0.698 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Grammar as a Foreign Language\n",
      "Cos similarity score: 0.698 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Semi-supervised Sequence Learning\n",
      "Cos similarity score: 0.697 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: On the Evaluation of Common-Sense Reasoning in Natural Language Understanding\n",
      "Cos similarity score: 0.697 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: The Proposition Bank: An Annotated Corpus of Semantic Roles\n",
      "Cos similarity score: 0.696 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Sharp Nearby, Fuzzy Far Away: How Neural Language Models Use Context\n",
      "Cos similarity score: 0.696 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Multilingual Models for Compositional Distributed Semantics\n",
      "Cos similarity score: 0.696 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Character-based Neural Machine Translation\n",
      "Cos similarity score: 0.696 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Deterministic Coreference Resolution Based on Entity-Centric, Precision-Ranked Rules\n",
      "Cos similarity score: 0.695 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Extractive Summarization using Continuous Vector Space Models\n",
      "Cos similarity score: 0.695 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: What Do Recurrent Neural Network Grammars Learn About Syntax?\n",
      "Cos similarity score: 0.695 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Fine-grained Analysis of Sentence Embeddings Using Auxiliary Prediction Tasks\n",
      "Cos similarity score: 0.695 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Visualizing and Understanding Recurrent Networks\n",
      "Cos similarity score: 0.695 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Can Semantic Role Labeling Improve SMT?\n",
      "Cos similarity score: 0.695 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: When Are Tree Structures Necessary for Deep Learning of Representations?\n",
      "Cos similarity score: 0.695 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Natural language inference\n",
      "Cos similarity score: 0.694 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Massively Multilingual Word Embeddings\n",
      "Cos similarity score: 0.694 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Ask Me Anything: Dynamic Memory Networks for Natural Language Processing\n",
      "Cos similarity score: 0.693 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond\n",
      "Cos similarity score: 0.693 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Natural Language Inference over Interaction Space\n",
      "Cos similarity score: 0.693 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Event Detection and Factuality Assessment with Non-Expert Supervision\n",
      "Cos similarity score: 0.693 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Phrase-Based & Neural Unsupervised Machine Translation\n",
      "Cos similarity score: 0.693 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Evaluating Discourse Phenomena in Neural Machine Translation\n",
      "Cos similarity score: 0.692 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Word Representation Models for Morphologically Rich Languages in Neural Machine Translation\n",
      "Cos similarity score: 0.691 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: A Simple but Tough-to-Beat Baseline for Sentence Embeddings\n",
      "Cos similarity score: 0.691 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Towards String-To-Tree Neural Machine Translation\n",
      "Cos similarity score: 0.691 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: No Training Required: Exploring Random Encoders for Sentence Classification\n",
      "Cos similarity score: 0.691 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Simple and Effective Multi-Paragraph Reading Comprehension\n",
      "Cos similarity score: 0.690 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Ordinal Common-sense Inference\n",
      "Cos similarity score: 0.690 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Analysis of sentence embedding models using prediction tasks in natural language processing\n",
      "Cos similarity score: 0.690 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: The Prague Dependency Treebank\n",
      "Cos similarity score: 0.689 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Unsupervised Machine Translation Using Monolingual Corpora Only\n",
      "Cos similarity score: 0.689 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Interpreting Neural Networks to Improve Politeness Comprehension\n",
      "Cos similarity score: 0.689 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: A Convolutional Encoder Model for Neural Machine Translation\n",
      "Cos similarity score: 0.689 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Named Entity Recognition with Bidirectional LSTM-CNNs\n",
      "Cos similarity score: 0.689 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Knowledge-Based Weak Supervision for Information Extraction of Overlapping Relations\n",
      "Cos similarity score: 0.688 and it's \u001b[32mcited in     \u001b[0mthe main paper with title: The Stanford CoreNLP Natural Language Processing Toolkit\n",
      "Cos similarity score: 0.688 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: An overview of Natural Language Inference Data Collection: The way forward?\n",
      "Cos similarity score: 0.688 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning\n",
      "Cos similarity score: 0.687 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: A Neural Reordering Model for Phrase-based Translation\n",
      "Cos similarity score: 0.687 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: A Class-Based Agreement Model for Generating Accurately Inflected Translations\n",
      "Cos similarity score: 0.687 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Effective Approaches to Attention-based Neural Machine Translation\n",
      "Cos similarity score: 0.687 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: The Importance of Syntactic Parsing and Inference in Semantic Role Labeling\n",
      "Cos similarity score: 0.687 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Language Modeling Teaches You More than Translation Does : Lessons Learned Through Auxiliary Task Analysis\n",
      "Cos similarity score: 0.686 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Universal Stanford dependencies: A cross-linguistic typology\n",
      "Cos similarity score: 0.686 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: SemEval-2010 Task 8: Multi-Way Classification of Semantic Relations Between Pairs of Nominals\n",
      "Cos similarity score: 0.686 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Learning to Generate Reviews and Discovering Sentiment\n",
      "Cos similarity score: 0.685 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Linguistic Input Features Improve Neural Machine Translation\n",
      "Cos similarity score: 0.685 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Looking for ELMo's friends: Sentence-Level Pretraining Beyond Language Modeling\n",
      "Cos similarity score: 0.685 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Learning Effective and Interpretable Semantic Models using Non-Negative Sparse Embedding\n",
      "Cos similarity score: 0.685 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Parser Evaluation over Local and Non-Local Deep Dependencies in a Large Corpus\n",
      "Cos similarity score: 0.684 and it's \u001b[32mcited in     \u001b[0mthe main paper with title: What you can cram into a single $&!#* vector: Probing sentence embeddings for linguistic properties\n",
      "Cos similarity score: 0.684 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: An Effective Approach to Unsupervised Machine Translation\n",
      "Cos similarity score: 0.684 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: A Model of Coherence Based on Distributed Sentence Representation\n",
      "Cos similarity score: 0.684 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Sequence to Sequence Learning with Neural Networks\n",
      "Cos similarity score: 0.684 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Beyond Word Importance: Contextual Decomposition to Extract Interactions from LSTMs\n",
      "Cos similarity score: 0.684 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Transformer-XL: Attentive Language Models beyond a Fixed-Length Context\n",
      "Cos similarity score: 0.683 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: A Linguistic Evaluation of Rule-Based, Phrase-Based, and Neural MT Engines\n",
      "Cos similarity score: 0.683 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Investigating Language Universal and Specific Properties in Word Embeddings\n",
      "Cos similarity score: 0.683 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: TransferTransfo: A Transfer Learning Approach for Neural Network Based Conversational Agents\n",
      "Cos similarity score: 0.682 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: The Groningen Meaning Bank\n",
      "Cos similarity score: 0.682 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Character-Aware Neural Language Models\n",
      "Cos similarity score: 0.682 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: The Third PASCAL Recognizing Textual Entailment Challenge\n",
      "Cos similarity score: 0.682 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Structured Attention Networks\n",
      "Cos similarity score: 0.682 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Assessing Composition in Sentence Vector Representations\n",
      "Cos similarity score: 0.682 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: VectorSLU: A Continuous Word Vector Approach to Answer Selection in Community Question Answering Systems\n",
      "Cos similarity score: 0.682 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Extracting implicit knowledge from text\n",
      "Cos similarity score: 0.681 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Neural Tree Indexers for Text Understanding\n",
      "Cos similarity score: 0.681 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Gated Self-Matching Networks for Reading Comprehension and Question Answering\n",
      "Cos similarity score: 0.681 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Empirical Methods for Compound Splitting\n",
      "Cos similarity score: 0.681 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Learning Continuous Phrase Representations and Syntactic Parsing with Recursive Neural Networks\n",
      "Cos similarity score: 0.680 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: NLP with BERT: Sentiment Analysis Using SAS® Deep Learning and DLPy\n",
      "Cos similarity score: 0.680 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: A unified architecture for natural language processing: deep neural networks with multitask learning\n",
      "Cos similarity score: 0.680 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Understanding Neural Networks through Representation Erasure\n",
      "Cos similarity score: 0.679 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Dependency Treelet Translation: Syntactically Informed Phrasal SMT\n",
      "Cos similarity score: 0.678 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: A SICK cure for the evaluation of compositional distributional semantic models\n",
      "Cos similarity score: 0.678 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Non-Autoregressive Neural Machine Translation\n",
      "Cos similarity score: 0.678 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Deep multi-task learning with low level tasks supervised at lower layers\n",
      "Cos similarity score: 0.677 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Cross-lingual Language Model Pretraining\n",
      "Cos similarity score: 0.677 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Non-Projective Parsing for Statistical Machine Translation\n",
      "Cos similarity score: 0.676 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: ClearTK 2.0: Design Patterns for Machine Learning in UIMA\n",
      "Cos similarity score: 0.676 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: More Constructions, More Genres: Extending Stanford Dependencies\n",
      "Cos similarity score: 0.676 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: A Challenge Set Approach to Evaluating Machine Translation\n",
      "Cos similarity score: 0.676 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Neural Machine Translation\n",
      "Cos similarity score: 0.676 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Utilizing Target-Side Semantic Role Labels to Assist Hierarchical Phrase-based Machine Translation\n",
      "Cos similarity score: 0.675 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Colorless Green Recurrent Networks Dream Hierarchically\n",
      "Cos similarity score: 0.675 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Grounded Compositional Semantics for Finding and Describing Images with Sentences\n",
      "Cos similarity score: 0.675 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: An Empirical Examination of Challenges in Chinese Parsing\n",
      "Cos similarity score: 0.674 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Determining Modality and Factuality for Text Entailment\n",
      "Cos similarity score: 0.674 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Exploring Word Sense Disambiguation Abilities of Neural Machine Translation Systems (Non-archival Extended Abstract)\n",
      "Cos similarity score: 0.674 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Multi-task Sequence to Sequence Learning\n",
      "Cos similarity score: 0.674 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Coarse-to-Fine n-Best Parsing and MaxEnt Discriminative Reranking\n",
      "Cos similarity score: 0.673 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Distributed representations, simple recurrent networks, and grammatical structure\n",
      "Cos similarity score: 0.673 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: \"What is relevant in a text document?\": An interpretable machine learning approach\n",
      "Cos similarity score: 0.673 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Rationale-Augmented Convolutional Neural Networks for Text Classification\n",
      "Cos similarity score: 0.673 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: A BERT Baseline for the Natural Questions\n",
      "Cos similarity score: 0.673 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Learning Noun-Modifier Semantic Relations with Corpus-based and WordNet-based Features\n",
      "Cos similarity score: 0.673 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Evaluating Layers of Representation in Neural Machine Translation on Part-of-Speech and Semantic Tagging Tasks\n",
      "Cos similarity score: 0.673 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Espresso: Leveraging Generic Patterns for Automatically Harvesting Semantic Relations\n",
      "Cos similarity score: 0.672 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: A Scalable Hierarchical Distributed Language Model\n",
      "Cos similarity score: 0.672 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Modeling Inflection and Word-Formation in SMT\n",
      "Cos similarity score: 0.672 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: 11,001 New Features for Statistical Machine Translation\n",
      "Cos similarity score: 0.672 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: SemEval-2017 Task 2: Multilingual and Cross-lingual Semantic Word Similarity\n",
      "Cos similarity score: 0.670 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Semantic Role Features for Machine Translation\n",
      "Cos similarity score: 0.670 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks\n",
      "Cos similarity score: 0.670 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Language processing and learning models for community question answering in Arabic\n",
      "Cos similarity score: 0.670 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Recursive Deep Models for Discourse Parsing\n",
      "Cos similarity score: 0.670 and it's \u001b[32mcited in     \u001b[0mthe main paper with title: Targeted Syntactic Evaluation of Language Models\n",
      "Cos similarity score: 0.669 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Jointly optimizing word representations for lexical and sentential tasks with the C-PHRASE model\n",
      "Cos similarity score: 0.669 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: MEANTIME, the NewsReader Multilingual Event and Time Corpus\n",
      "Cos similarity score: 0.669 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Unsupervised Neural Machine Translation\n",
      "Cos similarity score: 0.669 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Understanding and Improving Morphological Learning in the Neural Machine Translation Decoder\n",
      "Cos similarity score: 0.669 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: CCGbank: A Corpus of CCG Derivations and Dependency Structures Extracted from the Penn Treebank\n",
      "Cos similarity score: 0.668 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Analyzing Linguistic Knowledge in Sequential Model of Sentence\n",
      "Cos similarity score: 0.668 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: A Fast and Accurate Dependency Parser using Neural Networks\n",
      "Cos similarity score: 0.668 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Visually Grounded Learning of Keyword Prediction from Untranscribed Speech\n",
      "Cos similarity score: 0.667 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Extracting Pre-ordering Rules from Predicate-Argument Structures\n",
      "Cos similarity score: 0.667 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Identifying Relations for Open Information Extraction\n",
      "Cos similarity score: 0.667 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Effective Self-Training for Parsing\n",
      "Cos similarity score: 0.667 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: A connectionist approach to machine translation\n",
      "Cos similarity score: 0.667 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: The Microsoft Research Sentence Completion Challenge\n",
      "Cos similarity score: 0.666 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Edinburgh Neural Machine Translation Systems for WMT 16\n",
      "Cos similarity score: 0.665 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Massive Exploration of Neural Machine Translation Architectures\n",
      "Cos similarity score: 0.665 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Long Short-Term Memory-Networks for Machine Reading\n",
      "Cos similarity score: 0.665 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: GloVe: Global Vectors for Word Representation\n",
      "Cos similarity score: 0.664 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Fully Character-Level Neural Machine Translation without Explicit Segmentation\n",
      "Cos similarity score: 0.664 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Challenging Language-Dependent Segmentation for Arabic: An Application to Machine Translation and Part-of-Speech Tagging\n",
      "Cos similarity score: 0.663 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Supertagging: An Approach to Almost Parsing\n",
      "Cos similarity score: 0.663 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: The Importance of Being Recurrent for Modeling Hierarchical Structure\n",
      "Cos similarity score: 0.662 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Open Language Learning for Information Extraction\n",
      "Cos similarity score: 0.662 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Synthetic and Natural Noise Both Break Neural Machine Translation\n",
      "Cos similarity score: 0.662 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Enriching Word Vectors with Subword Information\n",
      "Cos similarity score: 0.662 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: The representational geometry of word meanings acquired by neural machine translation models\n",
      "Cos similarity score: 0.662 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: From Characters to Words to in Between: Do We Capture Morphology?\n",
      "Cos similarity score: 0.661 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: An Arabic-Hebrew parallel corpus of TED talks\n",
      "Cos similarity score: 0.661 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Unbounded Dependency Recovery for Parser Evaluation\n",
      "Cos similarity score: 0.661 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Producing Unseen Morphological Variants in Statistical Machine Translation\n",
      "Cos similarity score: 0.660 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Exploring Compositional Architectures and Word Vector Representations for Prepositional Phrase Attachment\n",
      "Cos similarity score: 0.660 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Recurrent Continuous Translation Models\n",
      "Cos similarity score: 0.659 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Building a Large Annotated Corpus of English: The Penn Treebank\n",
      "Cos similarity score: 0.659 and it's \u001b[32mcited in     \u001b[0mthe main paper with title: Attention is All you Need\n",
      "Cos similarity score: 0.659 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Revisiting Visual Question Answering Baselines\n",
      "Cos similarity score: 0.658 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Multitask Learning with Low-Level Auxiliary Tasks for Encoder-Decoder Based Speech Recognition\n",
      "Cos similarity score: 0.658 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation\n",
      "Cos similarity score: 0.658 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: UIMA: an architectural approach to unstructured information processing in the corporate research environment\n",
      "Cos similarity score: 0.658 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Word Sense Disambiguation Improves Statistical Machine Translation\n",
      "Cos similarity score: 0.657 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Distant supervision for relation extraction without labeled data\n",
      "Cos similarity score: 0.657 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Classifying Relations by Ranking with Convolutional Neural Networks\n",
      "Cos similarity score: 0.657 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Evaluating the morphological competence of Machine Translation Systems\n",
      "Cos similarity score: 0.656 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: State-of-the-Art Speech Recognition with Sequence-to-Sequence Models\n",
      "Cos similarity score: 0.656 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Travatar: A Forest-to-String Machine Translation Engine based on Tree Transducers\n",
      "Cos similarity score: 0.656 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: The Goldilocks Principle: Reading Children's Books with Explicit Memory Representations\n",
      "Cos similarity score: 0.655 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: A fast and simple algorithm for training neural probabilistic language models\n",
      "Cos similarity score: 0.655 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: SemEval-2017 Task 7: Detection and Interpretation of English Puns\n",
      "Cos similarity score: 0.654 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Comparing Representations of Semantic Roles for String-To-Tree Decoding\n",
      "Cos similarity score: 0.654 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Distributional Semantics Beyond Words: Supervised Learning of Analogy and Paraphrase\n",
      "Cos similarity score: 0.653 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Generating Complex Morphology for Machine Translation\n",
      "Cos similarity score: 0.653 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: A Conditional Random Field Word Segmenter for Sighan Bakeoff 2005\n",
      "Cos similarity score: 0.653 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Representation of Linguistic Form and Function in Recurrent Neural Networks\n",
      "Cos similarity score: 0.653 and it's \u001b[32mcited in     \u001b[0mthe main paper with title: Does String-Based Neural MT Learn Source Syntax?\n",
      "Cos similarity score: 0.653 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models\n",
      "Cos similarity score: 0.652 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer\n",
      "Cos similarity score: 0.652 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: The Necessity of Parsing for Predicate Argument Recognition\n",
      "Cos similarity score: 0.651 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: One billion word benchmark for measuring progress in statistical language modeling\n",
      "Cos similarity score: 0.651 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation\n",
      "Cos similarity score: 0.651 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: SciTaiL: A Textual Entailment Dataset from Science Question Answering\n",
      "Cos similarity score: 0.651 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Stochastic Answer Networks for Machine Reading Comprehension\n",
      "Cos similarity score: 0.651 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: The Disambiguation of Nominalisations\n",
      "Cos similarity score: 0.650 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Linguistic Regularities in Continuous Space Word Representations\n",
      "Cos similarity score: 0.650 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Learning to Parse and Translate Improves Neural Machine Translation\n",
      "Cos similarity score: 0.650 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Probabilistic Top-Down Parsing and Language Modeling\n",
      "Cos similarity score: 0.650 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Dependency Grammar and Dependency Parsing\n",
      "Cos similarity score: 0.650 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Fully convolutional networks for semantic segmentation\n",
      "Cos similarity score: 0.649 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Max-Margin Tensor Neural Network for Chinese Word Segmentation\n",
      "Cos similarity score: 0.649 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Using “Annotator Rationales” to Improve Machine Learning for Text Categorization\n",
      "Cos similarity score: 0.648 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: SUTime: A library for recognizing and normalizing time expressions\n",
      "Cos similarity score: 0.648 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Multi-Task Video Captioning with Video and Entailment Generation\n",
      "Cos similarity score: 0.648 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Hierarchical Probabilistic Neural Network Language Model\n",
      "Cos similarity score: 0.647 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Neural Machine Translation with Source Dependency Representation\n",
      "Cos similarity score: 0.647 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: CCG Supertags in Factored Statistical Machine Translation\n",
      "Cos similarity score: 0.646 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Google’s Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation\n",
      "Cos similarity score: 0.646 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Statistical Machine Translation\n",
      "Cos similarity score: 0.646 and it's \u001b[32mcited in     \u001b[0mthe main paper with title: On internal language representations in deep learning: an analysis of machine translation and speech recognition\n",
      "Cos similarity score: 0.646 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Deep Recurrent Models with Fast-Forward Connections for Neural Machine Translation\n",
      "Cos similarity score: 0.645 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: An Empirical Exploration of Recurrent Network Architectures\n",
      "Cos similarity score: 0.645 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Improving Statistical Machine Translation Using Word Sense Disambiguation\n",
      "Cos similarity score: 0.645 and it's \u001b[32mcited in     \u001b[0mthe main paper with title: Distributed Representations of Words and Phrases and their Compositionality\n",
      "Cos similarity score: 0.645 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Neural Machine Translation of Rare Words with Subword Units\n",
      "Cos similarity score: 0.644 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Europarl: A Parallel Corpus for Statistical Machine Translation\n",
      "Cos similarity score: 0.644 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Automatic disambiguation of English puns\n",
      "Cos similarity score: 0.644 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Word Translation Without Parallel Data\n",
      "Cos similarity score: 0.644 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: OntoNotes: The 90% Solution\n",
      "Cos similarity score: 0.643 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: On the difficulty of a distributional semantics of spoken language\n",
      "Cos similarity score: 0.643 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Reading Tea Leaves: How Humans Interpret Topic Models\n",
      "Cos similarity score: 0.643 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Overview of BioNLP’09 Shared Task on Event Extraction\n",
      "Cos similarity score: 0.642 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Semantic Compositionality through Recursive Matrix-Vector Spaces\n",
      "Cos similarity score: 0.642 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Continuous Space Translation Models for Phrase-Based Statistical Machine Translation\n",
      "Cos similarity score: 0.642 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Charagram: Embedding Words and Sentences via Character n-grams\n",
      "Cos similarity score: 0.641 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Text Classification Improved by Integrating Bidirectional LSTM with Two-dimensional Max Pooling\n",
      "Cos similarity score: 0.641 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Using the Output Embedding to Improve Language Models\n",
      "Cos similarity score: 0.641 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Judging Grammaticality with Tree Substitution Grammar Derivations\n",
      "Cos similarity score: 0.640 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Distributed Representations of Sentences and Documents\n",
      "Cos similarity score: 0.640 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Hierarchical Neural Story Generation\n",
      "Cos similarity score: 0.640 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: EESEN: End-to-end speech recognition using deep RNN models and WFST-based decoding\n",
      "Cos similarity score: 0.639 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Learning New Semi-Supervised Deep Auto-encoder Features for Statistical Machine Translation\n",
      "Cos similarity score: 0.639 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: End-to-end attention-based large vocabulary speech recognition\n",
      "Cos similarity score: 0.639 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Optimizing Chinese Word Segmentation for Machine Translation Performance\n",
      "Cos similarity score: 0.639 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: WIT3: Web Inventory of Transcribed and Translated Talks\n",
      "Cos similarity score: 0.639 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Morphology-aware statistical machine translation based on morphs induced in an unsupervised manner\n",
      "Cos similarity score: 0.638 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling\n",
      "Cos similarity score: 0.638 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Connectionist F-structure transfer\n",
      "Cos similarity score: 0.637 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Improved Tree-to-String Transducer for Machine Translation\n",
      "Cos similarity score: 0.637 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: What Can Syntax-Based MT Learn from Phrase-Based MT?\n",
      "Cos similarity score: 0.637 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks\n",
      "Cos similarity score: 0.635 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Towards the automatic detection and identification of English puns\n",
      "Cos similarity score: 0.635 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Deep architectures for Neural Machine Translation\n",
      "Cos similarity score: 0.635 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: What’s in an Embedding? Analyzing Word Embeddings through Multilingual Evaluation\n",
      "Cos similarity score: 0.634 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: RECOGNIZING STRONG AND WEAK OPINION CLAUSES\n",
      "Cos similarity score: 0.634 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Neural Network Acceptability Judgments\n",
      "Cos similarity score: 0.634 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Most “babies” are “little” and most “problems” are “huge”: Compositional Entailment in Adjective-Nouns\n",
      "Cos similarity score: 0.633 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Adversarial Examples for Evaluating Reading Comprehension Systems\n",
      "Cos similarity score: 0.633 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Neural Machine Translation by Jointly Learning to Align and Translate\n",
      "Cos similarity score: 0.632 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: From Group to Individual Labels Using Deep Features\n",
      "Cos similarity score: 0.632 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Introduction to the CoNLL-2000 Shared Task Chunking\n",
      "Cos similarity score: 0.631 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Classifying the Semantic Relations in Noun Compounds via a Domain-Specific Lexical Hierarchy\n",
      "Cos similarity score: 0.631 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: VerbOcean: Mining the Web for Fine-Grained Semantic Verb Relations\n",
      "Cos similarity score: 0.630 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: FOIL it! Find One mismatch between Image and Language caption\n",
      "Cos similarity score: 0.630 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: CoQA: A Conversational Question Answering Challenge\n",
      "Cos similarity score: 0.630 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: How Grammatical is Character-level Neural Machine Translation? Assessing MT Quality with Contrastive Translation Pairs\n",
      "Cos similarity score: 0.630 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence\n",
      "Cos similarity score: 0.630 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: JU_CSE_TAC: Textual Entailment Recognition System at TAC RTE-6\n",
      "Cos similarity score: 0.630 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Solving Relational Similarity Problems Using the Web as a Corpus\n",
      "Cos similarity score: 0.629 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: The Descent of Hierarchy, and Selection in Relational Semantics\n",
      "Cos similarity score: 0.628 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: A Structured Self-attentive Sentence Embedding\n",
      "Cos similarity score: 0.627 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Neural Morphological Tagging of Lemma Sequences for Machine Translation\n",
      "Cos similarity score: 0.626 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Unsupervised Cross-lingual Word Embedding by Multilingual Neural Language Models\n",
      "Cos similarity score: 0.626 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: The Berkeley FrameNet Project\n",
      "Cos similarity score: 0.626 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Parsing Natural Scenes and Natural Language with Recursive Neural Networks\n",
      "Cos similarity score: 0.625 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Corpus-based Learning of Analogies and Semantic Relations\n",
      "Cos similarity score: 0.625 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Embracing data abundance: BookTest Dataset for Reading Comprehension\n",
      "Cos similarity score: 0.625 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Improving Vector Space Word Representations Using Multilingual Correlation\n",
      "Cos similarity score: 0.625 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Large-Scale Machine Translation between Arabic and Hebrew: Available Corpora and Initial Results\n",
      "Cos similarity score: 0.625 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Hierarchical Phrase-Based Translation\n",
      "Cos similarity score: 0.624 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Natural Questions: A Benchmark for Question Answering Research\n",
      "Cos similarity score: 0.624 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Semantics-Based Machine Translation with Hyperedge Replacement Grammars\n",
      "Cos similarity score: 0.624 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Interpretation of Semantic Tweet Representations\n",
      "Cos similarity score: 0.623 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Neural Machine Translation in Linear Time\n",
      "Cos similarity score: 0.623 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Introduction to the CoNLL-2005 Shared Task: Semantic Role Labeling\n",
      "Cos similarity score: 0.623 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: On the Elements of an Accurate Tree-to-String Machine Translation System\n",
      "Cos similarity score: 0.623 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: A Character-Aware Encoder for Neural Machine Translation\n",
      "Cos similarity score: 0.623 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Book Review: Natural Language Processing with Python by Steven Bird, Ewan Klein, and Edward Loper\n",
      "Cos similarity score: 0.622 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Who Did What to Whom? A Contrastive Study of Syntacto-Semantic Dependencies\n",
      "Cos similarity score: 0.622 and it's \u001b[32mcited in     \u001b[0mthe main paper with title: SemEval-2010 Task 8: Multi-Way Classification of Semantic Relations between Pairs of Nominals\n",
      "Cos similarity score: 0.622 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Phrase reordering for statistical machine translation based on predicate-argument structure\n",
      "Cos similarity score: 0.622 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: HotFlip: White-Box Adversarial Examples for NLP\n",
      "Cos similarity score: 0.622 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Axiomatic Attribution for Deep Networks\n",
      "Cos similarity score: 0.622 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Adjoining Tree-to-String Translation\n",
      "Cos similarity score: 0.621 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Listen, Attend and Spell\n",
      "Cos similarity score: 0.621 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Distilling the Knowledge in a Neural Network\n",
      "Cos similarity score: 0.621 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: FRAGE: Frequency-Agnostic Word Representation\n",
      "Cos similarity score: 0.620 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: A Character-level Convolutional Neural Network for Distinguishing Similar Languages and Dialects\n",
      "Cos similarity score: 0.620 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Hybrid speech recognition with Deep Bidirectional LSTM\n",
      "Cos similarity score: 0.620 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Temporal Ensembling for Semi-Supervised Learning\n",
      "Cos similarity score: 0.620 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Learning Word Vectors for Sentiment Analysis\n",
      "Cos similarity score: 0.620 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Shallow Semantic Trees for SMT\n",
      "Cos similarity score: 0.619 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Learning Word-Like Units from Joint Audio-Visual Analysis\n",
      "Cos similarity score: 0.619 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Factorization tricks for LSTM networks\n",
      "Cos similarity score: 0.619 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Neural Reordering Model Considering Phrase Translation and Word Alignment for Phrase-based Translation\n",
      "Cos similarity score: 0.619 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Acoustic modelling with CD-CTC-SMBR LSTM RNNS\n",
      "Cos similarity score: 0.618 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation\n",
      "Cos similarity score: 0.618 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Unsupervised Learning of Visual Representations by Solving Jigsaw Puzzles\n",
      "Cos similarity score: 0.617 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Efficient Estimation of Word Representations in Vector Space\n",
      "Cos similarity score: 0.617 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Evaluation of Dependency Parsers on Unbounded Dependencies\n",
      "Cos similarity score: 0.617 and it's \u001b[32mcited in     \u001b[0mthe main paper with title: A Gold Standard Dependency Corpus for English\n",
      "Cos similarity score: 0.616 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Speech2Vec: A Sequence-to-Sequence Framework for Learning Word Embeddings from Speech\n",
      "Cos similarity score: 0.616 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Rethinking the Inception Architecture for Computer Vision\n",
      "Cos similarity score: 0.616 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Unsupervised Learning of Spoken Language with Visual Context\n",
      "Cos similarity score: 0.616 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Learning Generic Sentence Representations Using Convolutional Neural Networks\n",
      "Cos similarity score: 0.615 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Synchronous Tree-Adjoining Grammars\n",
      "Cos similarity score: 0.615 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling\n",
      "Cos similarity score: 0.615 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Librispeech: An ASR corpus based on public domain audio books\n",
      "Cos similarity score: 0.615 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Classification of Semantic Relationships between Nominals Using Pattern Clusters\n",
      "Cos similarity score: 0.615 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Using Syntactic Coupling Features for Discriminating Phrase-Based Translations (WMT-08 Shared Translation Task)\n",
      "Cos similarity score: 0.615 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: A Syntax-Directed Translator with Extended Domain of Locality\n",
      "Cos similarity score: 0.614 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Natural Language Processing with Python\n",
      "Cos similarity score: 0.614 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Hindi-to-Urdu Machine Translation through Transliteration\n",
      "Cos similarity score: 0.614 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: The IIT Bombay English-Hindi Parallel Corpus\n",
      "Cos similarity score: 0.612 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Linguistically Motivated Unsupervised Segmentation for Machine Translation\n",
      "Cos similarity score: 0.612 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: The VerbCorner Project: Toward an Empirically-Based Semantic Decomposition of Verbs\n",
      "Cos similarity score: 0.612 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: A Decoder for Syntax-based Statistical MT\n",
      "Cos similarity score: 0.611 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Diagnostic Classifiers Revealing how Neural Networks Process Hierarchical Structure\n",
      "Cos similarity score: 0.611 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: ReMixMatch: Semi-Supervised Learning with Distribution Matching and Augmentation Anchoring\n",
      "Cos similarity score: 0.611 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: The Parallel Meaning Bank: Towards a Multilingual Corpus of Translations Annotated with Compositional Meaning Representations\n",
      "Cos similarity score: 0.610 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Applying Morphology Generation Models to Machine Translation\n",
      "Cos similarity score: 0.610 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Distributional vectors encode referential attributes\n",
      "Cos similarity score: 0.610 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: A Hybrid Morpheme-Word Representation for Machine Translation of Morphologically Rich Languages\n",
      "Cos similarity score: 0.610 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Semantic Roles for String to Tree Machine Translation\n",
      "Cos similarity score: 0.609 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Explaining Recurrent Neural Network Predictions in Sentiment Analysis\n",
      "Cos similarity score: 0.609 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Neural Speech Recognizer: Acoustic-to-Word LSTM Model for Large Vocabulary Speech Recognition\n",
      "Cos similarity score: 0.609 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: DivideMix: Learning with Noisy Labels as Semi-supervised Learning\n",
      "Cos similarity score: 0.609 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Convolutional, Long Short-Term Memory, fully connected Deep Neural Networks\n",
      "Cos similarity score: 0.608 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: A causal framework for explaining the predictions of black-box sequence-to-sequence models\n",
      "Cos similarity score: 0.608 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Stanford typed dependencies manual\n",
      "Cos similarity score: 0.607 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Effective Information Extraction with Semantic Affinity Patterns and Relevant Regions\n",
      "Cos similarity score: 0.607 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: A Simple Framework for Contrastive Learning of Visual Representations\n",
      "Cos similarity score: 0.607 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Speech and language processing - an introduction to natural language processing, computational linguistics, and speech recognition\n",
      "Cos similarity score: 0.606 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: The Winograd Schema Challenge\n",
      "Cos similarity score: 0.606 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Relation Extraction with Matrix Factorization and Universal Schemas\n",
      "Cos similarity score: 0.606 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Convolutional Neural Networks for Sentence Classification\n",
      "Cos similarity score: 0.606 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Improving Entity Recommendation with Search Log and Multi-Task Learning\n",
      "Cos similarity score: 0.606 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: From Frequency to Meaning: Vector Space Models of Semantics\n",
      "Cos similarity score: 0.605 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Character-level Convolutional Networks for Text Classification\n",
      "Cos similarity score: 0.605 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Combining Word-Level and Character-Level Models for Machine Translation Between Closely-Related Languages\n",
      "Cos similarity score: 0.605 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Do We Train on Test Data? Purging CIFAR of Near-Duplicates\n",
      "Cos similarity score: 0.605 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: A Natural Language Translation Neural Network\n",
      "Cos similarity score: 0.605 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Introduction to the CoNLL-2004 Shared Task: Semantic Role Labeling\n",
      "Cos similarity score: 0.605 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Momentum Contrast for Unsupervised Visual Representation Learning\n",
      "Cos similarity score: 0.605 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Methods for interpreting and understanding deep neural networks\n",
      "Cos similarity score: 0.604 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: A Neural Probabilistic Language Model\n",
      "Cos similarity score: 0.604 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Investigating gated recurrent networks for speech synthesis\n",
      "Cos similarity score: 0.604 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: SRILM - an extensible language modeling toolkit\n",
      "Cos similarity score: 0.604 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Performance Impact Caused by Hidden Bias of Training Data for Recognizing Textual Entailment\n",
      "Cos similarity score: 0.604 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Representations of language in a model of visually grounded speech signal\n",
      "Cos similarity score: 0.604 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: The United Nations Parallel Corpus v1.0\n",
      "Cos similarity score: 0.604 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: A Discriminative Model for Tree-to-Tree Translation\n",
      "Cos similarity score: 0.603 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Using Test Suites in Evaluation of Machine Translation Systems\n",
      "Cos similarity score: 0.603 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Automatic Interpretation of Noun Compounds Using WordNet Similarity\n",
      "Cos similarity score: 0.603 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: MADAMIRA: A Fast, Comprehensive Tool for Morphological Analysis and Disambiguation of Arabic\n",
      "Cos similarity score: 0.603 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Tregex and Tsurgeon: tools for querying and manipulating tree data structures\n",
      "Cos similarity score: 0.602 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Structures, Not Strings: Linguistics as Part of the Cognitive Sciences\n",
      "Cos similarity score: 0.602 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Can Active Memory Replace Attention?\n",
      "Cos similarity score: 0.602 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Offline bilingual word vectors, orthogonal transformations and the inverted softmax\n",
      "Cos similarity score: 0.602 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: From speech to letters - using a novel neural network architecture for grapheme based ASR\n",
      "Cos similarity score: 0.601 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Direct Acoustics-to-Word Models for English Conversational Speech Recognition\n",
      "Cos similarity score: 0.601 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Bleu: a Method for Automatic Evaluation of Machine Translation\n",
      "Cos similarity score: 0.600 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition\n",
      "Cos similarity score: 0.600 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Deep multimodal semantic embeddings for speech and images\n",
      "Cos similarity score: 0.600 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Syntax-Based Statistical Machine Translation\n",
      "Cos similarity score: 0.599 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Convolutional Sequence to Sequence Learning\n",
      "Cos similarity score: 0.599 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Towards End-To-End Speech Recognition with Recurrent Neural Networks\n",
      "Cos similarity score: 0.599 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Continuous Space Language Models for Statistical Machine Translation\n",
      "Cos similarity score: 0.599 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Batch normalized recurrent neural networks\n",
      "Cos similarity score: 0.599 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: SemEval 2014 Task 8: Broad-Coverage Semantic Dependency Parsing\n",
      "Cos similarity score: 0.598 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: The Alignment Template Approach to Statistical Machine Translation\n",
      "Cos similarity score: 0.598 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Deep Speech 2 : End-to-End Speech Recognition in English and Mandarin\n",
      "Cos similarity score: 0.597 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: On the Properties of Neural Machine Translation: Encoder–Decoder Approaches\n",
      "Cos similarity score: 0.597 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Natural Language Inference from Multiple Premises\n",
      "Cos similarity score: 0.597 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Handbook of Natural Language Processing and Machine Translation: DARPA Global Autonomous Language Exploitation\n",
      "Cos similarity score: 0.596 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Unsupervised Clustering using Pseudo-semi-supervised Learning\n",
      "Cos similarity score: 0.596 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Deep Convolutional Neural Networks for Sentiment Analysis of Short Texts\n",
      "Cos similarity score: 0.595 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Learning Recursive Distributed Representations for Holistic Computation\n",
      "Cos similarity score: 0.595 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: WSABIE: Scaling Up to Large Vocabulary Image Annotation\n",
      "Cos similarity score: 0.595 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Do CIFAR-10 Classifiers Generalize to CIFAR-10?\n",
      "Cos similarity score: 0.595 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Strategies for training large scale neural network language models\n",
      "Cos similarity score: 0.595 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Early years in machine translation: memoirs and biographies of pioneers\n",
      "Cos similarity score: 0.595 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Probing for semantic evidence of composition by means of simple classification tasks\n",
      "Cos similarity score: 0.594 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Class-Based n-gram Models of Natural Language\n",
      "Cos similarity score: 0.594 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books\n",
      "Cos similarity score: 0.594 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: QMDIS: QCRI-MIT Advanced Dialect Identification System\n",
      "Cos similarity score: 0.594 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Unsupervised Learning of Visual Features by Contrasting Cluster Assignments\n",
      "Cos similarity score: 0.593 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Illusory licensing effects across dependency types: ERP evidence\n",
      "Cos similarity score: 0.593 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: GATCluster: Self-Supervised Gaussian-Attention Network for Image Clustering\n",
      "Cos similarity score: 0.593 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Deep Semantic Clustering by Partition Confidence Maximisation\n",
      "Cos similarity score: 0.592 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Unsupervised Representation Learning by Predicting Image Rotations\n",
      "Cos similarity score: 0.592 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Controlling Politeness in Neural Machine Translation via Side Constraints\n",
      "Cos similarity score: 0.591 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: A tree-to-tree alignment-based model for statistical machine translation\n",
      "Cos similarity score: 0.591 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Automatic differentiation in PyTorch\n",
      "Cos similarity score: 0.591 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: MixMatch: A Holistic Approach to Semi-Supervised Learning\n",
      "Cos similarity score: 0.590 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Towards Unsupervised Automatic Speech Recognition Trained by Unaligned Speech and Text only\n",
      "Cos similarity score: 0.590 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: JANUS: a speech-to-speech translation system using connectionist and symbolic processing strategies\n",
      "Cos similarity score: 0.589 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: OpenNMT: Open-Source Toolkit for Neural Machine Translation\n",
      "Cos similarity score: 0.589 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Parallel Data, Tools and Interfaces in OPUS\n",
      "Cos similarity score: 0.589 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Linguistics Vanguard\n",
      "Cos similarity score: 0.589 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Improving Word Sense Disambiguation in Neural Machine Translation with Sense Embeddings\n",
      "Cos similarity score: 0.589 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Visualisation and 'diagnostic classifiers' reveal how recurrent and recursive neural networks process hierarchical structure\n",
      "Cos similarity score: 0.589 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Factored Translation Models\n",
      "Cos similarity score: 0.588 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results\n",
      "Cos similarity score: 0.588 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: TSNLP - Test Suites for Natural Language Processing\n",
      "Cos similarity score: 0.588 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Modeling Syntactic and Semantic Structures in Hierarchical Phrase-based Translation\n",
      "Cos similarity score: 0.588 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data\n",
      "Cos similarity score: 0.588 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Very deep convolutional networks for end-to-end speech recognition\n",
      "Cos similarity score: 0.587 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Thematic proto-roles and argument selection\n",
      "Cos similarity score: 0.587 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Moses: Open Source Toolkit for Statistical Machine Translation\n",
      "Cos similarity score: 0.587 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: A computational model of S-selection\n",
      "Cos similarity score: 0.586 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: The emergence of grammaticality in connectionist networks.\n",
      "Cos similarity score: 0.586 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: A probabilistic framework for segment-based speech recognition\n",
      "Cos similarity score: 0.586 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Visual interpretability for deep learning: a survey\n",
      "Cos similarity score: 0.586 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Using a stochastic context-free grammar as a language model for speech recognition\n",
      "Cos similarity score: 0.585 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: SCAN: Learning to Classify Images Without Labels\n",
      "Cos similarity score: 0.585 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Learning the speech front-end with raw waveform CLDNNs\n",
      "Cos similarity score: 0.585 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: On the semantics of noun compounds\n",
      "Cos similarity score: 0.585 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Unsupervised Morphology Rivals Supervised Morphology for Arabic MT\n",
      "Cos similarity score: 0.584 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Recognizing textual entailment: Rational, evaluation and approaches – Erratum\n",
      "Cos similarity score: 0.584 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Learning Non-Isomorphic Tree Mappings for Machine Translation\n",
      "Cos similarity score: 0.584 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: From phonemes to images: levels of representation in a recurrent neural model of visually-grounded language learning\n",
      "Cos similarity score: 0.583 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Verbnet: a broad-coverage, comprehensive verb lexicon\n",
      "Cos similarity score: 0.582 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Darmstadt Knowledge Processing Repository Based on UIMA\n",
      "Cos similarity score: 0.582 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Towards A Rigorous Science of Interpretable Machine Learning\n",
      "Cos similarity score: 0.582 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Exploiting Similarities among Languages for Machine Translation\n",
      "Cos similarity score: 0.581 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: A Hierarchical Phrase-Based Model for Statistical Machine Translation\n",
      "Cos similarity score: 0.581 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Learning Weakly Supervised Multimodal Phoneme Embeddings\n",
      "Cos similarity score: 0.581 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Part-of-Speech Tagging With Neural Networks\n",
      "Cos similarity score: 0.581 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Humor Recognition and Humor Anchor Extraction\n",
      "Cos similarity score: 0.581 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Contrastive Clustering\n",
      "Cos similarity score: 0.580 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Analyzing Hidden Representations in End-to-End Automatic Speech Recognition Systems\n",
      "Cos similarity score: 0.580 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Negative and Positive Polarity Items: Variation, Licensing, and Compositionality\n",
      "Cos similarity score: 0.579 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Associative Deep Clustering: Training a Classification Network with No Labels\n",
      "Cos similarity score: 0.579 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Statistical Language Models Based on Neural Networks\n",
      "Cos similarity score: 0.579 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Long Short-Term Memory\n",
      "Cos similarity score: 0.578 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Continuous space language models\n",
      "Cos similarity score: 0.578 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Encoding of phonology in a recurrent neural model of grounded speech\n",
      "Cos similarity score: 0.578 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Lexicon-Free Conversational Speech Recognition with Neural Networks\n",
      "Cos similarity score: 0.578 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Exploring neural network architectures for acoustic modeling\n",
      "Cos similarity score: 0.577 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Self-labelling via simultaneous clustering and representation learning\n",
      "Cos similarity score: 0.577 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Learning representations by back-propagating errors\n",
      "Cos similarity score: 0.576 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Intriguing properties of neural networks\n",
      "Cos similarity score: 0.576 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Domain Adaptation for Large-Scale Sentiment Classification: A Deep Learning Approach\n",
      "Cos similarity score: 0.576 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Regularization With Stochastic Transformations and Perturbations for Deep Semi-Supervised Learning\n",
      "Cos similarity score: 0.574 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Unsupervised Data Augmentation\n",
      "Cos similarity score: 0.573 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions\n",
      "Cos similarity score: 0.573 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Using latent semantic analysis to improve access to textual information\n",
      "Cos similarity score: 0.572 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Using a Semantic Concordance for Sense Identification\n",
      "Cos similarity score: 0.572 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Visualizing and Understanding Convolutional Networks\n",
      "Cos similarity score: 0.572 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Long short-term memory recurrent neural network architectures for large scale acoustic modeling\n",
      "Cos similarity score: 0.572 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Statistical Phrase-Based Translation\n",
      "Cos similarity score: 0.571 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Houdini : Fooling Deep Structured Visual and Speech Recognition Models with Adversarial Examples\n",
      "Cos similarity score: 0.571 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: On the unification of syntactic annotations under the Stanford dependency scheme: A case study on BioInfer and GENIA\n",
      "Cos similarity score: 0.570 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: A Theoretically Grounded Application of Dropout in Recurrent Neural Networks\n",
      "Cos similarity score: 0.569 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Bootstrap Your Own Latent: A New Approach to Self-Supervised Learning\n",
      "Cos similarity score: 0.569 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Speech recognition with deep recurrent neural networks\n",
      "Cos similarity score: 0.568 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Deep Clustering: On the Link Between Discriminative Models and K-Means\n",
      "Cos similarity score: 0.568 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Weighted finite-state transducers in speech recognition\n",
      "Cos similarity score: 0.567 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Finding Structure in Time\n",
      "Cos similarity score: 0.567 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Explainable Outfit Recommendation with Joint Outfit Matching and Comment Generation\n",
      "Cos similarity score: 0.566 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Jointly Learning Explainable Rules for Recommendation with Knowledge Graph\n",
      "Cos similarity score: 0.566 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: The MIT SUMMIT Speech Recognition System: A Progress Report\n",
      "Cos similarity score: 0.566 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: GATE: an Architecture for Development of Robust HLT applications\n",
      "Cos similarity score: 0.566 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Tree-to-String Alignment Template for Statistical Machine Translation\n",
      "Cos similarity score: 0.565 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: String-to-Dependency Statistical Machine Translation\n",
      "Cos similarity score: 0.564 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks\n",
      "Cos similarity score: 0.562 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Pseudo-Supervised Deep Subspace Clustering\n",
      "Cos similarity score: 0.562 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Grammaticality, Acceptability, and Probability: A Probabilistic View of Linguistic Knowledge\n",
      "Cos similarity score: 0.562 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Can recurrent neural networks learn natural language grammars?\n",
      "Cos similarity score: 0.562 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Generating Typed Dependency Parses from Phrase Structure Parses\n",
      "Cos similarity score: 0.562 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Improving Outfit Recommendation with Co-supervision of Fashion Generation\n",
      "Cos similarity score: 0.561 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer\n",
      "Cos similarity score: 0.561 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Deep Discriminative Clustering Analysis\n",
      "Cos similarity score: 0.560 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Explainable Recommendation via Multi-Task Learning in Opinionated Text Data\n",
      "Cos similarity score: 0.560 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Neurophysiological dynamics of phrase-structure building during sentence processing\n",
      "Cos similarity score: 0.560 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Structure at the lexical level and its implication for transfer grammar\n",
      "Cos similarity score: 0.560 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: WaveNet: A Generative Model for Raw Audio\n",
      "Cos similarity score: 0.560 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Arabic Preprocessing Schemes for Statistical Machine Translation\n",
      "Cos similarity score: 0.559 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Improving Neural Language Models with a Continuous Cache\n",
      "Cos similarity score: 0.559 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Improved Regularization of Convolutional Neural Networks with Cutout\n",
      "Cos similarity score: 0.558 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Semi-supervised Learning with Ladder Networks\n",
      "Cos similarity score: 0.558 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: RandAugment: Practical data augmentation with no separate search\n",
      "Cos similarity score: 0.558 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Semantic Classification with Distributional Kernels\n",
      "Cos similarity score: 0.557 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Deep Learning Scaling is Predictable, Empirically\n",
      "Cos similarity score: 0.557 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Natural Language Understanding\n",
      "Cos similarity score: 0.556 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Neural Word Embedding as Implicit Matrix Factorization\n",
      "Cos similarity score: 0.556 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Greedy Layer-Wise Training of Deep Networks\n",
      "Cos similarity score: 0.556 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: 5: Grammatical Illusions and Selective Fallibility in Real-Time Language Comprehension\n",
      "Cos similarity score: 0.555 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Invariant Information Clustering for Unsupervised Image Classification and Segmentation\n",
      "Cos similarity score: 0.555 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Discriminatively Boosted Image Clustering with Fully Convolutional Auto-Encoders\n",
      "Cos similarity score: 0.555 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Neural Architectures for Named Entity Recognition\n",
      "Cos similarity score: 0.555 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Machine Translation: Past, Present, Future\n",
      "Cos similarity score: 0.554 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Hot Topics Surrounding Acceptability Judgement Tasks\n",
      "Cos similarity score: 0.554 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Machine Translation Using Probabilistic Synchronous Dependency Insertion Grammars\n",
      "Cos similarity score: 0.553 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: The Kaldi Speech Recognition Toolkit\n",
      "Cos similarity score: 0.553 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Learning representations by backpropagating errors\n",
      "Cos similarity score: 0.552 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based on Minimum Cuts\n",
      "Cos similarity score: 0.552 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: SEMAX: Multi-Task Learning for Improving Recommendations\n",
      "Cos similarity score: 0.552 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Multimodal Explanations: Justifying Decisions and Pointing to the Evidence\n",
      "Cos similarity score: 0.552 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Notes on transitivity and theme in English: Part 2\n",
      "Cos similarity score: 0.552 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Representation and structure in connectionist models\n",
      "Cos similarity score: 0.551 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Broken agreement    \n",
      "Cos similarity score: 0.551 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Pseudo-Label : The Simple and Efficient Semi-Supervised Learning Method for Deep Neural Networks\n",
      "Cos similarity score: 0.551 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Overcoming catastrophic forgetting in neural networks\n",
      "Cos similarity score: 0.550 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Identity Mappings in Deep Residual Networks\n",
      "Cos similarity score: 0.550 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: The role of veridicality and factivity in clause selection *\n",
      "Cos similarity score: 0.550 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Variational Deep Embedding: An Unsupervised and Generative Approach to Clustering\n",
      "Cos similarity score: 0.550 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Deep Unsupervised Clustering with Gaussian Mixture Variational Autoencoders\n",
      "Cos similarity score: 0.548 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Layer-Wise Relevance Propagation for Deep Neural Network Architectures\n",
      "Cos similarity score: 0.547 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Concept to code: deep learning for multitask recommendation\n",
      "Cos similarity score: 0.547 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Content extraction using diverse feature sets\n",
      "Cos similarity score: 0.547 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Deep Residual Learning for Image Recognition\n",
      "Cos similarity score: 0.547 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Joint Unsupervised Learning of Deep Representations and Image Clusters\n",
      "Cos similarity score: 0.546 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Towards K-means-friendly Spaces: Simultaneous Deep Learning and Clustering\n",
      "Cos similarity score: 0.545 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Feature Learning in Deep Neural Networks - Studies on Speech Recognition Tasks.\n",
      "Cos similarity score: 0.545 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Understanding how Deep Belief Networks perform acoustic modelling\n",
      "Cos similarity score: 0.545 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Unsupervised Feature Learning via Non-parametric Instance Discrimination\n",
      "Cos similarity score: 0.544 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Recurrent Neural Network Grammars\n",
      "Cos similarity score: 0.542 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Training Very Deep Networks\n",
      "Cos similarity score: 0.541 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: The Mathematics of Statistical Machine Translation: Parameter Estimation\n",
      "Cos similarity score: 0.541 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Adam: A Method for Stochastic Optimization\n",
      "Cos similarity score: 0.541 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: What Does the Speaker Embedding Encode?\n",
      "Cos similarity score: 0.540 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Building machines that learn and think like people\n",
      "Cos similarity score: 0.540 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Interference effects from grammatically unavailable constituents during sentence processing.\n",
      "Cos similarity score: 0.540 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Neural Models of Factuality\n",
      "Cos similarity score: 0.540 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Naive v. expert intuitions: An empirical study of acceptability judgments\n",
      "Cos similarity score: 0.539 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Deep Comprehensive Correlation Mining for Image Clustering\n",
      "Cos similarity score: 0.538 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: An Experimental Investigation of Agent Prototypicality and Agent Prominence in German\n",
      "Cos similarity score: 0.538 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Deep Clustering via Joint Convolutional Autoencoder Embedding and Relative Entropy Minimization\n",
      "Cos similarity score: 0.537 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Detecting Gene Relations from MEDLINE Abstracts\n",
      "Cos similarity score: 0.537 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Generating Sequences With Recurrent Neural Networks\n",
      "Cos similarity score: 0.537 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Findings of the 2017 Conference on Machine Translation (WMT17)\n",
      "Cos similarity score: 0.537 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Extensions of recurrent neural network language model\n",
      "Cos similarity score: 0.537 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: A Novel Distributional Approach to Multilingual Conceptual Metaphor Recognition\n",
      "Cos similarity score: 0.535 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Understanding Black-box Predictions via Influence Functions\n",
      "Cos similarity score: 0.535 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Multitask Learning  \n",
      "Cos similarity score: 0.535 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Exploring how deep neural networks form phonemic categories\n",
      "Cos similarity score: 0.534 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Two routes to actorhood: lexicalized potency to act and identification of the actor role\n",
      "Cos similarity score: 0.534 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Discriminative Training for Large-Vocabulary Speech Recognition Using Minimum Classification Error\n",
      "Cos similarity score: 0.534 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: The Combinatorial Structure of Thought: The Family of Causative Concepts\n",
      "Cos similarity score: 0.533 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: The Natural Language Decathlon: Multitask Learning as Question Answering\n",
      "Cos similarity score: 0.533 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Nearest Neighbor Matching for Deep Clustering\n",
      "Cos similarity score: 0.531 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Agreement Attraction in Comprehension: Representations and Processes.\n",
      "Cos similarity score: 0.531 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Semi-supervised Learning by Entropy Minimization\n",
      "Cos similarity score: 0.531 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift\n",
      "Cos similarity score: 0.530 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Auto-Encoding Variational Bayes\n",
      "Cos similarity score: 0.529 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Recurrent neural network based language model\n",
      "Cos similarity score: 0.529 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: TUGAS: Exploiting unlabelled data for Twitter sentiment analysis\n",
      "Cos similarity score: 0.528 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Deep Adaptive Image Clustering\n",
      "Cos similarity score: 0.527 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Quantitative methods in syntax/semantics research: A response to Sprouse and Almeida (2013)\n",
      "Cos similarity score: 0.526 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Deep Clustering for Unsupervised Learning of Visual Features\n",
      "Cos similarity score: 0.524 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: The Chameleon-like Nature of Evaluative Adjectives\n",
      "Cos similarity score: 0.524 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: TIMIT Acoustic-Phonetic Continuous Speech Corpus\n",
      "Cos similarity score: 0.523 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Recursive Hetero-associative Memories for Translation\n",
      "Cos similarity score: 0.522 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Is Passive Syntax Semantically Constrained? Evidence From Adult Grammaticality Judgment and Comprehension Studies\n",
      "Cos similarity score: 0.522 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: ImageNet classification with deep convolutional neural networks\n",
      "Cos similarity score: 0.522 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: ADADELTA: An Adaptive Learning Rate Method\n",
      "Cos similarity score: 0.521 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Illinois-LH: A Denotational and Distributional Approach to Semantics\n",
      "Cos similarity score: 0.520 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Towards Principled Unsupervised Learning\n",
      "Cos similarity score: 0.520 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Dropout: a simple way to prevent neural networks from overfitting\n",
      "Cos similarity score: 0.519 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Visualizing Data using t-SNE\n",
      "Cos similarity score: 0.519 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Curriculum learning \n",
      "Cos similarity score: 0.519 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: ImageNet Large Scale Visual Recognition Challenge\n",
      "Cos similarity score: 0.519 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Example-based machine translation using connectionist matching\n",
      "Cos similarity score: 0.519 and it's \u001b[32mcited in     \u001b[0mthe main paper with title: Semantic Proto-Roles\n",
      "Cos similarity score: 0.519 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Recognizing Textual Entailment: Models and Applications\n",
      "Cos similarity score: 0.518 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Probabilistic Topic Models\n",
      "Cos similarity score: 0.517 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Gender Bias in Coreference Resolution\n",
      "Cos similarity score: 0.516 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: From humor recognition to irony detection: The figurative language of social media\n",
      "Cos similarity score: 0.515 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Explanation Methods in Deep Learning: Users, Values, Concerns and Challenges\n",
      "Cos similarity score: 0.514 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Explaining and Harnessing Adversarial Examples\n",
      "Cos similarity score: 0.513 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Unsupervised Deep Embedding for Clustering Analysis\n",
      "Cos similarity score: 0.513 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Black-Box Generation of Adversarial Text Sequences to Evade Deep Learning Classifiers\n",
      "Cos similarity score: 0.513 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Deep Text Classification Can be Fooled\n",
      "Cos similarity score: 0.512 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Self-Supervised Representation Learning With MUlti-Segmental Informational Coding (MUSIC)\n",
      "Cos similarity score: 0.511 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Deconvolutional networks\n",
      "Cos similarity score: 0.511 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Proto-Properties and Grammatical Encoding: A Correspondence Theory of Argument Selection\n",
      "Cos similarity score: 0.510 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Acoustic modeling with deep neural networks using raw time signal for LVCSR\n",
      "Cos similarity score: 0.510 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Deep Self-Evolution Clustering\n",
      "Cos similarity score: 0.509 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Weak quantitative standards in linguistics research\n",
      "Cos similarity score: 0.508 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Normalized Word Embedding and Orthogonal Transform for Bilingual Word Translation\n",
      "Cos similarity score: 0.506 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: What’s in a translation rule?\n",
      "Cos similarity score: 0.506 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: On the Role of Nonlinear Transformations in Deep Neural Network Acoustic Models\n",
      "Cos similarity score: 0.506 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Multi-Task Feature Learning for Knowledge Graph Enhanced Recommendation\n",
      "Cos similarity score: 0.506 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Measuring Semantic Similarity by Latent Relational Analysis\n",
      "Cos similarity score: 0.505 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Thematic roles: Core knowledge or linguistic construct?\n",
      "Cos similarity score: 0.504 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion\n",
      "Cos similarity score: 0.504 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: SPICE: Semantic Pseudo-Labeling for Image Clustering\n",
      "Cos similarity score: 0.504 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Normal and pathological development of subject–verb agreement in speech production: a study on French children\n",
      "Cos similarity score: 0.504 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Sinkhorn Distances: Lightspeed Computation of Optimal Transport\n",
      "Cos similarity score: 0.503 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Gradient Flow in Recurrent Nets: the Difficulty of Learning Long-Term Dependencies\n",
      "Cos similarity score: 0.502 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Deep Subspace Clustering Networks\n",
      "Cos similarity score: 0.502 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Phoneme recognition using time-delay neural networks\n",
      "Cos similarity score: 0.502 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Mitigating Embedding and Class Assignment Mismatch in Unsupervised Image Classification\n",
      "Cos similarity score: 0.501 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Simple Fast Algorithms for the Editing Distance Between Trees and Related Problems\n",
      "Cos similarity score: 0.501 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Noise-Contrastive Estimation of Unnormalized Statistical Models, with Applications to Natural Image Statistics\n",
      "Cos similarity score: 0.500 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Machine humour : an implemented model of puns\n",
      "Cos similarity score: 0.499 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Automatic Labeling of Semantic Roles\n",
      "Cos similarity score: 0.498 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Towards Crafting Text Adversarial Samples\n",
      "Cos similarity score: 0.498 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Layer Normalization \n",
      "Cos similarity score: 0.497 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks\n",
      "Cos similarity score: 0.496 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Automatic Recognition of Spoken Digits\n",
      "Cos similarity score: 0.495 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Social Bias in Elicited Natural Language Inferences\n",
      "Cos similarity score: 0.494 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Hidden factors and hidden topics: understanding rating dimensions with review text\n",
      "Cos similarity score: 0.494 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Unsupervised Multi-Manifold Clustering by Learning Deep Representation\n",
      "Cos similarity score: 0.492 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: A Syntax-based Statistical Translation Model\n",
      "Cos similarity score: 0.492 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Xception: Deep Learning with Depthwise Separable Convolutions\n",
      "Cos similarity score: 0.490 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Argument Realization\n",
      "Cos similarity score: 0.490 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: On the Informativity of Different Measures of Linguistic Acceptability\n",
      "Cos similarity score: 0.490 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Some thoughts on agentivity\n",
      "Cos similarity score: 0.488 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Deep Image Prior    \n",
      "Cos similarity score: 0.486 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Spoken Language Processing: A Guide to Theory, Algorithm and System Development\n",
      "Cos similarity score: 0.486 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Annotating Expressions of Opinions and Emotions in Language\n",
      "Cos similarity score: 0.483 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Recent Advances in Natural Language Processing: Selected Papers from RANLP ’95\n",
      "Cos similarity score: 0.480 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Strike (With) a Pose: Neural Networks Are Easily Fooled by Strange Poses of Familiar Objects\n",
      "Cos similarity score: 0.479 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: The 2005 PASCAL Visual Object Classes Challenge\n",
      "Cos similarity score: 0.476 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: The Mythos of Model Interpretability\n",
      "Cos similarity score: 0.475 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Sentiment Analysis: An Overview from Linguistics\n",
      "Cos similarity score: 0.475 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: The Semantic Proto-Role Linking Model\n",
      "Cos similarity score: 0.474 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Joint Image Clustering and Labeling by Matrix Factorization\n",
      "Cos similarity score: 0.473 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Deep Embedding Network for Clustering\n",
      "Cos similarity score: 0.473 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: A framework for syntactic translation\n",
      "Cos similarity score: 0.473 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Introduction to Arabic Natural Language Processing\n",
      "Cos similarity score: 0.472 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Robust Classification with Convolutional Prototype Learning\n",
      "Cos similarity score: 0.470 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Care Episode Retrieval\n",
      "Cos similarity score: 0.470 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: An efficient prototype merging strategy for the condensed 1-NN rule through class-conditional hierarchical clustering\n",
      "Cos similarity score: 0.470 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: An Efficient Algorithm for Projective Dependency Parsing\n",
      "Cos similarity score: 0.469 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Theoretical aspects of mechanical speech recognition\n",
      "Cos similarity score: 0.468 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: HOME: High-Order Mixed-Moment-based Embedding for Representation Learning\n",
      "Cos similarity score: 0.468 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Improving Unsupervised Image Clustering With Robust Learning\n",
      "Cos similarity score: 0.468 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Gate Activation Signal Analysis for Gated Recurrent Neural Networks and its Correlation with Phoneme Boundaries\n",
      "Cos similarity score: 0.466 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Why Clowns Taste Funny: The Relationship between Humor and Semantic Ambiguity\n",
      "Cos similarity score: 0.465 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: The advantages and challenges of “big data”: Insights from the 14 billion word iWeb corpus\n",
      "Cos similarity score: 0.464 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: DeepCluster: A General Clustering Framework Based on Deep Learning\n",
      "Cos similarity score: 0.463 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Modeling Consumer Buying Decision for Recommendation Based on Multi-Task Deep Learning\n",
      "Cos similarity score: 0.463 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: SemEval 2015 Task 18: Broad-Coverage Semantic Dependency Parsing\n",
      "Cos similarity score: 0.463 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Thematic role properties of subjects and objects\n",
      "Cos similarity score: 0.462 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Does label smoothing mitigate label noise?\n",
      "Cos similarity score: 0.462 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Why I like it: multi-task learning for recommendation and explanation\n",
      "Cos similarity score: 0.461 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Cluster Ensembles --- A Knowledge Reuse Framework for Combining Multiple Partitions\n",
      "Cos similarity score: 0.460 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Bridging Nonlinearities and Stochastic Regularizers with Gaussian Error Linear Units\n",
      "Cos similarity score: 0.459 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Neural GPUs Learn Algorithms\n",
      "Cos similarity score: 0.458 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Simple Black-Box Adversarial Attacks on Deep Neural Networks\n",
      "Cos similarity score: 0.456 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Fooling End-To-End Speaker Verification With Adversarial Examples\n",
      "Cos similarity score: 0.452 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: The design and operation of the mechanical speech recognizer at University College London\n",
      "Cos similarity score: 0.451 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Visualisation and ‘diagnostic classifiers’ reveal how recurrent and recursive neural networks process hierarchical structure\n",
      "Cos similarity score: 0.450 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Segmentation for English-to-Arabic Statistical Machine Translation\n",
      "Cos similarity score: 0.448 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Self-Supervised Convolutional Subspace Clustering Network\n",
      "Cos similarity score: 0.448 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Neural Multi-task Recommendation from Multi-behavior Data\n",
      "Cos similarity score: 0.447 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Clustering-friendly Representation Learning via Instance Discrimination and Feature Decorrelation\n",
      "Cos similarity score: 0.445 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: The experiencer as an agent\n",
      "Cos similarity score: 0.445 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Colorful Image Colorization\n",
      "Cos similarity score: 0.445 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Audio Adversarial Examples: Targeted Attacks on Speech-to-Text\n",
      "Cos similarity score: 0.442 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Locality Preserving Nonnegative Matrix Factorization\n",
      "Cos similarity score: 0.439 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Language, Consciousness, Culture: Essays on Mental Structure\n",
      "Cos similarity score: 0.439 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Crafting adversarial input sequences for recurrent neural networks\n",
      "Cos similarity score: 0.436 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Models for the Semantic Classification of Noun Phrases\n",
      "Cos similarity score: 0.435 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Explanation in Artificial Intelligence: Insights from the Social Sciences\n",
      "Cos similarity score: 0.434 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Core knowledge.     \n",
      "Cos similarity score: 0.432 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Deep Adversarial Subspace Clustering\n",
      "Cos similarity score: 0.431 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Impersonal Passives and the Unaccusative Hypothesis\n",
      "Cos similarity score: 0.430 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: From Understanding Computation to Understanding Neural Circuitry\n",
      "Cos similarity score: 0.424 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Speaker-independent phone recognition using hidden Markov models\n",
      "Cos similarity score: 0.424 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Some methods for classification and analysis of multivariate observations\n",
      "Cos similarity score: 0.422 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Semi-Supervised Structured Subspace Learning for Multi-View Clustering\n",
      "Cos similarity score: 0.419 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Transferability in Machine Learning: from Phenomena to Black-Box Attacks using Adversarial Samples\n",
      "Cos similarity score: 0.416 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Semantic Types of Some Generic Relation Arguments: Detection and Evaluation\n",
      "Cos similarity score: 0.416 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Accountability of AI Under the Law: The Role of Explanation\n",
      "Cos similarity score: 0.411 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Delving into Transferable Adversarial Examples and Black-box Attacks\n",
      "Cos similarity score: 0.408 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Neural Network Classifiers for Speech Recognition\n",
      "Cos similarity score: 0.408 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: A comparison of informal and formal acceptability judgments using a random sample from Linguistic Inquiry 2001--2010\n",
      "Cos similarity score: 0.406 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: A common misapplication of statistical inference: Nuisance control with null-hypothesis significance tests\n",
      "Cos similarity score: 0.405 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Practical Black-Box Attacks against Machine Learning\n",
      "Cos similarity score: 0.405 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: An approach to computer speech recognition by direct analysis of the speech wave\n",
      "Cos similarity score: 0.404 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Backpropagation Through Time: What It Does and How to Do It\n",
      "Cos similarity score: 0.403 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: A Maximum Likelihood Approach to Continuous Speech Recognition\n",
      "Cos similarity score: 0.402 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: iCmSC: Incomplete Cross-Modal Subspace Clustering\n",
      "Cos similarity score: 0.401 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: On comparing partitions\n",
      "Cos similarity score: 0.401 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Design sensitivity and statistical power in acceptability judgment experiments\n",
      "Cos similarity score: 0.400 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\n",
      "Cos similarity score: 0.395 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Fast Agglomerative Clustering Using a k-Nearest Neighbor Graph\n",
      "Cos similarity score: 0.395 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Joint Learning of Latent Similarity and Local Embedding for Multi-View Clustering\n",
      "Cos similarity score: 0.389 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Using the Framework \n",
      "Cos similarity score: 0.389 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Using Machine Teaching to Identify Optimal Training-Set Attacks on Machine Learners\n",
      "Cos similarity score: 0.387 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\n",
      "Cos similarity score: 0.385 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\n",
      "Cos similarity score: 0.381 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: The reliability of acceptability judgments across languages\n",
      "Cos similarity score: 0.381 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: The Relationships Among Various Nonnegative Matrix Factorization Methods for Clustering\n",
      "Cos similarity score: 0.380 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\n",
      "Cos similarity score: 0.379 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Transformer-XL: Language Modeling with Longer-Term Dependency\n",
      "Cos similarity score: 0.372 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Agglomerative clustering using the concept of mutual nearest neighbourhood\n",
      "Cos similarity score: 0.372 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: History and Development of Speech Recognition\n",
      "Cos similarity score: 0.370 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Analyzing Ordinal Data with Metric Models: What Could Possibly Go Wrong?\n",
      "Cos similarity score: 0.369 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Open Source Toolkit for Statistical Machine Translation: Factored Translation Models and Lattice Decoding\n",
      "Cos similarity score: 0.365 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: A multi-prototype clustering algorithm\n",
      "Cos similarity score: 0.365 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Poisoning Attacks against Support Vector Machines\n",
      "Cos similarity score: 0.360 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Recognition of consonant based on the perceptron model\n",
      "Cos similarity score: 0.350 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: A Statistical Approach to Machine Translation\n",
      "Cos similarity score: 0.321 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: The effect of rating scale format on response styles: the number of response categories and response catgory labels\n",
      "Cos similarity score: 0.307 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Stative Adjectives and Verbs in English\n",
      "Cos similarity score: 0.302 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: In other words. A coursebook on translation [Review of: M. Bakker (1994) -]\n",
      "Cos similarity score: 0.300 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: On Spectral Clustering: Analysis and an algorithm\n",
      "Cos similarity score: 0.293 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: A lightweight weakly supervised learning segmentation algorithm for imbalanced image based on rotation density peaks\n",
      "Cos similarity score: 0.270 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Categorical Data Analysis\n",
      "Cos similarity score: 0.259 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Challenges for Transparency\n",
      "Cos similarity score: 0.248 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Summary and Future Directions.\n",
      "Cos similarity score: 0.248 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: “Cloze Procedure”: A New Tool for Measuring Readability\n",
      "Cos similarity score: 0.247 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Background to Framenet\n",
      "Cos similarity score: 0.225 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: An Overview of Vulnerabilities of Voice Controlled Systems\n",
      "Cos similarity score: 0.210 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Advances in Neural Information Processing Systems 27\n",
      "Cos similarity score: 0.207 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Competition in argument interpretation: Evidence from the neurobiology of language\n",
      "Cos similarity score: 0.202 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Brief History of Automatic Speech Recognition\n",
      "Cos similarity score: 0.200 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Interpolated estimation of Markov source parameters from sparse data\n",
      "Cos similarity score: 0.182 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Language and Cognitive Processes\n",
      "Cos similarity score: 0.181 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: References          \n",
      "Cos similarity score: 0.174 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Minimum Classification Error (MCE) Approach in Pattern Recognition\n",
      "Cos similarity score: 0.130 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: The Language of Riddles: New Perspectives\n",
      "Cos similarity score: 0.126 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: Women, Fire, and Dangerous Things\n",
      "Cos similarity score: 0.091 and it's \u001b[31mnot cited in \u001b[0mthe main paper with title: LoPar: Design and Implementation\n",
      "[[tensor(1.), 'BERT Rediscovers the Classical NLP Pipeline'], [tensor(0.8000), 'Semi-supervised sequence tagging with bidirectional language models'], [tensor(0.7896), 'End-to-end Neural Coreference Resolution'], [tensor(0.7824), 'Inference is Everything: Recasting Semantic Resources into a Unified Evaluation Framework'], [tensor(0.7815), 'LSTM CCG Parsing'], [tensor(0.7809), 'Constituency Parsing with a Self-Attentive Encoder'], [tensor(0.7784), 'Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond'], [tensor(0.7776), 'A Joint Many-Task Model: Growing a Neural Network for Multiple NLP Tasks'], [tensor(0.7756), 'context2vec: Learning Generic Context Embedding with Bidirectional LSTM'], [tensor(0.7731), 'Tailoring Continuous Word Representations for Dependency Parsing'], [tensor(0.7731), 'Universal Language Model Fine-tuning for Text Classification'], [tensor(0.7720), 'Bottom-Up Abstractive Summarization'], [tensor(0.7714), 'Learning Global Features for Coreference Resolution'], [tensor(0.7690), 'Deep Semantic Role Labeling: What Works and What’s Next'], [tensor(0.7655), 'Semantic Parsing via Paraphrasing'], [tensor(0.7583), 'Dissecting Contextual Word Embeddings: Architecture and Representation'], [tensor(0.7554), 'Deep Contextualized Word Representations'], [tensor(0.7522), 'Neural Sequence Learning Models for Word Sense Disambiguation'], [tensor(0.7518), 'Learning Distributed Representations of Sentences from Unlabelled Data'], [tensor(0.7518), 'Semantic Tagging with Deep Residual Networks'], [tensor(0.7508), 'What do you learn from context? Probing for sentence structure in contextualized word representations'], [tensor(0.7499), 'Deep Semantic Role Labeling with Self-Attention'], [tensor(0.7492), 'AllenNLP: A Deep Semantic Natural Language Processing Platform'], [tensor(0.7484), 'On the Evaluation of Semantic Phenomena in Neural Machine Translation Using Natural Language Inference'], [tensor(0.7482), 'Character-Level Question Answering with Attention'], [tensor(0.7471), 'Language Models are Unsupervised Multitask Learners'], [tensor(0.7457), 'A large annotated corpus for learning natural language inference'], [tensor(0.7447), 'Embeddings for Word Sense Disambiguation: An Evaluation Study'], [tensor(0.7445), 'A Neural Network for Factoid Question Answering over Paragraphs'], [tensor(0.7398), 'Jointly Predicting Predicates and Arguments in Neural Semantic Role Labeling'], [tensor(0.7395), 'Story Cloze Task: UW NLP System'], [tensor(0.7390), 'Exploring the Syntactic Abilities of RNNs with Multi-task Learning'], [tensor(0.7389), 'Skip-Thought Vectors'], [tensor(0.7386), 'GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding'], [tensor(0.7384), 'Bidirectional Attention Flow for Machine Comprehension'], [tensor(0.7382), 'Frame-Semantic Parsing with Softmax-Margin Segmental RNNs and a Syntactic Scaffold'], [tensor(0.7380), 'Efficient Contextualized Representation: Language Model Pruning for Sequence Labeling'], [tensor(0.7369), 'Supervised Learning of Universal Sentence Representations from Natural Language Inference Data'], [tensor(0.7367), 'The LAMBADA dataset: Word prediction requiring a broad discourse context'], [tensor(0.7359), 'SemEval-2007 Task 04: Classification of Semantic Relations between Nominals'], [tensor(0.7354), 'LSTMs Can Learn Syntax-Sensitive Dependencies Well, But Modeling Structure Makes Them Better'], [tensor(0.7349), 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding'], [tensor(0.7348), 'Learned in Translation: Contextualized Word Vectors'], [tensor(0.7346), 'Collecting Diverse Natural Language Inference Problems for Sentence Representation Evaluation'], [tensor(0.7346), 'Deep RNNs Encode Soft Hierarchical Syntax'], [tensor(0.7345), 'Extending a Parser to Distant Domains Using a Few Dozen Partially Annotated Examples'], [tensor(0.7335), 'Recognizing textual entailment: Rational, evaluation and approaches'], [tensor(0.7329), 'Neural-Davidsonian Semantic Proto-role Labeling'], [tensor(0.7324), 'End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF'], [tensor(0.7312), 'Deep Biaffine Attention for Neural Dependency Parsing'], [tensor(0.7310), 'Entity Tracking Improves Cloze-style Reading Comprehension'], [tensor(0.7298), 'What Syntax Can Contribute in the Entailment Task'], [tensor(0.7298), 'Fast and Accurate Shift-Reduce Constituent Parsing'], [tensor(0.7297), 'Get To The Point: Summarization with Pointer-Generator Networks'], [tensor(0.7294), 'Neural Machine Translation with Source-Side Latent Graph Parsing'], [tensor(0.7289), 'Multilingual Language Processing From Bytes'], [tensor(0.7280), 'Natural Language Processing (Almost) from Scratch'], [tensor(0.7277), 'Word Sense Disambiguation: A Unified Evaluation Framework and Empirical Comparison'], [tensor(0.7273), 'Word Representations: A Simple and General Method for Semi-Supervised Learning'], [tensor(0.7273), 'Assessing the Ability of LSTMs to Learn Syntax-Sensitive Dependencies'], [tensor(0.7272), 'Hypothesis Only Baselines in Natural Language Inference'], [tensor(0.7271), 'Language Modeling Teaches You More than Translation Does: Lessons Learned Through Auxiliary Syntactic Task Analysis'], [tensor(0.7268), 'Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection'], [tensor(0.7267), 'A Deep Reinforced Model for Abstractive Summarization'], [tensor(0.7264), 'Resolving Complex Cases of Definite Pronouns: The Winograd Schema Challenge'], [tensor(0.7258), 'Recursive deep learning for natural language processing and computer vision'], [tensor(0.7255), 'On the State of the Art of Evaluation in Neural Language Models'], [tensor(0.7252), 'End-to-end learning of semantic role labeling using recurrent neural networks'], [tensor(0.7250), 'Visualizing and Understanding Neural Models in NLP'], [tensor(0.7246), 'A Hierarchical Neural Autoencoder for Paragraphs and Documents'], [tensor(0.7245), 'Improving Language Understanding by Generative Pre-Training'], [tensor(0.7245), 'A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference'], [tensor(0.7243), 'Feature-Rich Part-of-Speech Tagging with a Cyclic Dependency Network'], [tensor(0.7240), 'What’s Going On in Neural Constituency Parsers? An Analysis'], [tensor(0.7240), 'Enhanced LSTM for Natural Language Inference'], [tensor(0.7238), 'Towards Robust Linguistic Analysis using OntoNotes'], [tensor(0.7238), 'Pointer Sentinel Mixture Models'], [tensor(0.7227), 'XNLI: Evaluating Cross-lingual Sentence Representations'], [tensor(0.7225), 'Language Modeling with Gated Convolutional Networks'], [tensor(0.7223), 'A Simple Method for Commonsense Reasoning'], [tensor(0.7215), 'Stress Test Evaluation for Natural Language Inference'], [tensor(0.7212), 'Higher-Order Coreference Resolution with Coarse-to-Fine Inference'], [tensor(0.7212), 'Syntactic Contributions in the Entailment Task: an implementation'], [tensor(0.7209), 'Investigating the Usefulness of Generalized Word Representations in SMT'], [tensor(0.7209), 'A Convolutional Neural Network for Modelling Sentences'], [tensor(0.7207), 'Analyzing and Integrating Dependency Parsers'], [tensor(0.7204), 'Generating Entailment Rules from FrameNet'], [tensor(0.7204), 'Unsupervised Pretraining for Sequence to Sequence Learning'], [tensor(0.7204), 'Learning Character-level Representations for Part-of-Speech Tagging'], [tensor(0.7204), 'Evaluating Compositionality in Sentence Embeddings'], [tensor(0.7199), 'SentEval: An Evaluation Toolkit for Universal Sentence Representations'], [tensor(0.7186), 'Target-side Word Segmentation Strategies for Neural Machine Translation'], [tensor(0.7171), 'Classification of semantic relations between nominals'], [tensor(0.7171), 'What do Neural Machine Translation Models Learn about Morphology?'], [tensor(0.7166), 'Learning and Evaluating General Linguistic Intelligence'], [tensor(0.7156), 'Handling Homographs in Neural Machine Translation'], [tensor(0.7144), 'SQuAD: 100,000+ Questions for Machine Comprehension of Text'], [tensor(0.7141), 'Generating Wikipedia by Summarizing Long Sequences'], [tensor(0.7137), 'Relation Alignment for Textual Entailment Recognition'], [tensor(0.7136), 'Neural Attention for Learning to Rank Questions in Community Question Answering'], [tensor(0.7132), 'Wizard of Wikipedia: Knowledge-Powered Conversational agents'], [tensor(0.7132), 'Learning Accurate, Compact, and Interpretable Tree Annotation'], [tensor(0.7128), 'Natural Language Multitasking: Analyzing and Improving Syntactic Saliency of Hidden Representations'], [tensor(0.7128), 'Regularizing and Optimizing LSTM Language Models'], [tensor(0.7127), 'CoNLL-2012 Shared Task: Modeling Multilingual Unrestricted Coreference in OntoNotes'], [tensor(0.7126), 'An approach using Named Entities for Recognizing Textual Entailment'], [tensor(0.7125), 'A Neural Conversational Model'], [tensor(0.7123), 'Character-Level Language Modeling with Deeper Self-Attention'], [tensor(0.7122), 'Efficient Non-parametric Estimation of Multiple Embeddings per Word in Vector Space'], [tensor(0.7117), 'Predicting Target Language CCG Supertags Improves Neural Machine Translation'], [tensor(0.7113), 'Trimming and Improving Skip-thought Vectors'], [tensor(0.7112), 'Easy Victories and Uphill Battles in Coreference Resolution'], [tensor(0.7107), 'Breaking NLI Systems with Sentences that Require Simple Lexical Inferences'], [tensor(0.7098), 'EuroSense: Automatic Harvesting of Multilingual Sense Annotations from Parallel Text'], [tensor(0.7092), 'Tree-to-Sequence Attentional Neural Machine Translation'], [tensor(0.7091), 'End-To-End Memory Networks'], [tensor(0.7084), 'Semantic Roles for SMT: A Hybrid Two-Pass Model'], [tensor(0.7080), 'Annotation Artifacts in Natural Language Inference Data'], [tensor(0.7077), 'Maximum entropy models for natural language ambiguity resolution'], [tensor(0.7074), 'An NLP Curator (or: How I Learned to Stop Worrying and Love NLP Pipelines)'], [tensor(0.7071), 'Dialog-based Language Learning'], [tensor(0.7057), 'Exploring the Limits of Language Modeling'], [tensor(0.7053), 'Scalable Inference and Training of Context-Rich Syntactic Translation Models'], [tensor(0.7051), 'Self-Training PCFG Grammars with Latent Annotations Across Languages'], [tensor(0.7044), 'Deep Reinforcement Learning for Mention-Ranking Coreference Models'], [tensor(0.7039), 'Automatic Acquisition of Hyponyms from Large Text Corpora'], [tensor(0.7036), 'Parser Showdown at the Wall Street Corral: An Empirical Investigation of Error Types in Parser Output'], [tensor(0.7036), 'Rationalizing Neural Predictions'], [tensor(0.7031), 'Multilingual Dependency Analysis with a Two-Stage Discriminative Parser'], [tensor(0.7029), 'Overview of the 2012 Shared Task on Parsing the Web'], [tensor(0.7012), 'Self-Adaptive Hierarchical Sentence Model'], [tensor(0.7010), 'Reading Wikipedia to Answer Open-Domain Questions'], [tensor(0.7007), 'Multi-Task Deep Neural Networks for Natural Language Understanding'], [tensor(0.7005), 'Large-Scale Syntactic Language Modeling with Treelets'], [tensor(0.7004), 'Advances in Pre-Training Distributed Word Representations'], [tensor(0.6998), 'Don’t count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors'], [tensor(0.6998), 'Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank'], [tensor(0.6995), 'Semantic Proto-Role Labeling'], [tensor(0.6994), 'Visualizing and Understanding Neural Machine Translation'], [tensor(0.6987), 'How Robust Are Character-Based Word Embeddings in Tagging and MT Against Wrod Scramlbing or Randdm Nouse?'], [tensor(0.6983), 'Zero-Shot Cross-lingual Classification Using Multilingual Neural Machine Translation'], [tensor(0.6983), 'A Compositional and Interpretable Semantic Space'], [tensor(0.6982), 'Improving SMT quality with morpho-syntactic analysis'], [tensor(0.6979), 'FrameNet+: Fast Paraphrastic Tripling of FrameNet'], [tensor(0.6978), 'Grammar as a Foreign Language'], [tensor(0.6975), 'Semi-supervised Sequence Learning'], [tensor(0.6967), 'On the Evaluation of Common-Sense Reasoning in Natural Language Understanding'], [tensor(0.6966), 'The Proposition Bank: An Annotated Corpus of Semantic Roles'], [tensor(0.6963), 'Sharp Nearby, Fuzzy Far Away: How Neural Language Models Use Context'], [tensor(0.6963), 'Multilingual Models for Compositional Distributed Semantics'], [tensor(0.6960), 'Character-based Neural Machine Translation'], [tensor(0.6959), 'Deterministic Coreference Resolution Based on Entity-Centric, Precision-Ranked Rules'], [tensor(0.6955), 'Extractive Summarization using Continuous Vector Space Models'], [tensor(0.6954), 'What Do Recurrent Neural Network Grammars Learn About Syntax?'], [tensor(0.6953), 'Fine-grained Analysis of Sentence Embeddings Using Auxiliary Prediction Tasks'], [tensor(0.6951), 'Visualizing and Understanding Recurrent Networks'], [tensor(0.6948), 'Can Semantic Role Labeling Improve SMT?'], [tensor(0.6947), 'When Are Tree Structures Necessary for Deep Learning of Representations?'], [tensor(0.6946), 'Natural language inference'], [tensor(0.6938), 'Massively Multilingual Word Embeddings'], [tensor(0.6937), 'Ask Me Anything: Dynamic Memory Networks for Natural Language Processing'], [tensor(0.6930), 'Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond'], [tensor(0.6928), 'Natural Language Inference over Interaction Space'], [tensor(0.6927), 'Event Detection and Factuality Assessment with Non-Expert Supervision'], [tensor(0.6926), 'Phrase-Based & Neural Unsupervised Machine Translation'], [tensor(0.6926), 'Evaluating Discourse Phenomena in Neural Machine Translation'], [tensor(0.6920), 'Word Representation Models for Morphologically Rich Languages in Neural Machine Translation'], [tensor(0.6912), 'A Simple but Tough-to-Beat Baseline for Sentence Embeddings'], [tensor(0.6911), 'Towards String-To-Tree Neural Machine Translation'], [tensor(0.6909), 'No Training Required: Exploring Random Encoders for Sentence Classification'], [tensor(0.6906), 'Simple and Effective Multi-Paragraph Reading Comprehension'], [tensor(0.6903), 'Ordinal Common-sense Inference'], [tensor(0.6903), 'Analysis of sentence embedding models using prediction tasks in natural language processing'], [tensor(0.6898), 'The Prague Dependency Treebank'], [tensor(0.6893), 'Unsupervised Machine Translation Using Monolingual Corpora Only'], [tensor(0.6892), 'Interpreting Neural Networks to Improve Politeness Comprehension'], [tensor(0.6891), 'A Convolutional Encoder Model for Neural Machine Translation'], [tensor(0.6890), 'Named Entity Recognition with Bidirectional LSTM-CNNs'], [tensor(0.6885), 'Knowledge-Based Weak Supervision for Information Extraction of Overlapping Relations'], [tensor(0.6885), 'The Stanford CoreNLP Natural Language Processing Toolkit'], [tensor(0.6884), 'An overview of Natural Language Inference Data Collection: The way forward?'], [tensor(0.6877), 'Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning'], [tensor(0.6874), 'A Neural Reordering Model for Phrase-based Translation'], [tensor(0.6873), 'A Class-Based Agreement Model for Generating Accurately Inflected Translations'], [tensor(0.6872), 'Effective Approaches to Attention-based Neural Machine Translation'], [tensor(0.6869), 'The Importance of Syntactic Parsing and Inference in Semantic Role Labeling'], [tensor(0.6866), 'Language Modeling Teaches You More than Translation Does : Lessons Learned Through Auxiliary Task Analysis'], [tensor(0.6863), 'Universal Stanford dependencies: A cross-linguistic typology'], [tensor(0.6859), 'SemEval-2010 Task 8: Multi-Way Classification of Semantic Relations Between Pairs of Nominals'], [tensor(0.6857), 'Learning to Generate Reviews and Discovering Sentiment'], [tensor(0.6853), 'Linguistic Input Features Improve Neural Machine Translation'], [tensor(0.6851), \"Looking for ELMo's friends: Sentence-Level Pretraining Beyond Language Modeling\"], [tensor(0.6850), 'Learning Effective and Interpretable Semantic Models using Non-Negative Sparse Embedding'], [tensor(0.6846), 'Parser Evaluation over Local and Non-Local Deep Dependencies in a Large Corpus'], [tensor(0.6844), 'What you can cram into a single $&!#* vector: Probing sentence embeddings for linguistic properties'], [tensor(0.6844), 'An Effective Approach to Unsupervised Machine Translation'], [tensor(0.6844), 'A Model of Coherence Based on Distributed Sentence Representation'], [tensor(0.6842), 'Sequence to Sequence Learning with Neural Networks'], [tensor(0.6837), 'Beyond Word Importance: Contextual Decomposition to Extract Interactions from LSTMs'], [tensor(0.6836), 'Transformer-XL: Attentive Language Models beyond a Fixed-Length Context'], [tensor(0.6828), 'A Linguistic Evaluation of Rule-Based, Phrase-Based, and Neural MT Engines'], [tensor(0.6828), 'Investigating Language Universal and Specific Properties in Word Embeddings'], [tensor(0.6826), 'TransferTransfo: A Transfer Learning Approach for Neural Network Based Conversational Agents'], [tensor(0.6824), 'The Groningen Meaning Bank'], [tensor(0.6823), 'Character-Aware Neural Language Models'], [tensor(0.6820), 'The Third PASCAL Recognizing Textual Entailment Challenge'], [tensor(0.6818), 'Structured Attention Networks'], [tensor(0.6817), 'Assessing Composition in Sentence Vector Representations'], [tensor(0.6816), 'VectorSLU: A Continuous Word Vector Approach to Answer Selection in Community Question Answering Systems'], [tensor(0.6816), 'Extracting implicit knowledge from text'], [tensor(0.6815), 'Neural Tree Indexers for Text Understanding'], [tensor(0.6813), 'Gated Self-Matching Networks for Reading Comprehension and Question Answering'], [tensor(0.6812), 'Empirical Methods for Compound Splitting'], [tensor(0.6810), 'Learning Continuous Phrase Representations and Syntactic Parsing with Recursive Neural Networks'], [tensor(0.6802), 'NLP with BERT: Sentiment Analysis Using SAS® Deep Learning and DLPy'], [tensor(0.6798), 'A unified architecture for natural language processing: deep neural networks with multitask learning'], [tensor(0.6796), 'Understanding Neural Networks through Representation Erasure'], [tensor(0.6795), 'Dependency Treelet Translation: Syntactically Informed Phrasal SMT'], [tensor(0.6784), 'A SICK cure for the evaluation of compositional distributional semantic models'], [tensor(0.6780), 'Non-Autoregressive Neural Machine Translation'], [tensor(0.6777), 'Deep multi-task learning with low level tasks supervised at lower layers'], [tensor(0.6768), 'Cross-lingual Language Model Pretraining'], [tensor(0.6767), 'Non-Projective Parsing for Statistical Machine Translation'], [tensor(0.6763), 'ClearTK 2.0: Design Patterns for Machine Learning in UIMA'], [tensor(0.6763), 'More Constructions, More Genres: Extending Stanford Dependencies'], [tensor(0.6762), 'A Challenge Set Approach to Evaluating Machine Translation'], [tensor(0.6759), 'Neural Machine Translation'], [tensor(0.6755), 'Utilizing Target-Side Semantic Role Labels to Assist Hierarchical Phrase-based Machine Translation'], [tensor(0.6754), 'Colorless Green Recurrent Networks Dream Hierarchically'], [tensor(0.6751), 'Grounded Compositional Semantics for Finding and Describing Images with Sentences'], [tensor(0.6748), 'An Empirical Examination of Challenges in Chinese Parsing'], [tensor(0.6741), 'Determining Modality and Factuality for Text Entailment'], [tensor(0.6737), 'Exploring Word Sense Disambiguation Abilities of Neural Machine Translation Systems (Non-archival Extended Abstract)'], [tensor(0.6737), 'Multi-task Sequence to Sequence Learning'], [tensor(0.6736), 'Coarse-to-Fine n-Best Parsing and MaxEnt Discriminative Reranking'], [tensor(0.6734), 'Distributed representations, simple recurrent networks, and grammatical structure'], [tensor(0.6733), '\"What is relevant in a text document?\": An interpretable machine learning approach'], [tensor(0.6731), 'Rationale-Augmented Convolutional Neural Networks for Text Classification'], [tensor(0.6729), 'A BERT Baseline for the Natural Questions'], [tensor(0.6729), 'Learning Noun-Modifier Semantic Relations with Corpus-based and WordNet-based Features'], [tensor(0.6726), 'Evaluating Layers of Representation in Neural Machine Translation on Part-of-Speech and Semantic Tagging Tasks'], [tensor(0.6726), 'Espresso: Leveraging Generic Patterns for Automatically Harvesting Semantic Relations'], [tensor(0.6720), 'A Scalable Hierarchical Distributed Language Model'], [tensor(0.6719), 'Modeling Inflection and Word-Formation in SMT'], [tensor(0.6718), '11,001 New Features for Statistical Machine Translation'], [tensor(0.6715), 'SemEval-2017 Task 2: Multilingual and Cross-lingual Semantic Word Similarity'], [tensor(0.6704), 'Semantic Role Features for Machine Translation'], [tensor(0.6701), 'Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks'], [tensor(0.6700), 'Language processing and learning models for community question answering in Arabic'], [tensor(0.6699), 'Recursive Deep Models for Discourse Parsing'], [tensor(0.6696), 'Targeted Syntactic Evaluation of Language Models'], [tensor(0.6695), 'Jointly optimizing word representations for lexical and sentential tasks with the C-PHRASE model'], [tensor(0.6694), 'MEANTIME, the NewsReader Multilingual Event and Time Corpus'], [tensor(0.6693), 'Unsupervised Neural Machine Translation'], [tensor(0.6688), 'Understanding and Improving Morphological Learning in the Neural Machine Translation Decoder'], [tensor(0.6686), 'CCGbank: A Corpus of CCG Derivations and Dependency Structures Extracted from the Penn Treebank'], [tensor(0.6683), 'Analyzing Linguistic Knowledge in Sequential Model of Sentence'], [tensor(0.6682), 'A Fast and Accurate Dependency Parser using Neural Networks'], [tensor(0.6680), 'Visually Grounded Learning of Keyword Prediction from Untranscribed Speech'], [tensor(0.6671), 'Extracting Pre-ordering Rules from Predicate-Argument Structures'], [tensor(0.6669), 'Identifying Relations for Open Information Extraction'], [tensor(0.6668), 'Effective Self-Training for Parsing'], [tensor(0.6668), 'A connectionist approach to machine translation'], [tensor(0.6667), 'The Microsoft Research Sentence Completion Challenge'], [tensor(0.6657), 'Edinburgh Neural Machine Translation Systems for WMT 16'], [tensor(0.6654), 'Massive Exploration of Neural Machine Translation Architectures'], [tensor(0.6654), 'Long Short-Term Memory-Networks for Machine Reading'], [tensor(0.6648), 'GloVe: Global Vectors for Word Representation'], [tensor(0.6644), 'Fully Character-Level Neural Machine Translation without Explicit Segmentation'], [tensor(0.6642), 'Challenging Language-Dependent Segmentation for Arabic: An Application to Machine Translation and Part-of-Speech Tagging'], [tensor(0.6633), 'Supertagging: An Approach to Almost Parsing'], [tensor(0.6628), 'The Importance of Being Recurrent for Modeling Hierarchical Structure'], [tensor(0.6619), 'Open Language Learning for Information Extraction'], [tensor(0.6618), 'Synthetic and Natural Noise Both Break Neural Machine Translation'], [tensor(0.6617), 'Enriching Word Vectors with Subword Information'], [tensor(0.6617), 'The representational geometry of word meanings acquired by neural machine translation models'], [tensor(0.6616), 'From Characters to Words to in Between: Do We Capture Morphology?'], [tensor(0.6615), 'An Arabic-Hebrew parallel corpus of TED talks'], [tensor(0.6612), 'Unbounded Dependency Recovery for Parser Evaluation'], [tensor(0.6608), 'Producing Unseen Morphological Variants in Statistical Machine Translation'], [tensor(0.6602), 'Exploring Compositional Architectures and Word Vector Representations for Prepositional Phrase Attachment'], [tensor(0.6599), 'Recurrent Continuous Translation Models'], [tensor(0.6592), 'Building a Large Annotated Corpus of English: The Penn Treebank'], [tensor(0.6590), 'Attention is All you Need'], [tensor(0.6587), 'Revisiting Visual Question Answering Baselines'], [tensor(0.6580), 'Multitask Learning with Low-Level Auxiliary Tasks for Encoder-Decoder Based Speech Recognition'], [tensor(0.6577), 'Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation'], [tensor(0.6577), 'UIMA: an architectural approach to unstructured information processing in the corporate research environment'], [tensor(0.6575), 'Word Sense Disambiguation Improves Statistical Machine Translation'], [tensor(0.6574), 'Distant supervision for relation extraction without labeled data'], [tensor(0.6569), 'Classifying Relations by Ranking with Convolutional Neural Networks'], [tensor(0.6567), 'Evaluating the morphological competence of Machine Translation Systems'], [tensor(0.6563), 'State-of-the-Art Speech Recognition with Sequence-to-Sequence Models'], [tensor(0.6561), 'Travatar: A Forest-to-String Machine Translation Engine based on Tree Transducers'], [tensor(0.6559), \"The Goldilocks Principle: Reading Children's Books with Explicit Memory Representations\"], [tensor(0.6552), 'A fast and simple algorithm for training neural probabilistic language models'], [tensor(0.6551), 'SemEval-2017 Task 7: Detection and Interpretation of English Puns'], [tensor(0.6543), 'Comparing Representations of Semantic Roles for String-To-Tree Decoding'], [tensor(0.6538), 'Distributional Semantics Beyond Words: Supervised Learning of Analogy and Paraphrase'], [tensor(0.6533), 'Generating Complex Morphology for Machine Translation'], [tensor(0.6532), 'A Conditional Random Field Word Segmenter for Sighan Bakeoff 2005'], [tensor(0.6532), 'Representation of Linguistic Form and Function in Recurrent Neural Networks'], [tensor(0.6530), 'Does String-Based Neural MT Learn Source Syntax?'], [tensor(0.6525), 'Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models'], [tensor(0.6523), 'Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer'], [tensor(0.6515), 'The Necessity of Parsing for Predicate Argument Recognition'], [tensor(0.6512), 'One billion word benchmark for measuring progress in statistical language modeling'], [tensor(0.6509), \"Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation\"], [tensor(0.6508), 'SciTaiL: A Textual Entailment Dataset from Science Question Answering'], [tensor(0.6507), 'Stochastic Answer Networks for Machine Reading Comprehension'], [tensor(0.6507), 'The Disambiguation of Nominalisations'], [tensor(0.6504), 'Linguistic Regularities in Continuous Space Word Representations'], [tensor(0.6503), 'Learning to Parse and Translate Improves Neural Machine Translation'], [tensor(0.6502), 'Probabilistic Top-Down Parsing and Language Modeling'], [tensor(0.6502), 'Dependency Grammar and Dependency Parsing'], [tensor(0.6497), 'Fully convolutional networks for semantic segmentation'], [tensor(0.6492), 'Max-Margin Tensor Neural Network for Chinese Word Segmentation'], [tensor(0.6486), 'Using “Annotator Rationales” to Improve Machine Learning for Text Categorization'], [tensor(0.6483), 'SUTime: A library for recognizing and normalizing time expressions'], [tensor(0.6480), 'Multi-Task Video Captioning with Video and Entailment Generation'], [tensor(0.6477), 'Hierarchical Probabilistic Neural Network Language Model'], [tensor(0.6469), 'Neural Machine Translation with Source Dependency Representation'], [tensor(0.6467), 'CCG Supertags in Factored Statistical Machine Translation'], [tensor(0.6462), 'Google’s Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation'], [tensor(0.6459), 'Statistical Machine Translation'], [tensor(0.6457), 'On internal language representations in deep learning: an analysis of machine translation and speech recognition'], [tensor(0.6455), 'Deep Recurrent Models with Fast-Forward Connections for Neural Machine Translation'], [tensor(0.6452), 'An Empirical Exploration of Recurrent Network Architectures'], [tensor(0.6451), 'Improving Statistical Machine Translation Using Word Sense Disambiguation'], [tensor(0.6450), 'Distributed Representations of Words and Phrases and their Compositionality'], [tensor(0.6448), 'Neural Machine Translation of Rare Words with Subword Units'], [tensor(0.6442), 'Europarl: A Parallel Corpus for Statistical Machine Translation'], [tensor(0.6440), 'Automatic disambiguation of English puns'], [tensor(0.6438), 'Word Translation Without Parallel Data'], [tensor(0.6438), 'OntoNotes: The 90% Solution'], [tensor(0.6434), 'On the difficulty of a distributional semantics of spoken language'], [tensor(0.6429), 'Reading Tea Leaves: How Humans Interpret Topic Models'], [tensor(0.6429), 'Overview of BioNLP’09 Shared Task on Event Extraction'], [tensor(0.6423), 'Semantic Compositionality through Recursive Matrix-Vector Spaces'], [tensor(0.6419), 'Continuous Space Translation Models for Phrase-Based Statistical Machine Translation'], [tensor(0.6418), 'Charagram: Embedding Words and Sentences via Character n-grams'], [tensor(0.6415), 'Text Classification Improved by Integrating Bidirectional LSTM with Two-dimensional Max Pooling'], [tensor(0.6411), 'Using the Output Embedding to Improve Language Models'], [tensor(0.6410), 'Judging Grammaticality with Tree Substitution Grammar Derivations'], [tensor(0.6402), 'Distributed Representations of Sentences and Documents'], [tensor(0.6400), 'Hierarchical Neural Story Generation'], [tensor(0.6395), 'EESEN: End-to-end speech recognition using deep RNN models and WFST-based decoding'], [tensor(0.6395), 'Learning New Semi-Supervised Deep Auto-encoder Features for Statistical Machine Translation'], [tensor(0.6392), 'End-to-end attention-based large vocabulary speech recognition'], [tensor(0.6390), 'Optimizing Chinese Word Segmentation for Machine Translation Performance'], [tensor(0.6390), 'WIT3: Web Inventory of Transcribed and Translated Talks'], [tensor(0.6388), 'Morphology-aware statistical machine translation based on morphs induced in an unsupervised manner'], [tensor(0.6382), 'Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling'], [tensor(0.6380), 'Connectionist F-structure transfer'], [tensor(0.6375), 'Improved Tree-to-String Transducer for Machine Translation'], [tensor(0.6370), 'What Can Syntax-Based MT Learn from Phrase-Based MT?'], [tensor(0.6367), 'Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks'], [tensor(0.6350), 'Towards the automatic detection and identification of English puns'], [tensor(0.6349), 'Deep architectures for Neural Machine Translation'], [tensor(0.6346), 'What’s in an Embedding? Analyzing Word Embeddings through Multilingual Evaluation'], [tensor(0.6345), 'RECOGNIZING STRONG AND WEAK OPINION CLAUSES'], [tensor(0.6343), 'Neural Network Acceptability Judgments'], [tensor(0.6341), 'Most “babies” are “little” and most “problems” are “huge”: Compositional Entailment in Adjective-Nouns'], [tensor(0.6334), 'Adversarial Examples for Evaluating Reading Comprehension Systems'], [tensor(0.6328), 'Neural Machine Translation by Jointly Learning to Align and Translate'], [tensor(0.6324), 'From Group to Individual Labels Using Deep Features'], [tensor(0.6322), 'Introduction to the CoNLL-2000 Shared Task Chunking'], [tensor(0.6313), 'Classifying the Semantic Relations in Noun Compounds via a Domain-Specific Lexical Hierarchy'], [tensor(0.6306), 'VerbOcean: Mining the Web for Fine-Grained Semantic Verb Relations'], [tensor(0.6304), 'FOIL it! Find One mismatch between Image and Language caption'], [tensor(0.6303), 'CoQA: A Conversational Question Answering Challenge'], [tensor(0.6299), 'How Grammatical is Character-level Neural Machine Translation? Assessing MT Quality with Contrastive Translation Pairs'], [tensor(0.6298), 'FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence'], [tensor(0.6298), 'JU_CSE_TAC: Textual Entailment Recognition System at TAC RTE-6'], [tensor(0.6297), 'Solving Relational Similarity Problems Using the Web as a Corpus'], [tensor(0.6289), 'The Descent of Hierarchy, and Selection in Relational Semantics'], [tensor(0.6276), 'A Structured Self-attentive Sentence Embedding'], [tensor(0.6275), 'Neural Morphological Tagging of Lemma Sequences for Machine Translation'], [tensor(0.6263), 'Unsupervised Cross-lingual Word Embedding by Multilingual Neural Language Models'], [tensor(0.6263), 'The Berkeley FrameNet Project'], [tensor(0.6257), 'Parsing Natural Scenes and Natural Language with Recursive Neural Networks'], [tensor(0.6255), 'Corpus-based Learning of Analogies and Semantic Relations'], [tensor(0.6253), 'Embracing data abundance: BookTest Dataset for Reading Comprehension'], [tensor(0.6253), 'Improving Vector Space Word Representations Using Multilingual Correlation'], [tensor(0.6251), 'Large-Scale Machine Translation between Arabic and Hebrew: Available Corpora and Initial Results'], [tensor(0.6246), 'Hierarchical Phrase-Based Translation'], [tensor(0.6241), 'Natural Questions: A Benchmark for Question Answering Research'], [tensor(0.6241), 'Semantics-Based Machine Translation with Hyperedge Replacement Grammars'], [tensor(0.6237), 'Interpretation of Semantic Tweet Representations'], [tensor(0.6231), 'Neural Machine Translation in Linear Time'], [tensor(0.6227), 'Introduction to the CoNLL-2005 Shared Task: Semantic Role Labeling'], [tensor(0.6226), 'On the Elements of an Accurate Tree-to-String Machine Translation System'], [tensor(0.6226), 'A Character-Aware Encoder for Neural Machine Translation'], [tensor(0.6226), 'Book Review: Natural Language Processing with Python by Steven Bird, Ewan Klein, and Edward Loper'], [tensor(0.6222), 'Who Did What to Whom? A Contrastive Study of Syntacto-Semantic Dependencies'], [tensor(0.6220), 'SemEval-2010 Task 8: Multi-Way Classification of Semantic Relations between Pairs of Nominals'], [tensor(0.6220), 'Phrase reordering for statistical machine translation based on predicate-argument structure'], [tensor(0.6219), 'HotFlip: White-Box Adversarial Examples for NLP'], [tensor(0.6217), 'Axiomatic Attribution for Deep Networks'], [tensor(0.6215), 'Adjoining Tree-to-String Translation'], [tensor(0.6215), 'Listen, Attend and Spell'], [tensor(0.6206), 'Distilling the Knowledge in a Neural Network'], [tensor(0.6205), 'FRAGE: Frequency-Agnostic Word Representation'], [tensor(0.6203), 'A Character-level Convolutional Neural Network for Distinguishing Similar Languages and Dialects'], [tensor(0.6203), 'Hybrid speech recognition with Deep Bidirectional LSTM'], [tensor(0.6199), 'Temporal Ensembling for Semi-Supervised Learning'], [tensor(0.6199), 'Learning Word Vectors for Sentiment Analysis'], [tensor(0.6198), 'Shallow Semantic Trees for SMT'], [tensor(0.6191), 'Learning Word-Like Units from Joint Audio-Visual Analysis'], [tensor(0.6191), 'Factorization tricks for LSTM networks'], [tensor(0.6186), 'Neural Reordering Model Considering Phrase Translation and Word Alignment for Phrase-based Translation'], [tensor(0.6186), 'Acoustic modelling with CD-CTC-SMBR LSTM RNNS'], [tensor(0.6180), 'Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation'], [tensor(0.6179), 'Unsupervised Learning of Visual Representations by Solving Jigsaw Puzzles'], [tensor(0.6173), 'Efficient Estimation of Word Representations in Vector Space'], [tensor(0.6169), 'Evaluation of Dependency Parsers on Unbounded Dependencies'], [tensor(0.6167), 'A Gold Standard Dependency Corpus for English'], [tensor(0.6164), 'Speech2Vec: A Sequence-to-Sequence Framework for Learning Word Embeddings from Speech'], [tensor(0.6160), 'Rethinking the Inception Architecture for Computer Vision'], [tensor(0.6159), 'Unsupervised Learning of Spoken Language with Visual Context'], [tensor(0.6156), 'Learning Generic Sentence Representations Using Convolutional Neural Networks'], [tensor(0.6154), 'Synchronous Tree-Adjoining Grammars'], [tensor(0.6153), 'Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling'], [tensor(0.6150), 'Librispeech: An ASR corpus based on public domain audio books'], [tensor(0.6149), 'Classification of Semantic Relationships between Nominals Using Pattern Clusters'], [tensor(0.6148), 'Using Syntactic Coupling Features for Discriminating Phrase-Based Translations (WMT-08 Shared Translation Task)'], [tensor(0.6145), 'A Syntax-Directed Translator with Extended Domain of Locality'], [tensor(0.6144), 'Natural Language Processing with Python'], [tensor(0.6140), 'Hindi-to-Urdu Machine Translation through Transliteration'], [tensor(0.6140), 'The IIT Bombay English-Hindi Parallel Corpus'], [tensor(0.6124), 'Linguistically Motivated Unsupervised Segmentation for Machine Translation'], [tensor(0.6124), 'The VerbCorner Project: Toward an Empirically-Based Semantic Decomposition of Verbs'], [tensor(0.6119), 'A Decoder for Syntax-based Statistical MT'], [tensor(0.6112), 'Diagnostic Classifiers Revealing how Neural Networks Process Hierarchical Structure'], [tensor(0.6111), 'ReMixMatch: Semi-Supervised Learning with Distribution Matching and Augmentation Anchoring'], [tensor(0.6110), 'The Parallel Meaning Bank: Towards a Multilingual Corpus of Translations Annotated with Compositional Meaning Representations'], [tensor(0.6105), 'Applying Morphology Generation Models to Machine Translation'], [tensor(0.6097), 'Distributional vectors encode referential attributes'], [tensor(0.6096), 'A Hybrid Morpheme-Word Representation for Machine Translation of Morphologically Rich Languages'], [tensor(0.6096), 'Semantic Roles for String to Tree Machine Translation'], [tensor(0.6094), 'Explaining Recurrent Neural Network Predictions in Sentiment Analysis'], [tensor(0.6093), 'Neural Speech Recognizer: Acoustic-to-Word LSTM Model for Large Vocabulary Speech Recognition'], [tensor(0.6089), 'DivideMix: Learning with Noisy Labels as Semi-supervised Learning'], [tensor(0.6089), 'Convolutional, Long Short-Term Memory, fully connected Deep Neural Networks'], [tensor(0.6076), 'A causal framework for explaining the predictions of black-box sequence-to-sequence models'], [tensor(0.6075), 'Stanford typed dependencies manual'], [tensor(0.6074), 'Effective Information Extraction with Semantic Affinity Patterns and Relevant Regions'], [tensor(0.6070), 'A Simple Framework for Contrastive Learning of Visual Representations'], [tensor(0.6065), 'Speech and language processing - an introduction to natural language processing, computational linguistics, and speech recognition'], [tensor(0.6062), 'The Winograd Schema Challenge'], [tensor(0.6059), 'Relation Extraction with Matrix Factorization and Universal Schemas'], [tensor(0.6057), 'Convolutional Neural Networks for Sentence Classification'], [tensor(0.6056), 'Improving Entity Recommendation with Search Log and Multi-Task Learning'], [tensor(0.6055), 'From Frequency to Meaning: Vector Space Models of Semantics'], [tensor(0.6054), 'Character-level Convolutional Networks for Text Classification'], [tensor(0.6053), 'Combining Word-Level and Character-Level Models for Machine Translation Between Closely-Related Languages'], [tensor(0.6052), 'Do We Train on Test Data? Purging CIFAR of Near-Duplicates'], [tensor(0.6050), 'A Natural Language Translation Neural Network'], [tensor(0.6046), 'Introduction to the CoNLL-2004 Shared Task: Semantic Role Labeling'], [tensor(0.6046), 'Momentum Contrast for Unsupervised Visual Representation Learning'], [tensor(0.6046), 'Methods for interpreting and understanding deep neural networks'], [tensor(0.6044), 'A Neural Probabilistic Language Model'], [tensor(0.6042), 'Investigating gated recurrent networks for speech synthesis'], [tensor(0.6042), 'SRILM - an extensible language modeling toolkit'], [tensor(0.6040), 'Performance Impact Caused by Hidden Bias of Training Data for Recognizing Textual Entailment'], [tensor(0.6039), 'Representations of language in a model of visually grounded speech signal'], [tensor(0.6038), 'The United Nations Parallel Corpus v1.0'], [tensor(0.6036), 'A Discriminative Model for Tree-to-Tree Translation'], [tensor(0.6034), 'Using Test Suites in Evaluation of Machine Translation Systems'], [tensor(0.6031), 'Automatic Interpretation of Noun Compounds Using WordNet Similarity'], [tensor(0.6031), 'MADAMIRA: A Fast, Comprehensive Tool for Morphological Analysis and Disambiguation of Arabic'], [tensor(0.6026), 'Tregex and Tsurgeon: tools for querying and manipulating tree data structures'], [tensor(0.6023), 'Structures, Not Strings: Linguistics as Part of the Cognitive Sciences'], [tensor(0.6023), 'Can Active Memory Replace Attention?'], [tensor(0.6019), 'Offline bilingual word vectors, orthogonal transformations and the inverted softmax'], [tensor(0.6015), 'From speech to letters - using a novel neural network architecture for grapheme based ASR'], [tensor(0.6013), 'Direct Acoustics-to-Word Models for English Conversational Speech Recognition'], [tensor(0.6009), 'Bleu: a Method for Automatic Evaluation of Machine Translation'], [tensor(0.6003), 'Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition'], [tensor(0.6002), 'Deep multimodal semantic embeddings for speech and images'], [tensor(0.6001), 'Syntax-Based Statistical Machine Translation'], [tensor(0.5992), 'Convolutional Sequence to Sequence Learning'], [tensor(0.5988), 'Towards End-To-End Speech Recognition with Recurrent Neural Networks'], [tensor(0.5987), 'Continuous Space Language Models for Statistical Machine Translation'], [tensor(0.5987), 'Batch normalized recurrent neural networks'], [tensor(0.5986), 'SemEval 2014 Task 8: Broad-Coverage Semantic Dependency Parsing'], [tensor(0.5976), 'The Alignment Template Approach to Statistical Machine Translation'], [tensor(0.5975), 'Deep Speech 2 : End-to-End Speech Recognition in English and Mandarin'], [tensor(0.5974), 'On the Properties of Neural Machine Translation: Encoder–Decoder Approaches'], [tensor(0.5973), 'Natural Language Inference from Multiple Premises'], [tensor(0.5970), 'Handbook of Natural Language Processing and Machine Translation: DARPA Global Autonomous Language Exploitation'], [tensor(0.5963), 'Unsupervised Clustering using Pseudo-semi-supervised Learning'], [tensor(0.5962), 'Deep Convolutional Neural Networks for Sentiment Analysis of Short Texts'], [tensor(0.5954), 'Learning Recursive Distributed Representations for Holistic Computation'], [tensor(0.5952), 'WSABIE: Scaling Up to Large Vocabulary Image Annotation'], [tensor(0.5949), 'Do CIFAR-10 Classifiers Generalize to CIFAR-10?'], [tensor(0.5947), 'Strategies for training large scale neural network language models'], [tensor(0.5945), 'Early years in machine translation: memoirs and biographies of pioneers'], [tensor(0.5945), 'Probing for semantic evidence of composition by means of simple classification tasks'], [tensor(0.5943), 'Class-Based n-gram Models of Natural Language'], [tensor(0.5943), 'Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books'], [tensor(0.5942), 'QMDIS: QCRI-MIT Advanced Dialect Identification System'], [tensor(0.5940), 'Unsupervised Learning of Visual Features by Contrasting Cluster Assignments'], [tensor(0.5932), 'Illusory licensing effects across dependency types: ERP evidence'], [tensor(0.5930), 'GATCluster: Self-Supervised Gaussian-Attention Network for Image Clustering'], [tensor(0.5927), 'Deep Semantic Clustering by Partition Confidence Maximisation'], [tensor(0.5924), 'Unsupervised Representation Learning by Predicting Image Rotations'], [tensor(0.5920), 'Controlling Politeness in Neural Machine Translation via Side Constraints'], [tensor(0.5912), 'A tree-to-tree alignment-based model for statistical machine translation'], [tensor(0.5910), 'Automatic differentiation in PyTorch'], [tensor(0.5906), 'MixMatch: A Holistic Approach to Semi-Supervised Learning'], [tensor(0.5904), 'Towards Unsupervised Automatic Speech Recognition Trained by Unaligned Speech and Text only'], [tensor(0.5897), 'JANUS: a speech-to-speech translation system using connectionist and symbolic processing strategies'], [tensor(0.5895), 'OpenNMT: Open-Source Toolkit for Neural Machine Translation'], [tensor(0.5894), 'Parallel Data, Tools and Interfaces in OPUS'], [tensor(0.5893), 'Linguistics Vanguard'], [tensor(0.5890), 'Improving Word Sense Disambiguation in Neural Machine Translation with Sense Embeddings'], [tensor(0.5889), \"Visualisation and 'diagnostic classifiers' reveal how recurrent and recursive neural networks process hierarchical structure\"], [tensor(0.5888), 'Factored Translation Models'], [tensor(0.5885), 'Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results'], [tensor(0.5883), 'TSNLP - Test Suites for Natural Language Processing'], [tensor(0.5881), 'Modeling Syntactic and Semantic Structures in Hierarchical Phrase-based Translation'], [tensor(0.5878), 'Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data'], [tensor(0.5876), 'Very deep convolutional networks for end-to-end speech recognition'], [tensor(0.5873), 'Thematic proto-roles and argument selection'], [tensor(0.5869), 'Moses: Open Source Toolkit for Statistical Machine Translation'], [tensor(0.5867), 'A computational model of S-selection'], [tensor(0.5865), 'The emergence of grammaticality in connectionist networks.'], [tensor(0.5862), 'A probabilistic framework for segment-based speech recognition'], [tensor(0.5862), 'Visual interpretability for deep learning: a survey'], [tensor(0.5860), 'Using a stochastic context-free grammar as a language model for speech recognition'], [tensor(0.5853), 'SCAN: Learning to Classify Images Without Labels'], [tensor(0.5853), 'Learning the speech front-end with raw waveform CLDNNs'], [tensor(0.5852), 'On the semantics of noun compounds'], [tensor(0.5851), 'Unsupervised Morphology Rivals Supervised Morphology for Arabic MT'], [tensor(0.5845), 'Recognizing textual entailment: Rational, evaluation and approaches – Erratum'], [tensor(0.5844), 'Learning Non-Isomorphic Tree Mappings for Machine Translation'], [tensor(0.5838), 'From phonemes to images: levels of representation in a recurrent neural model of visually-grounded language learning'], [tensor(0.5828), 'Verbnet: a broad-coverage, comprehensive verb lexicon'], [tensor(0.5824), 'Darmstadt Knowledge Processing Repository Based on UIMA'], [tensor(0.5823), 'Towards A Rigorous Science of Interpretable Machine Learning'], [tensor(0.5819), 'Exploiting Similarities among Languages for Machine Translation'], [tensor(0.5814), 'A Hierarchical Phrase-Based Model for Statistical Machine Translation'], [tensor(0.5810), 'Learning Weakly Supervised Multimodal Phoneme Embeddings'], [tensor(0.5810), 'Part-of-Speech Tagging With Neural Networks'], [tensor(0.5808), 'Humor Recognition and Humor Anchor Extraction'], [tensor(0.5808), 'Contrastive Clustering'], [tensor(0.5805), 'Analyzing Hidden Representations in End-to-End Automatic Speech Recognition Systems'], [tensor(0.5797), 'Negative and Positive Polarity Items: Variation, Licensing, and Compositionality'], [tensor(0.5794), 'Associative Deep Clustering: Training a Classification Network with No Labels'], [tensor(0.5792), 'Statistical Language Models Based on Neural Networks'], [tensor(0.5791), 'Long Short-Term Memory'], [tensor(0.5782), 'Continuous space language models'], [tensor(0.5779), 'Encoding of phonology in a recurrent neural model of grounded speech'], [tensor(0.5777), 'Lexicon-Free Conversational Speech Recognition with Neural Networks'], [tensor(0.5777), 'Exploring neural network architectures for acoustic modeling'], [tensor(0.5774), 'Self-labelling via simultaneous clustering and representation learning'], [tensor(0.5770), 'Learning representations by back-propagating errors'], [tensor(0.5761), 'Intriguing properties of neural networks'], [tensor(0.5761), 'Domain Adaptation for Large-Scale Sentiment Classification: A Deep Learning Approach'], [tensor(0.5758), 'Regularization With Stochastic Transformations and Perturbations for Deep Semi-Supervised Learning'], [tensor(0.5741), 'Unsupervised Data Augmentation'], [tensor(0.5728), 'From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions'], [tensor(0.5726), 'Using latent semantic analysis to improve access to textual information'], [tensor(0.5722), 'Using a Semantic Concordance for Sense Identification'], [tensor(0.5719), 'Visualizing and Understanding Convolutional Networks'], [tensor(0.5716), 'Long short-term memory recurrent neural network architectures for large scale acoustic modeling'], [tensor(0.5715), 'Statistical Phrase-Based Translation'], [tensor(0.5711), 'Houdini : Fooling Deep Structured Visual and Speech Recognition Models with Adversarial Examples'], [tensor(0.5710), 'On the unification of syntactic annotations under the Stanford dependency scheme: A case study on BioInfer and GENIA'], [tensor(0.5701), 'A Theoretically Grounded Application of Dropout in Recurrent Neural Networks'], [tensor(0.5694), 'Bootstrap Your Own Latent: A New Approach to Self-Supervised Learning'], [tensor(0.5691), 'Speech recognition with deep recurrent neural networks'], [tensor(0.5685), 'Deep Clustering: On the Link Between Discriminative Models and K-Means'], [tensor(0.5684), 'Weighted finite-state transducers in speech recognition'], [tensor(0.5670), 'Finding Structure in Time'], [tensor(0.5668), 'Explainable Outfit Recommendation with Joint Outfit Matching and Comment Generation'], [tensor(0.5663), 'Jointly Learning Explainable Rules for Recommendation with Knowledge Graph'], [tensor(0.5659), 'The MIT SUMMIT Speech Recognition System: A Progress Report'], [tensor(0.5658), 'GATE: an Architecture for Development of Robust HLT applications'], [tensor(0.5656), 'Tree-to-String Alignment Template for Statistical Machine Translation'], [tensor(0.5646), 'String-to-Dependency Statistical Machine Translation'], [tensor(0.5637), 'Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks'], [tensor(0.5624), 'Pseudo-Supervised Deep Subspace Clustering'], [tensor(0.5622), 'Grammaticality, Acceptability, and Probability: A Probabilistic View of Linguistic Knowledge'], [tensor(0.5620), 'Can recurrent neural networks learn natural language grammars?'], [tensor(0.5619), 'Generating Typed Dependency Parses from Phrase Structure Parses'], [tensor(0.5616), 'Improving Outfit Recommendation with Co-supervision of Fashion Generation'], [tensor(0.5613), 'BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer'], [tensor(0.5610), 'Deep Discriminative Clustering Analysis'], [tensor(0.5603), 'Explainable Recommendation via Multi-Task Learning in Opinionated Text Data'], [tensor(0.5601), 'Neurophysiological dynamics of phrase-structure building during sentence processing'], [tensor(0.5598), 'Structure at the lexical level and its implication for transfer grammar'], [tensor(0.5598), 'WaveNet: A Generative Model for Raw Audio'], [tensor(0.5597), 'Arabic Preprocessing Schemes for Statistical Machine Translation'], [tensor(0.5588), 'Improving Neural Language Models with a Continuous Cache'], [tensor(0.5587), 'Improved Regularization of Convolutional Neural Networks with Cutout'], [tensor(0.5581), 'Semi-supervised Learning with Ladder Networks'], [tensor(0.5581), 'RandAugment: Practical data augmentation with no separate search'], [tensor(0.5577), 'Semantic Classification with Distributional Kernels'], [tensor(0.5568), 'Deep Learning Scaling is Predictable, Empirically'], [tensor(0.5566), 'Natural Language Understanding'], [tensor(0.5565), 'Neural Word Embedding as Implicit Matrix Factorization'], [tensor(0.5562), 'Greedy Layer-Wise Training of Deep Networks'], [tensor(0.5561), '5: Grammatical Illusions and Selective Fallibility in Real-Time Language Comprehension'], [tensor(0.5554), 'Invariant Information Clustering for Unsupervised Image Classification and Segmentation'], [tensor(0.5553), 'Discriminatively Boosted Image Clustering with Fully Convolutional Auto-Encoders'], [tensor(0.5553), 'Neural Architectures for Named Entity Recognition'], [tensor(0.5547), 'Machine Translation: Past, Present, Future'], [tensor(0.5545), 'Hot Topics Surrounding Acceptability Judgement Tasks'], [tensor(0.5540), 'Machine Translation Using Probabilistic Synchronous Dependency Insertion Grammars'], [tensor(0.5532), 'The Kaldi Speech Recognition Toolkit'], [tensor(0.5528), 'Learning representations by backpropagating errors'], [tensor(0.5523), 'A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based on Minimum Cuts'], [tensor(0.5522), 'SEMAX: Multi-Task Learning for Improving Recommendations'], [tensor(0.5522), 'Multimodal Explanations: Justifying Decisions and Pointing to the Evidence'], [tensor(0.5518), 'Notes on transitivity and theme in English: Part 2'], [tensor(0.5518), 'Representation and structure in connectionist models'], [tensor(0.5513), 'Broken agreement'], [tensor(0.5508), 'Pseudo-Label : The Simple and Efficient Semi-Supervised Learning Method for Deep Neural Networks'], [tensor(0.5507), 'Overcoming catastrophic forgetting in neural networks'], [tensor(0.5504), 'Identity Mappings in Deep Residual Networks'], [tensor(0.5503), 'The role of veridicality and factivity in clause selection *'], [tensor(0.5500), 'Variational Deep Embedding: An Unsupervised and Generative Approach to Clustering'], [tensor(0.5498), 'Deep Unsupervised Clustering with Gaussian Mixture Variational Autoencoders'], [tensor(0.5477), 'Layer-Wise Relevance Propagation for Deep Neural Network Architectures'], [tensor(0.5474), 'Concept to code: deep learning for multitask recommendation'], [tensor(0.5472), 'Content extraction using diverse feature sets'], [tensor(0.5472), 'Deep Residual Learning for Image Recognition'], [tensor(0.5470), 'Joint Unsupervised Learning of Deep Representations and Image Clusters'], [tensor(0.5457), 'Towards K-means-friendly Spaces: Simultaneous Deep Learning and Clustering'], [tensor(0.5454), 'Feature Learning in Deep Neural Networks - Studies on Speech Recognition Tasks.'], [tensor(0.5449), 'Understanding how Deep Belief Networks perform acoustic modelling'], [tensor(0.5448), 'Unsupervised Feature Learning via Non-parametric Instance Discrimination'], [tensor(0.5439), 'Recurrent Neural Network Grammars'], [tensor(0.5422), 'Training Very Deep Networks'], [tensor(0.5412), 'The Mathematics of Statistical Machine Translation: Parameter Estimation'], [tensor(0.5411), 'Adam: A Method for Stochastic Optimization'], [tensor(0.5408), 'What Does the Speaker Embedding Encode?'], [tensor(0.5404), 'Building machines that learn and think like people'], [tensor(0.5399), 'Interference effects from grammatically unavailable constituents during sentence processing.'], [tensor(0.5398), 'Neural Models of Factuality'], [tensor(0.5396), 'Naive v. expert intuitions: An empirical study of acceptability judgments'], [tensor(0.5394), 'Deep Comprehensive Correlation Mining for Image Clustering'], [tensor(0.5381), 'An Experimental Investigation of Agent Prototypicality and Agent Prominence in German'], [tensor(0.5376), 'Deep Clustering via Joint Convolutional Autoencoder Embedding and Relative Entropy Minimization'], [tensor(0.5374), 'Detecting Gene Relations from MEDLINE Abstracts'], [tensor(0.5371), 'Generating Sequences With Recurrent Neural Networks'], [tensor(0.5368), 'Findings of the 2017 Conference on Machine Translation (WMT17)'], [tensor(0.5367), 'Extensions of recurrent neural network language model'], [tensor(0.5366), 'A Novel Distributional Approach to Multilingual Conceptual Metaphor Recognition'], [tensor(0.5349), 'Understanding Black-box Predictions via Influence Functions'], [tensor(0.5348), 'Multitask Learning'], [tensor(0.5346), 'Exploring how deep neural networks form phonemic categories'], [tensor(0.5345), 'Two routes to actorhood: lexicalized potency to act and identification of the actor role'], [tensor(0.5342), 'Discriminative Training for Large-Vocabulary Speech Recognition Using Minimum Classification Error'], [tensor(0.5340), 'The Combinatorial Structure of Thought: The Family of Causative Concepts'], [tensor(0.5331), 'The Natural Language Decathlon: Multitask Learning as Question Answering'], [tensor(0.5328), 'Nearest Neighbor Matching for Deep Clustering'], [tensor(0.5313), 'Agreement Attraction in Comprehension: Representations and Processes.'], [tensor(0.5312), 'Semi-supervised Learning by Entropy Minimization'], [tensor(0.5306), 'Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift'], [tensor(0.5299), 'Auto-Encoding Variational Bayes'], [tensor(0.5294), 'Recurrent neural network based language model'], [tensor(0.5286), 'TUGAS: Exploiting unlabelled data for Twitter sentiment analysis'], [tensor(0.5283), 'Deep Adaptive Image Clustering'], [tensor(0.5267), 'Quantitative methods in syntax/semantics research: A response to Sprouse and Almeida (2013)'], [tensor(0.5255), 'Deep Clustering for Unsupervised Learning of Visual Features'], [tensor(0.5244), 'The Chameleon-like Nature of Evaluative Adjectives'], [tensor(0.5236), 'TIMIT Acoustic-Phonetic Continuous Speech Corpus'], [tensor(0.5226), 'Recursive Hetero-associative Memories for Translation'], [tensor(0.5224), 'Is Passive Syntax Semantically Constrained? Evidence From Adult Grammaticality Judgment and Comprehension Studies'], [tensor(0.5220), 'ImageNet classification with deep convolutional neural networks'], [tensor(0.5219), 'ADADELTA: An Adaptive Learning Rate Method'], [tensor(0.5210), 'Illinois-LH: A Denotational and Distributional Approach to Semantics'], [tensor(0.5204), 'Towards Principled Unsupervised Learning'], [tensor(0.5200), 'Dropout: a simple way to prevent neural networks from overfitting'], [tensor(0.5194), 'Visualizing Data using t-SNE'], [tensor(0.5193), 'Curriculum learning'], [tensor(0.5193), 'ImageNet Large Scale Visual Recognition Challenge'], [tensor(0.5191), 'Example-based machine translation using connectionist matching'], [tensor(0.5190), 'Semantic Proto-Roles'], [tensor(0.5187), 'Recognizing Textual Entailment: Models and Applications'], [tensor(0.5182), 'Probabilistic Topic Models'], [tensor(0.5172), 'Gender Bias in Coreference Resolution'], [tensor(0.5162), 'From humor recognition to irony detection: The figurative language of social media'], [tensor(0.5149), 'Explanation Methods in Deep Learning: Users, Values, Concerns and Challenges'], [tensor(0.5143), 'Explaining and Harnessing Adversarial Examples'], [tensor(0.5132), 'Unsupervised Deep Embedding for Clustering Analysis'], [tensor(0.5130), 'Black-Box Generation of Adversarial Text Sequences to Evade Deep Learning Classifiers'], [tensor(0.5129), 'Deep Text Classification Can be Fooled'], [tensor(0.5119), 'Self-Supervised Representation Learning With MUlti-Segmental Informational Coding (MUSIC)'], [tensor(0.5106), 'Deconvolutional networks'], [tensor(0.5106), 'Proto-Properties and Grammatical Encoding: A Correspondence Theory of Argument Selection'], [tensor(0.5103), 'Acoustic modeling with deep neural networks using raw time signal for LVCSR'], [tensor(0.5095), 'Deep Self-Evolution Clustering'], [tensor(0.5093), 'Weak quantitative standards in linguistics research'], [tensor(0.5076), 'Normalized Word Embedding and Orthogonal Transform for Bilingual Word Translation'], [tensor(0.5061), 'What’s in a translation rule?'], [tensor(0.5061), 'On the Role of Nonlinear Transformations in Deep Neural Network Acoustic Models'], [tensor(0.5060), 'Multi-Task Feature Learning for Knowledge Graph Enhanced Recommendation'], [tensor(0.5058), 'Measuring Semantic Similarity by Latent Relational Analysis'], [tensor(0.5046), 'Thematic roles: Core knowledge or linguistic construct?'], [tensor(0.5042), 'Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion'], [tensor(0.5041), 'SPICE: Semantic Pseudo-Labeling for Image Clustering'], [tensor(0.5039), 'Normal and pathological development of subject–verb agreement in speech production: a study on French children'], [tensor(0.5037), 'Sinkhorn Distances: Lightspeed Computation of Optimal Transport'], [tensor(0.5031), 'Gradient Flow in Recurrent Nets: the Difficulty of Learning Long-Term Dependencies'], [tensor(0.5025), 'Deep Subspace Clustering Networks'], [tensor(0.5023), 'Phoneme recognition using time-delay neural networks'], [tensor(0.5022), 'Mitigating Embedding and Class Assignment Mismatch in Unsupervised Image Classification'], [tensor(0.5013), 'Simple Fast Algorithms for the Editing Distance Between Trees and Related Problems'], [tensor(0.5012), 'Noise-Contrastive Estimation of Unnormalized Statistical Models, with Applications to Natural Image Statistics'], [tensor(0.5001), 'Machine humour : an implemented model of puns'], [tensor(0.4992), 'Automatic Labeling of Semantic Roles'], [tensor(0.4983), 'Towards Crafting Text Adversarial Samples'], [tensor(0.4975), 'Layer Normalization'], [tensor(0.4972), 'Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks'], [tensor(0.4958), 'Automatic Recognition of Spoken Digits'], [tensor(0.4949), 'Social Bias in Elicited Natural Language Inferences'], [tensor(0.4939), 'Hidden factors and hidden topics: understanding rating dimensions with review text'], [tensor(0.4936), 'Unsupervised Multi-Manifold Clustering by Learning Deep Representation'], [tensor(0.4922), 'A Syntax-based Statistical Translation Model'], [tensor(0.4922), 'Xception: Deep Learning with Depthwise Separable Convolutions'], [tensor(0.4904), 'Argument Realization'], [tensor(0.4903), 'On the Informativity of Different Measures of Linguistic Acceptability'], [tensor(0.4895), 'Some thoughts on agentivity'], [tensor(0.4884), 'Deep Image Prior'], [tensor(0.4864), 'Spoken Language Processing: A Guide to Theory, Algorithm and System Development'], [tensor(0.4859), 'Annotating Expressions of Opinions and Emotions in Language'], [tensor(0.4834), 'Recent Advances in Natural Language Processing: Selected Papers from RANLP ’95'], [tensor(0.4800), 'Strike (With) a Pose: Neural Networks Are Easily Fooled by Strange Poses of Familiar Objects'], [tensor(0.4789), 'The 2005 PASCAL Visual Object Classes Challenge'], [tensor(0.4763), 'The Mythos of Model Interpretability'], [tensor(0.4750), 'Sentiment Analysis: An Overview from Linguistics'], [tensor(0.4745), 'The Semantic Proto-Role Linking Model'], [tensor(0.4739), 'Joint Image Clustering and Labeling by Matrix Factorization'], [tensor(0.4731), 'Deep Embedding Network for Clustering'], [tensor(0.4730), 'A framework for syntactic translation'], [tensor(0.4725), 'Introduction to Arabic Natural Language Processing'], [tensor(0.4720), 'Robust Classification with Convolutional Prototype Learning'], [tensor(0.4699), 'Care Episode Retrieval'], [tensor(0.4699), 'An efficient prototype merging strategy for the condensed 1-NN rule through class-conditional hierarchical clustering'], [tensor(0.4696), 'An Efficient Algorithm for Projective Dependency Parsing'], [tensor(0.4688), 'Theoretical aspects of mechanical speech recognition'], [tensor(0.4681), 'HOME: High-Order Mixed-Moment-based Embedding for Representation Learning'], [tensor(0.4680), 'Improving Unsupervised Image Clustering With Robust Learning'], [tensor(0.4679), 'Gate Activation Signal Analysis for Gated Recurrent Neural Networks and its Correlation with Phoneme Boundaries'], [tensor(0.4664), 'Why Clowns Taste Funny: The Relationship between Humor and Semantic Ambiguity'], [tensor(0.4645), 'The advantages and challenges of “big data”: Insights from the 14 billion word iWeb corpus'], [tensor(0.4643), 'DeepCluster: A General Clustering Framework Based on Deep Learning'], [tensor(0.4633), 'Modeling Consumer Buying Decision for Recommendation Based on Multi-Task Deep Learning'], [tensor(0.4632), 'SemEval 2015 Task 18: Broad-Coverage Semantic Dependency Parsing'], [tensor(0.4626), 'Thematic role properties of subjects and objects'], [tensor(0.4620), 'Does label smoothing mitigate label noise?'], [tensor(0.4619), 'Why I like it: multi-task learning for recommendation and explanation'], [tensor(0.4614), 'Cluster Ensembles --- A Knowledge Reuse Framework for Combining Multiple Partitions'], [tensor(0.4601), 'Bridging Nonlinearities and Stochastic Regularizers with Gaussian Error Linear Units'], [tensor(0.4590), 'Neural GPUs Learn Algorithms'], [tensor(0.4577), 'Simple Black-Box Adversarial Attacks on Deep Neural Networks'], [tensor(0.4562), 'Fooling End-To-End Speaker Verification With Adversarial Examples'], [tensor(0.4521), 'The design and operation of the mechanical speech recognizer at University College London'], [tensor(0.4514), 'Visualisation and ‘diagnostic classifiers’ reveal how recurrent and recursive neural networks process hierarchical structure'], [tensor(0.4498), 'Segmentation for English-to-Arabic Statistical Machine Translation'], [tensor(0.4480), 'Self-Supervised Convolutional Subspace Clustering Network'], [tensor(0.4478), 'Neural Multi-task Recommendation from Multi-behavior Data'], [tensor(0.4466), 'Clustering-friendly Representation Learning via Instance Discrimination and Feature Decorrelation'], [tensor(0.4449), 'The experiencer as an agent'], [tensor(0.4447), 'Colorful Image Colorization'], [tensor(0.4447), 'Audio Adversarial Examples: Targeted Attacks on Speech-to-Text'], [tensor(0.4422), 'Locality Preserving Nonnegative Matrix Factorization'], [tensor(0.4390), 'Language, Consciousness, Culture: Essays on Mental Structure'], [tensor(0.4389), 'Crafting adversarial input sequences for recurrent neural networks'], [tensor(0.4361), 'Models for the Semantic Classification of Noun Phrases'], [tensor(0.4350), 'Explanation in Artificial Intelligence: Insights from the Social Sciences'], [tensor(0.4340), 'Core knowledge.'], [tensor(0.4318), 'Deep Adversarial Subspace Clustering'], [tensor(0.4310), 'Impersonal Passives and the Unaccusative Hypothesis'], [tensor(0.4303), 'From Understanding Computation to Understanding Neural Circuitry'], [tensor(0.4244), 'Speaker-independent phone recognition using hidden Markov models'], [tensor(0.4239), 'Some methods for classification and analysis of multivariate observations'], [tensor(0.4218), 'Semi-Supervised Structured Subspace Learning for Multi-View Clustering'], [tensor(0.4185), 'Transferability in Machine Learning: from Phenomena to Black-Box Attacks using Adversarial Samples'], [tensor(0.4164), 'Semantic Types of Some Generic Relation Arguments: Detection and Evaluation'], [tensor(0.4162), 'Accountability of AI Under the Law: The Role of Explanation'], [tensor(0.4109), 'Delving into Transferable Adversarial Examples and Black-box Attacks'], [tensor(0.4081), 'Neural Network Classifiers for Speech Recognition'], [tensor(0.4079), 'A comparison of informal and formal acceptability judgments using a random sample from Linguistic Inquiry 2001--2010'], [tensor(0.4063), 'A common misapplication of statistical inference: Nuisance control with null-hypothesis significance tests'], [tensor(0.4051), 'Practical Black-Box Attacks against Machine Learning'], [tensor(0.4049), 'An approach to computer speech recognition by direct analysis of the speech wave'], [tensor(0.4043), 'Backpropagation Through Time: What It Does and How to Do It'], [tensor(0.4027), 'A Maximum Likelihood Approach to Continuous Speech Recognition'], [tensor(0.4018), 'iCmSC: Incomplete Cross-Modal Subspace Clustering'], [tensor(0.4010), 'On comparing partitions'], [tensor(0.4005), 'Design sensitivity and statistical power in acceptability judgment experiments'], [tensor(0.4001), 'Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)'], [tensor(0.3951), 'Fast Agglomerative Clustering Using a k-Nearest Neighbor Graph'], [tensor(0.3950), 'Joint Learning of Latent Similarity and Local Embedding for Multi-View Clustering'], [tensor(0.3893), 'Using the Framework'], [tensor(0.3891), 'Using Machine Teaching to Identify Optimal Training-Set Attacks on Machine Learners'], [tensor(0.3874), 'Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)'], [tensor(0.3852), 'Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)'], [tensor(0.3811), 'The reliability of acceptability judgments across languages'], [tensor(0.3808), 'The Relationships Among Various Nonnegative Matrix Factorization Methods for Clustering'], [tensor(0.3805), 'Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)'], [tensor(0.3792), 'Transformer-XL: Language Modeling with Longer-Term Dependency'], [tensor(0.3723), 'Agglomerative clustering using the concept of mutual nearest neighbourhood'], [tensor(0.3718), 'History and Development of Speech Recognition'], [tensor(0.3701), 'Analyzing Ordinal Data with Metric Models: What Could Possibly Go Wrong?'], [tensor(0.3694), 'Open Source Toolkit for Statistical Machine Translation: Factored Translation Models and Lattice Decoding'], [tensor(0.3651), 'A multi-prototype clustering algorithm'], [tensor(0.3647), 'Poisoning Attacks against Support Vector Machines'], [tensor(0.3602), 'Recognition of consonant based on the perceptron model'], [tensor(0.3504), 'A Statistical Approach to Machine Translation'], [tensor(0.3213), 'The effect of rating scale format on response styles: the number of response categories and response catgory labels'], [tensor(0.3074), 'Stative Adjectives and Verbs in English'], [tensor(0.3020), 'In other words. A coursebook on translation [Review of: M. Bakker (1994) -]'], [tensor(0.3001), 'On Spectral Clustering: Analysis and an algorithm'], [tensor(0.2926), 'A lightweight weakly supervised learning segmentation algorithm for imbalanced image based on rotation density peaks'], [tensor(0.2702), 'Categorical Data Analysis'], [tensor(0.2590), 'Challenges for Transparency'], [tensor(0.2482), 'Summary and Future Directions.'], [tensor(0.2479), '“Cloze Procedure”: A New Tool for Measuring Readability'], [tensor(0.2472), 'Background to Framenet'], [tensor(0.2251), 'An Overview of Vulnerabilities of Voice Controlled Systems'], [tensor(0.2098), 'Advances in Neural Information Processing Systems 27'], [tensor(0.2068), 'Competition in argument interpretation: Evidence from the neurobiology of language'], [tensor(0.2024), 'Brief History of Automatic Speech Recognition'], [tensor(0.2004), 'Interpolated estimation of Markov source parameters from sparse data'], [tensor(0.1823), 'Language and Cognitive Processes'], [tensor(0.1805), 'References'], [tensor(0.1736), 'Minimum Classification Error (MCE) Approach in Pattern Recognition'], [tensor(0.1302), 'The Language of Riddles: New Perspectives'], [tensor(0.1259), 'Women, Fire, and Dangerous Things'], [tensor(0.0913), 'LoPar: Design and Implementation']]\n"
     ]
    }
   ],
   "source": [
    "GREEN = '\\033[32m'\n",
    "RED = '\\033[31m'\n",
    "RESET = '\\033[0m'\n",
    "\n",
    "similarityScoresWithTitle.sort(reverse=True, key=getKey)\n",
    "for i in similarityScoresWithTitle:\n",
    "    print(f\"Cos similarity score: {i[0]:.3f} and it's {RED + 'not cited in ' if i[1] not in defaultPaper[1] else GREEN + 'cited in     '}{RESET}the main paper with title: {i[1]:20s}\")\n",
    "print(similarityScoresWithTitle)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
