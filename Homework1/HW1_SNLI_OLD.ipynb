{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zebzi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification,\\\n",
    "    TrainingArguments, Trainer, pipeline, DataCollatorWithPadding, set_seed\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_seed(42)\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset snli (C:/Users/zebzi/.cache/huggingface/datasets/snli/plain_text/1.0.0/1f60b67533b65ae0275561ff7828aad5ee4282d0e6f844fd148d05d3c6ea251b)\n",
      "100%|██████████| 3/3 [00:00<00:00, 435.30it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label'],\n",
       "        num_rows: 550152\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snli = load_dataset(\"snli\")\n",
    "# snli_test = load_dataset(\"json\", data_files=\"SNLI_Dataset/snli_1.0_test.jsonl\")\n",
    "snli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\zebzi\\.cache\\huggingface\\datasets\\snli\\plain_text\\1.0.0\\1f60b67533b65ae0275561ff7828aad5ee4282d0e6f844fd148d05d3c6ea251b\\cache-17e02a99a93fd477.arrow\n",
      "Loading cached processed dataset at C:\\Users\\zebzi\\.cache\\huggingface\\datasets\\snli\\plain_text\\1.0.0\\1f60b67533b65ae0275561ff7828aad5ee4282d0e6f844fd148d05d3c6ea251b\\cache-b71979dc8e6f8a8f.arrow\n",
      "Loading cached processed dataset at C:\\Users\\zebzi\\.cache\\huggingface\\datasets\\snli\\plain_text\\1.0.0\\1f60b67533b65ae0275561ff7828aad5ee4282d0e6f844fd148d05d3c6ea251b\\cache-3ac348b3e4f4ff95.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A person on a horse jumps over a broken down airplane.\n"
     ]
    }
   ],
   "source": [
    "print(snli[\"train\"][\"premise\"][0])\n",
    "snli = snli.filter(lambda example: example[\"label\"] != -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['premise', 'hypothesis', 'label'],\n",
      "        num_rows: 9824\n",
      "    })\n",
      "    train: Dataset({\n",
      "        features: ['premise', 'hypothesis', 'label'],\n",
      "        num_rows: 549367\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['premise', 'hypothesis', 'label'],\n",
      "        num_rows: 9842\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "type(snli[\"train\"][\"hypothesis\"])\n",
    "textTrain = np.char.add(np.char.add(snli['train']['premise'], ' '), snli['train']['hypothesis'])\n",
    "textTest = np.char.add(np.char.add(snli['test']['premise'], ' '), snli['test']['hypothesis'])\n",
    "textVali = np.char.add(np.char.add(snli['validation']['premise'], ' '), snli['validation']['hypothesis'])\n",
    "print(snli)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['premise', 'hypothesis', 'label'],\n",
      "        num_rows: 9824\n",
      "    })\n",
      "    train: Dataset({\n",
      "        features: ['premise', 'hypothesis', 'label'],\n",
      "        num_rows: 549367\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['premise', 'hypothesis', 'label'],\n",
      "        num_rows: 9842\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\zebzi\\.cache\\huggingface\\datasets\\snli\\plain_text\\1.0.0\\1f60b67533b65ae0275561ff7828aad5ee4282d0e6f844fd148d05d3c6ea251b\\cache-64c7ea292a984126.arrow\n",
      "Loading cached processed dataset at C:\\Users\\zebzi\\.cache\\huggingface\\datasets\\snli\\plain_text\\1.0.0\\1f60b67533b65ae0275561ff7828aad5ee4282d0e6f844fd148d05d3c6ea251b\\cache-3a136d10d4818fd9.arrow\n",
      "Loading cached processed dataset at C:\\Users\\zebzi\\.cache\\huggingface\\datasets\\snli\\plain_text\\1.0.0\\1f60b67533b65ae0275561ff7828aad5ee4282d0e6f844fd148d05d3c6ea251b\\cache-56d5f0c3e7bb6421.arrow\n"
     ]
    }
   ],
   "source": [
    "print(snli)\n",
    "snli[\"train\"] = snli[\"train\"].remove_columns([\"premise\", \"hypothesis\"])\n",
    "snli[\"train\"] = snli[\"train\"].add_column(\"text\", textTrain)\n",
    "\n",
    "snli[\"test\"] = snli[\"test\"].remove_columns([\"premise\", \"hypothesis\"])\n",
    "snli[\"test\"] = snli[\"test\"].add_column(\"text\", textTest)\n",
    "\n",
    "snli[\"validation\"] = snli[\"validation\"].remove_columns([\"premise\", \"hypothesis\"])\n",
    "snli[\"validation\"] = snli[\"validation\"].add_column(\"text\", textVali)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(snli)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\zebzi\\.cache\\huggingface\\datasets\\snli\\plain_text\\1.0.0\\1f60b67533b65ae0275561ff7828aad5ee4282d0e6f844fd148d05d3c6ea251b\\cache-e2ef9dd11bde0188.arrow\n",
      "Loading cached processed dataset at C:\\Users\\zebzi\\.cache\\huggingface\\datasets\\snli\\plain_text\\1.0.0\\1f60b67533b65ae0275561ff7828aad5ee4282d0e6f844fd148d05d3c6ea251b\\cache-eff9f8234b7d432d.arrow\n",
      "100%|██████████| 10/10 [00:00<00:00, 13.32ba/s]\n"
     ]
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True)\n",
    "    \n",
    "tokenized_snli= snli.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['label', 'text', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 9824\n",
      "    })\n",
      "    train: Dataset({\n",
      "        features: ['label', 'text', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 549367\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['label', 'text', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 9842\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_snli)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = numpy.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is this necessary? yes\n",
    "id2label = {0: \"entailment\", 1: \"neutral\", 2:\"contradiction\"}\n",
    "label2id = {\"entailment\": 0, \"neutral\": 1, \"contradiction\": 2}\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"C:/Users/zebzi/Documents/School/Senior_Year/CSCI 5541/NLP/hamza_model\", num_labels=3, id2label=id2label, label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "eval_loss_list = []\n",
    "eval_acc_list = []\n",
    "time_per_epoch_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTrainer(Trainer):\n",
    "    def _inner_training_loop(\n",
    "        self, batch_size=None, args=None, resume_from_checkpoint=None, trial=None, ignore_keys_for_eval=None\n",
    "    ):\n",
    "        number_of_epochs = args.num_train_epochs\n",
    "        start = time.time()\n",
    "        train_loss=[]\n",
    "        train_acc=[]\n",
    "        eval_acc=[]\n",
    "\n",
    "        criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "        self.optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, 1, gamma=0.9)\n",
    "        \n",
    "        train_dataloader = self.get_train_dataloader()\n",
    "        eval_dataloader = self.get_eval_dataloader()\n",
    "\n",
    "        for epoch in range(number_of_epochs):\n",
    "            train_loss_per_epoch = 0\n",
    "            train_acc_per_epoch = 0\n",
    "            with tqdm(train_dataloader, unit=\"batch\") as training_epoch:\n",
    "                training_epoch.set_description(f\"Training Epoch {epoch}\")\n",
    "                for step, inputs in enumerate(training_epoch):\n",
    "                    inputs = inputs.to(device)\n",
    "                    labels = inputs['labels']\n",
    "                    # forward pass\n",
    "                    self.optimizer.zero_grad()\n",
    "                    output = model(inputs['input_ids'])\n",
    "                    # get the loss\n",
    "                    loss = criterion(output['logits'], labels)\n",
    "                    train_loss_per_epoch += loss.item()\n",
    "                    #calculate gradients\n",
    "                    loss.backward()\n",
    "                    #update weights\n",
    "                    self.optimizer.step()\n",
    "                    train_acc_per_epoch += (output['logits'].argmax(1) == labels).sum().item()\n",
    "            # adjust the learning rate\n",
    "            self.scheduler.step()\n",
    "            train_loss_per_epoch /= len(train_dataloader)\n",
    "            train_acc_per_epoch /= (len(train_dataloader)*batch_size)\n",
    "            \n",
    "            \n",
    "            eval_loss_per_epoch = 0\n",
    "            eval_acc_per_epoch = 0\n",
    "            with tqdm(eval_dataloader, unit=\"batch\") as eval_epoch:\n",
    "                eval_epoch.set_description(f\"Evaluation Epoch {epoch}\")\n",
    "                for step, inputs in enumerate(eval_epoch):\n",
    "                    inputs = inputs.to(device)\n",
    "                    labels = inputs['labels']\n",
    "                    with torch.no_grad():\n",
    "                        output = model(inputs['input_ids'])\n",
    "                        loss = criterion(output['logits'], labels)\n",
    "                        eval_loss_per_epoch += loss.item()\n",
    "                        eval_acc_per_epoch += (output['logits'].argmax(1) == labels).sum().item()\n",
    "            eval_loss_per_epoch /= (len(eval_dataloader))\n",
    "            eval_acc_per_epoch /= (len(eval_dataloader)*batch_size)\n",
    "        \n",
    "            \n",
    "            print(f'\\tTrain Loss: {train_loss_per_epoch:.3f} | Train Acc: {train_acc_per_epoch*100:.2f}%')\n",
    "            print(f'\\tEval Loss: {eval_loss_per_epoch:.3f} | Eval Acc: {eval_acc_per_epoch*100:.2f}%')\n",
    "            train_loss_list.append(train_loss_per_epoch)\n",
    "            train_acc_list.append(train_acc_per_epoch)\n",
    "            eval_loss_list.append(eval_loss_per_epoch)\n",
    "            eval_acc_list.append(eval_acc_per_epoch)\n",
    "            time_per_epoch_list.append((time.time()-start)/60)\n",
    "    \n",
    "        print(f'Time: {(time.time()-start)/60:.3f} minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/transformers/v4.4.2/main_classes/trainer.html#trainingarguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"my_awesome_model\",\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=32, # was 32\n",
    "    per_device_eval_batch_size=32,  # was 32\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "# https://huggingface.co/transformers/v4.4.2/main_classes/trainer.html#id1\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_snli[\"train\"],\n",
    "    eval_dataset=tokenized_snli[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"C:/Users/zebzi/Documents/School/Senior_Year/CSCI 5541/NLP/modelSaver\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data for plotting\n",
    "t = np.arange(0.0, 100, 1)\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "plt.plot(train_loss_list, label=\"training loss\")\n",
    "plt.plot(train_acc_list, label=\"training accuracy\")\n",
    "# plt.plot(eval_loss_list, label=\"evaluation loss\")\n",
    "plt.plot(eval_acc_list, label=\"evaluation accuracy\")\n",
    "plt.legend()\n",
    "# ax.set(xlabel='time (s)', ylabel='voltage (mV)',\n",
    "#        title='About as simple as it gets, folks')\n",
    "plt.grid()\n",
    "\n",
    "# fig.savefig(\"test.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prediction is: contradiction\n"
     ]
    }
   ],
   "source": [
    "# Load the pipeline for natural language inference tasks\n",
    "nlp = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer, device=device)\n",
    "\n",
    "# Define your input sentences as strings\n",
    "premise = \"A person is playing a guitar.\"\n",
    "hypothesis = \"The person is singing.\"\n",
    "\n",
    "# Combine the premise and hypothesis into a single string\n",
    "text = premise + \" \" + hypothesis\n",
    "\n",
    "# Pass the input string through the pipeline to get the predicted label\n",
    "result = nlp(text)[0]\n",
    "\n",
    "# Print the prediction\n",
    "print(\"The prediction is:\", result[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The base prediction is:  LABEL_0\n"
     ]
    }
   ],
   "source": [
    "nlp_base = pipeline(\"sentiment-analysis\", model=\"bert-base-uncased\")\n",
    "result_base = nlp_base(text)[0]\n",
    "print(\"The base prediction is: \", result_base[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Premise: A person on a horse jumps over a broken down airplane\n",
      "Hypothesis:  A person is training his horse for a competition\n",
      "\n",
      "The prediction is:  contradiction\n",
      "The correct is:  neutral\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zebzi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\pipelines\\base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Premise: A person on a horse jumps over a broken down airplane\n",
      "Hypothesis:  A person is at a diner, ordering an omelette\n",
      "\n",
      "The prediction is:  contradiction\n",
      "The correct is:  entailment\n",
      "\n",
      "\n",
      "Premise: A person on a horse jumps over a broken down airplane\n",
      "Hypothesis:  A person is outdoors, on a horse\n",
      "\n",
      "The prediction is:  contradiction\n",
      "The correct is:  contradiction\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [19], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m temp \u001b[39m=\u001b[39m tokenized_snli[\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m][i]\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m premise \u001b[39m=\u001b[39m temp[\u001b[39m0\u001b[39m]\n\u001b[1;32m----> 5\u001b[0m hypothesis \u001b[39m=\u001b[39m temp[\u001b[39m1\u001b[39;49m]\n\u001b[0;32m      6\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mPremise: \u001b[39m\u001b[39m\"\u001b[39m,end\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[39mprint\u001b[39m(premise)\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "tokenized_snli[\"test\"][\"text\"][0]\n",
    "for i in range(10):\n",
    "    temp = tokenized_snli[\"train\"][\"text\"][i].split(\".\")\n",
    "    premise = temp[0]\n",
    "    hypothesis = temp[1]\n",
    "    print(\"Premise: \",end=\"\")\n",
    "    print(premise)\n",
    "    print(\"Hypothesis: \",end=\"\")\n",
    "    print(hypothesis)\n",
    "    print()\n",
    "    result = nlp(tokenized_snli[\"train\"][\"text\"][i])[0]\n",
    "    print(\"The prediction is: \", result[\"label\"])\n",
    "    print(\"The correct is: \", id2label[snli[\"test\"][\"label\"][i]])\n",
    "    print()\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "  premise = test_dataset[i][\"premise\"]\n",
    "  hypothesis = test_dataset[i][\"hypothesis\"]\n",
    "  print(\"Premise: \",end=\"\")\n",
    "  print(premise)\n",
    "  print(\"Hypothesis: \",end=\"\")\n",
    "  print(hypothesis)\n",
    "  print()\n",
    "  text = premise + \" \" + hypothesis\n",
    "  result = nlp(text)[0]\n",
    "  print(\"The prediction is: \", result[\"label\"])\n",
    "  print(\"The correct is: \", id2label[test_dataset[i][\"label\"]])\n",
    "  print()\n",
    "  print()\n",
    "  print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fb3845150f0d790535115e7834a43ebecc95258a3666952dae0331a6b302286d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
