{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForSequenceClassification,\\\n",
    "    TrainingArguments, Trainer, pipeline, DataCollatorWithPadding, set_seed\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_seed(42)\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset snli (C:/Users/zebzi/.cache/huggingface/datasets/snli/plain_text/1.0.0/1f60b67533b65ae0275561ff7828aad5ee4282d0e6f844fd148d05d3c6ea251b)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0781b13716124fd8a6de8816159604e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Get the dataset\n",
    "snli = load_dataset(\"snli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--train_text[0]--: A person on a horse jumps over a broken down airplane. A person is training his horse for a competition.\n",
      "--original + hypothesis--: A person on a horse jumps over a broken down airplane.A person is training his horse for a competition.\n",
      "\n",
      "--test_text[0]--: This church choir sings to the masses as they sing joyous songs from the book at a church. The church has cracks in the ceiling.\n",
      "--original + hypothesis--: This church choir sings to the masses as they sing joyous songs from the book at a church.The church has cracks in the ceiling.\n",
      "\n",
      "--valid_text[0]--: Two women are embracing while holding to go packages. The sisters are hugging goodbye while holding to go packages after just eating lunch.\n",
      "--original + hypothesis--: Two women are embracing while holding to go packages.The sisters are hugging goodbye while holding to go packages after just eating lunch.\n"
     ]
    }
   ],
   "source": [
    "# per chatGPT's suggestion, let's concatenate premise + hypothesis into a \"text\" column.\n",
    "train_text = np.char.add(np.char.add(snli['train']['premise'], ' '), snli['train']['hypothesis'])\n",
    "test_text = np.char.add(np.char.add(snli['test']['premise'], ' '), snli['test']['hypothesis'])\n",
    "valid_text = np.char.add(np.char.add(snli['validation']['premise'], ' '), snli['validation']['hypothesis'])\n",
    "\n",
    "# let's do a sanity check to make sure concatenation was done correctly\n",
    "print(f'--train_text[0]--: {train_text[0]}')\n",
    "print(f'--original + hypothesis--: {snli[\"train\"][\"premise\"][0]+snli[\"train\"][\"hypothesis\"][0]}\\n')\n",
    "\n",
    "print(f'--test_text[0]--: {test_text[0]}')\n",
    "print(f'--original + hypothesis--: {snli[\"test\"][\"premise\"][0]+snli[\"test\"][\"hypothesis\"][0]}\\n')\n",
    "\n",
    "print(f'--valid_text[0]--: {valid_text[0]}')\n",
    "print(f'--original + hypothesis--: {snli[\"validation\"][\"premise\"][0]+snli[\"validation\"][\"hypothesis\"][0]}')\n",
    "\n",
    "snli[\"train\"] = snli[\"train\"].remove_columns([\"premise\", \"hypothesis\"])\n",
    "snli[\"train\"] = snli[\"train\"].add_column(\"text\", train_text)\n",
    "\n",
    "snli[\"test\"] = snli[\"test\"].remove_columns([\"premise\", \"hypothesis\"])\n",
    "snli[\"test\"] = snli[\"test\"].add_column(\"text\", test_text)\n",
    "\n",
    "snli[\"validation\"] = snli[\"validation\"].remove_columns([\"premise\", \"hypothesis\"])\n",
    "snli[\"validation\"] = snli[\"validation\"].add_column(\"text\", valid_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\zebzi\\.cache\\huggingface\\datasets\\snli\\plain_text\\1.0.0\\1f60b67533b65ae0275561ff7828aad5ee4282d0e6f844fd148d05d3c6ea251b\\cache-a7e3f6a12dd5470b.arrow\n",
      "Loading cached processed dataset at C:\\Users\\zebzi\\.cache\\huggingface\\datasets\\snli\\plain_text\\1.0.0\\1f60b67533b65ae0275561ff7828aad5ee4282d0e6f844fd148d05d3c6ea251b\\cache-10263cd2a97d376b.arrow\n",
      "Loading cached processed dataset at C:\\Users\\zebzi\\.cache\\huggingface\\datasets\\snli\\plain_text\\1.0.0\\1f60b67533b65ae0275561ff7828aad5ee4282d0e6f844fd148d05d3c6ea251b\\cache-7dd2e0b9c4e30d43.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02a9d6721eb64771a4886b0afdf876e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a4e860545884887ad54e15bdafde341",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f20ab36d7b045f092c818aca31afaea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe3f8edf6ded4676b26a9571e53c8406",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\zebzi\\.cache\\huggingface\\datasets\\snli\\plain_text\\1.0.0\\1f60b67533b65ae0275561ff7828aad5ee4282d0e6f844fd148d05d3c6ea251b\\cache-6a07a2c8d2f1ad8c.arrow\n",
      "Loading cached processed dataset at C:\\Users\\zebzi\\.cache\\huggingface\\datasets\\snli\\plain_text\\1.0.0\\1f60b67533b65ae0275561ff7828aad5ee4282d0e6f844fd148d05d3c6ea251b\\cache-aa9d2d1182cd7304.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd755ac02b8949c19ae5a5a29b7a73e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "snli = snli.filter(lambda example: example[\"label\"] != -1)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True)\n",
    "    \n",
    "tokenized_snli= snli.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model that was trained in HW1\n",
    "id2label = {0: \"entailment\", 1: \"neutral\", 2:\"contradiction\"}\n",
    "label2id = {\"entailment\": 0, \"neutral\": 1, \"contradiction\": 2}\n",
    "\n",
    "config = AutoConfig.from_pretrained(\"config.json\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"pytorch_model.bin\", config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zebzi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\pipelines\\base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is i:  459\n"
     ]
    }
   ],
   "source": [
    "# Load the pipeline for natural language inference tasks\n",
    "nlp = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer, device=device)\n",
    "tokenized_snli[\"test\"][\"text\"][0]\n",
    "count = 0\n",
    "i = 0\n",
    "\n",
    "rowsList = []\n",
    "while(count < 50):\n",
    "    try:\n",
    "        temp = tokenized_snli[\"test\"][\"text\"][i + 100].split(\".\")\n",
    "        # print(tokenized_snli[\"test\"][\"text\"][i + 100])\n",
    "        premise = temp[0]\n",
    "        hypothesis = temp[1]\n",
    "    except:\n",
    "        print(\"no period\\n\")\n",
    "\n",
    "    result = nlp(tokenized_snli[\"test\"][\"text\"][i + 100])[0]\n",
    "    temp = []\n",
    "    if(result[\"label\"] != id2label[tokenized_snli[\"test\"][\"label\"][i+100]]):\n",
    "        count += 1\n",
    "        # print()\n",
    "        # print(\"Premise: \",end=\"\")\n",
    "        # print(premise)\n",
    "        # print(\"Hypothesis: \",end=\"\")\n",
    "        # print(hypothesis)\n",
    "        # print()\n",
    "        # print(f\"Confidence: {result['score']}\")\n",
    "        # print()\n",
    "        # print(\"The prediction is: \", result[\"label\"])\n",
    "        # print(\"The correct is: \", id2label[tokenized_snli[\"test\"][\"label\"][i+100]])\n",
    "        temp = [str(count), f\"Premise: {premise} \\nHypothesis: {hypothesis}\", id2label[tokenized_snli[\"test\"][\"label\"][i+100]], f\"Confidence: {result['score']:.3f} \\n\" + result['label'], str(len(premise) + len(hypothesis))]\n",
    "        rowsList.append(temp)\n",
    "    i += 1\n",
    "\n",
    "with open(\"hw5CSV.csv\", mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"\", \"Input Text\", \"Ground-Truth Label\", \"Predicted Label\", \"Input Text Length\"])\n",
    "    writer.writerows(rowsList)\n",
    "print(\"Here is i: \", i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
