{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zebzi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    LogitsProcessorList,\n",
    "    MinLengthLogitsProcessor,\n",
    "    StoppingCriteriaList,\n",
    "    MaxLengthCriteria,\n",
    "    AutoModelForSeq2SeqLM,\n",
    ")\n",
    "import numpy as np\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "# prompt = \"Today I believe we can finally\"\n",
    "prompt = \"It might be possible to\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zebzi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\generation\\utils.py:1186: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"It might be possible to get a better understanding of the nature of the problem, but it's not clear how to do that.\\n\\nThe problem\"]\n",
      "|   651 |  get     | -3.341 | 3.54%\n",
      "|   257 |  a       | -1.936 | 14.43%\n",
      "|  1365 |  better  | -3.269 | 3.80%\n",
      "|  4547 |  understanding | -1.486 | 22.63%\n",
      "|   286 |  of      | -0.151 | 86.03%\n",
      "|   262 |  the     | -1.247 | 28.73%\n",
      "|  3450 |  nature  | -4.258 | 1.41%\n",
      "|   286 |  of      | -0.083 | 92.02%\n",
      "|   262 |  the     | -1.289 | 27.57%\n",
      "|  1917 |  problem | -4.050 | 1.74%\n",
      "|    11 | ,        | -1.583 | 20.54%\n",
      "|   475 |  but     | -0.740 | 47.72%\n",
      "|   340 |  it      | -1.707 | 18.14%\n",
      "|   338 | 's       | -1.193 | 30.33%\n",
      "|   407 |  not     | -1.242 | 28.88%\n",
      "|  1598 |  clear   | -2.580 | 7.58%\n",
      "|   703 |  how     | -1.511 | 22.07%\n",
      "|   284 |  to      | -1.713 | 18.03%\n",
      "|   466 |  do      | -1.116 | 32.77%\n",
      "|   326 |  that    | -1.147 | 31.77%\n",
      "|    13 | .        | -0.663 | 51.54%\n",
      "|   198 | \n",
      "        | -1.139 | 32.01%\n",
      "|   198 | \n",
      "        | -0.002 | 99.84%\n",
      "|   464 | The      | -2.294 | 10.09%\n",
      "|  1917 |  problem | -2.906 | 5.47%\n",
      "Length of the output: 25\n",
      "Perplexity: 5.505372557671196\n",
      "likelihood: 0.7923689491236807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zebzi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\generation\\utils.py:1029: UserWarning: Use of index_put_ on expanded tensors is deprecated. Please clone() the tensor before performing this operation. This also applies to advanced indexing e.g. tensor[indices] = tensor (Triggered internally at ..\\aten\\src\\ATen\\native\\TensorAdvancedIndexing.cpp:634.)\n",
      "  beam_indices[beam_indices_mask] = 0\n",
      "c:\\Users\\zebzi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\generation\\utils.py:1029: UserWarning: Use of masked_fill_ on expanded tensors is deprecated. Please clone() the tensor before performing this operation. This also applies to advanced indexing e.g. tensor[mask] = scalar (Triggered internally at ..\\aten\\src\\ATen\\native\\TensorAdvancedIndexing.cpp:1654.)\n",
      "  beam_indices[beam_indices_mask] = 0\n"
     ]
    }
   ],
   "source": [
    "#Greedy Search TASK 1\n",
    "\n",
    "outputs = model.generate(input_ids, num_beams=1, do_sample=False, max_length=30,return_dict_in_generate=True, output_scores=True)\n",
    "transition_scores = model.compute_transition_scores(outputs.sequences, outputs.scores, normalize_logits=True)\n",
    "print(tokenizer.batch_decode(outputs[0], skip_special_tokens=True))\n",
    "\n",
    "input_length = 1 if model.config.is_encoder_decoder else input_ids.shape[1]\n",
    "generated_tokens = outputs.sequences[:, input_length:]\n",
    "\n",
    "perplexity = 0\n",
    "likelihood = 0\n",
    "\n",
    "for tok, score in zip(generated_tokens[0], transition_scores[0]):\n",
    "    # | token | token string | logits | probability\n",
    "    print(f\"| {tok:5d} | {tokenizer.decode(tok):8s} | {score.detach().numpy():.3f} | {np.exp(score.detach().numpy()):.2%}\")\n",
    "    likelihood += np.log(-1 * score.detach().numpy())\n",
    "    perplexity += np.log(np.exp(score.detach().numpy()))\n",
    "\n",
    "print(f\"Length of the output: {generated_tokens.shape[1]}\")\n",
    "print(f\"Perplexity: {np.exp((-1/ generated_tokens.shape[1]) * perplexity)}\")\n",
    "print(f\"likelihood: {likelihood}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"It might be possible to get a better understanding of how the system works, but it's not going to be easy.\\n\\nIn the meantime,\"]\n",
      "|   651 |  get     | -3.341 | 3.54%\n",
      "|   257 |  a       | -1.936 | 14.43%\n",
      "|  1365 |  better  | -3.269 | 3.80%\n",
      "|  4547 |  understanding | -1.486 | 22.63%\n",
      "|   286 |  of      | -0.151 | 86.02%\n",
      "|   703 |  how     | -1.795 | 16.61%\n",
      "|   262 |  the     | -1.791 | 16.67%\n",
      "|  1080 |  system  | -4.110 | 1.64%\n",
      "|  2499 |  works   | -0.338 | 71.32%\n",
      "|    11 | ,        | -1.423 | 24.09%\n",
      "|   475 |  but     | -0.623 | 53.65%\n",
      "|   340 |  it      | -1.656 | 19.08%\n",
      "|   338 | 's       | -0.925 | 39.67%\n",
      "|   407 |  not     | -1.277 | 27.90%\n",
      "|  1016 |  going   | -2.657 | 7.02%\n",
      "|   284 |  to      | -0.006 | 99.38%\n",
      "|   307 |  be      | -1.001 | 36.75%\n",
      "|  2562 |  easy    | -1.028 | 35.76%\n",
      "|    13 | .        | -0.485 | 61.58%\n",
      "|   198 | \n",
      "        | -1.203 | 30.02%\n",
      "|   198 | \n",
      "        | -0.001 | 99.91%\n",
      "|   818 | In       | -3.445 | 3.19%\n",
      "|   262 |  the     | -1.601 | 20.17%\n",
      "| 14324 |  meantime | -1.308 | 27.05%\n",
      "|    11 | ,        | -0.036 | 96.45%\n",
      "Length of the output: 25\n",
      "Perplexity: 4.37375969279929\n",
      "likelihood: -9.344831745940242\n"
     ]
    }
   ],
   "source": [
    "#Beam Search TASK 1\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    num_beams=3,\n",
    "    early_stopping=True,\n",
    "    max_length=30,\n",
    "    return_dict_in_generate=True,\n",
    "    output_scores=True,\n",
    ")\n",
    "transition_scores = model.compute_transition_scores(\n",
    "    outputs.sequences, outputs.scores, outputs.beam_indices, normalize_logits=False\n",
    ")\n",
    "# If you sum the generated tokens' scores and apply the length penalty, you'll get the sequence scores.\n",
    "# Tip: recomputing the scores is only guaranteed to match with `normalize_logits=False`. Depending on the\n",
    "# use case, you might want to recompute it with `normalize_logits=True`.\n",
    "output_length = input_length + np.sum(transition_scores.numpy() < 0, axis=1)\n",
    "length_penalty = model.generation_config.length_penalty\n",
    "reconstructed_scores = transition_scores.sum(axis=1) / (output_length**length_penalty)\n",
    "\n",
    "print(tokenizer.batch_decode(outputs[0], skip_special_tokens=True))\n",
    "\n",
    "# input_length = 1 if model.config.is_encoder_decoder else input_ids.shape[1]\n",
    "generated_tokens = outputs.sequences[:, input_length:]\n",
    "\n",
    "perplexity = 0\n",
    "likelihood = 0\n",
    "\n",
    "for tok, score in zip(generated_tokens[0], transition_scores[0]):\n",
    "    # | token | token string | logits | probability\n",
    "    print(f\"| {tok:5d} | {tokenizer.decode(tok):8s} | {score.detach().numpy():.3f} | {np.exp(score.detach().numpy()):.2%}\")\n",
    "    likelihood += np.log(-1 * score.detach().numpy())\n",
    "    perplexity += np.log(np.exp(score.detach().numpy()))\n",
    "\n",
    "print(f\"Length of the output: {generated_tokens.shape[1]}\")\n",
    "print(f\"Perplexity: {np.exp((-1/ generated_tokens.shape[1]) * perplexity)}\")\n",
    "print(f\"likelihood: {likelihood}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"It might be possible to make a new game with these features but it would be very hard to do. You'd have to learn something new about coding\"]\n",
      "|   787 |  make    | -2.534 | 7.93%\n",
      "|   257 |  a       | -1.169 | 31.07%\n",
      "|   649 |  new     | -3.294 | 3.71%\n",
      "|   983 |  game    | -2.451 | 8.62%\n",
      "|   351 |  with    | -1.891 | 15.09%\n",
      "|   777 |  these   | -3.688 | 2.50%\n",
      "|  3033 |  features | -2.157 | 11.57%\n",
      "|   475 |  but     | -3.300 | 3.69%\n",
      "|   340 |  it      | -1.692 | 18.42%\n",
      "|   561 |  would   | -1.570 | 20.81%\n",
      "|   307 |  be      | -1.086 | 33.75%\n",
      "|   845 |  very    | -1.930 | 14.51%\n",
      "|  1327 |  hard    | -1.324 | 26.61%\n",
      "|   284 |  to      | -0.510 | 60.08%\n",
      "|   466 |  do      | -1.742 | 17.52%\n",
      "|    13 | .        | -3.164 | 4.22%\n",
      "|   921 |  You     | -4.072 | 1.70%\n",
      "|  1549 | 'd       | -2.950 | 5.23%\n",
      "|   423 |  have    | -0.649 | 52.24%\n",
      "|   284 |  to      | -0.034 | 96.68%\n",
      "|  2193 |  learn   | -4.167 | 1.55%\n",
      "|  1223 |  something | -4.189 | 1.52%\n",
      "|   649 |  new     | -0.601 | 54.84%\n",
      "|   546 |  about   | -2.045 | 12.94%\n",
      "| 19617 |  coding  | -4.931 | 0.72%\n",
      "Length of the output: 25\n",
      "Perplexity: 9.831358447842083\n",
      "likelihood: 13.437232224226383\n"
     ]
    }
   ],
   "source": [
    "#Top-K Sampling TASK 1\n",
    "\n",
    "outputs = model.generate(input_ids, do_sample=True, max_length=30, return_dict_in_generate=True, output_scores=True, top_k=50)\n",
    "\n",
    "print(tokenizer.batch_decode(outputs[0], skip_special_tokens=True))\n",
    "\n",
    "input_length = 1 if model.config.is_encoder_decoder else input_ids.shape[1]\n",
    "generated_tokens = outputs.sequences[:, input_length:]\n",
    "\n",
    "transition_scores = model.compute_transition_scores(outputs.sequences, outputs.scores, normalize_logits=True)\n",
    "\n",
    "perplexity = 0\n",
    "likelihood = 0\n",
    "\n",
    "for tok, score in zip(generated_tokens[0], transition_scores[0]):\n",
    "    # | token | token string | logits | probability\n",
    "    print(f\"| {tok:5d} | {tokenizer.decode(tok):8s} | {score.detach().numpy():.3f} | {np.exp(score.detach().numpy()):.2%}\")\n",
    "    likelihood += np.log(-1 * score.detach().numpy())\n",
    "    perplexity += np.log(np.exp(score.detach().numpy()))\n",
    "\n",
    "print(f\"Length of the output: {generated_tokens.shape[1]}\")\n",
    "print(f\"Perplexity: {np.exp((-1/ generated_tokens.shape[1]) * perplexity)}\")\n",
    "print(f\"likelihood: {likelihood}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It might be possible to play rythm from here:\\n\\nThe man sat on the bench and gave the people what they asked for. This is']\n",
      "|   711 |  play    | -5.174 | 0.57%\n",
      "|   374 |  r       | -8.634 | 0.02%\n",
      "| 34853 | ythm     | -3.827 | 2.18%\n",
      "|   422 |  from    | -5.755 | 0.32%\n",
      "|   994 |  here    | -4.302 | 1.35%\n",
      "|    25 | :        | -4.675 | 0.93%\n",
      "|   198 | \n",
      "        | -0.955 | 38.50%\n",
      "|   198 | \n",
      "        | 0.000 | 100.00%\n",
      "|   464 | The      | -3.849 | 2.13%\n",
      "|   582 |  man     | -7.734 | 0.04%\n",
      "|  3332 |  sat     | -7.012 | 0.09%\n",
      "|   319 |  on      | -1.629 | 19.62%\n",
      "|   262 |  the     | -0.707 | 49.29%\n",
      "|  7624 |  bench   | -2.561 | 7.72%\n",
      "|   290 |  and     | -2.089 | 12.38%\n",
      "|  2921 |  gave    | -4.772 | 0.85%\n",
      "|   262 |  the     | -2.039 | 13.01%\n",
      "|   661 |  people  | -6.264 | 0.19%\n",
      "|   644 |  what    | -3.606 | 2.72%\n",
      "|   484 |  they    | -0.325 | 72.25%\n",
      "|  1965 |  asked   | -2.079 | 12.51%\n",
      "|   329 |  for     | -0.614 | 54.12%\n",
      "|    13 | .        | -0.703 | 49.50%\n",
      "|   770 |  This    | -4.817 | 0.81%\n",
      "|   318 |  is      | -1.612 | 19.94%\n",
      "Length of the output: 25\n",
      "Perplexity: 30.859925518080345\n",
      "likelihood: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zebzi\\AppData\\Local\\Temp\\ipykernel_18896\\2684038281.py:19: RuntimeWarning: divide by zero encountered in log\n",
      "  likelihood += np.log(-1 * score.detach().numpy())\n"
     ]
    }
   ],
   "source": [
    "#Top-P Sampling TASK 1 \n",
    "#Need to figure out a good value for top_p\n",
    "#top_p = 4 gave good values but it's supposed to be bounded (0,1)\n",
    "\n",
    "outputs = model.generate(input_ids, top_p = 0.92, top_k=0, do_sample=True, max_length=30, return_dict_in_generate=True, output_scores=True)\n",
    "print(tokenizer.batch_decode(outputs[0], skip_special_tokens=True))\n",
    "\n",
    "input_length = 1 if model.config.is_encoder_decoder else input_ids.shape[1]\n",
    "generated_tokens = outputs.sequences[:, input_length:]\n",
    "\n",
    "transition_scores = model.compute_transition_scores(outputs.sequences, outputs.scores, normalize_logits=True)\n",
    "\n",
    "perplexity = 0\n",
    "likelihood = 0\n",
    "\n",
    "for tok, score in zip(generated_tokens[0], transition_scores[0]):\n",
    "    # | token | token string | logits | probability\n",
    "    print(f\"| {tok:5d} | {tokenizer.decode(tok):8s} | {score.detach().numpy():.3f} | {np.exp(score.detach().numpy()):.2%}\")\n",
    "    likelihood += np.log(-1 * score.detach().numpy())\n",
    "    perplexity += np.log(np.exp(score.detach().numpy()))\n",
    "\n",
    "print(f\"Length of the output: {generated_tokens.shape[1]}\")\n",
    "print(f\"Perplexity: {np.exp((-1/ generated_tokens.shape[1]) * perplexity)}\")\n",
    "print(f\"likelihood: {likelihood}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**************************************************\n",
    "Task 2 starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset cnn_dailymail (C:/Users/zebzi/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de)\n",
      "100%|██████████| 3/3 [00:00<00:00,  9.37it/s]\n"
     ]
    }
   ],
   "source": [
    "#TASK 2\n",
    "#Load the dataset\n",
    "\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
    "\n",
    "max_length = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TASK 2 (switched from a downstream BERT because things were failing)\n",
    "tokenizer2 = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "model2 = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-large-cnn\")#, pad_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(CNN)The Palestinian Authority officially became the 123rd member of the International Criminal Court on Wednesday, a step that gives the court jurisdiction over alleged crimes in Palestinian territories. The formal accession was marked with a ceremony at The Hague, in the Netherlands, where the court is based. The Palestinians signed the ICC's founding Rome Statute in January, when they also accepted its jurisdiction over alleged crimes committed \"in the occupied Palestinian territory, including East Jerusalem, since June 13, 2014.\" Later that month, the ICC opened a preliminary examination into the situation in Palestinian territories, paving the way for possible war crimes investigations against Israelis. As members of the court, Palestinians may be subject to counter-charges as well. Israel and the United States, neither of which is an ICC member, opposed the Palestinians' efforts to join the body. But Palestinian Foreign Minister Riad al-Malki, speaking at Wednesday's ceremony, said it was a move toward greater justice. \"As Palestine formally becomes a State Party to the Rome Statute today, the world is also a step closer to ending a long era of impunity and injustice,\" he said, according to an ICC news release. \"Indeed, today brings us closer to our shared goals of justice and peace.\" Judge Kuniko Ozaki, a vice president of the ICC, said acceding to the treaty was just the first step for the Palestinians. \"As the Rome Statute today enters into force for the State of Palestine, Palestine acquires all the rights as well as responsibilities that come with being a State Party to the Statute. These are substantive commitments, which cannot be taken lightly,\" she said. Rights group Human Rights Watch welcomed the development. \"Governments seeking to penalize Palestine for joining the ICC should immediately end their pressure, and countries that support universal acceptance of the court's treaty should speak out to welcome its membership,\" said Balkees Jarrah, international justice counsel for the group. \"What's objectionable is the attempts to undermine international justice, not Palestine's decision to join a treaty to which over 100 countries around the world are members.\" In January, when the preliminary ICC examination was opened, Israeli Prime Minister Benjamin Netanyahu described it as an outrage, saying the court was overstepping its boundaries. The United States also said it \"strongly\" disagreed with the court's decision. \"As we have said repeatedly, we do not believe that Palestine is a state and therefore we do not believe that it is eligible to join the ICC,\" the State Department said in a statement. It urged the warring sides to resolve their differences through direct negotiations. \"We will continue to oppose actions against Israel at the ICC as counterproductive to the cause of peace,\" it said. But the ICC begs to differ with the definition of a state for its purposes and refers to the territories as \"Palestine.\" While a preliminary examination is not a formal investigation, it allows the court to review evidence and determine whether to investigate suspects on both sides. Prosecutor Fatou Bensouda said her office would \"conduct its analysis in full independence and impartiality.\" The war between Israel and Hamas militants in Gaza last summer left more than 2,000 people dead. The inquiry will include alleged war crimes committed since June. The International Criminal Court was set up in 2002 to prosecute genocide, crimes against humanity and war crimes. CNN's Vasco Cotovio, Kareem Khadder and Faith Karimi contributed to this report.\n",
      "<s>(CNN)The Palestinian Authority officially became the 123rd member of the International Criminal Court on Wednesday, a step that gives the court jurisdiction over alleged crimes in Palestinian territories. The formal accession was marked with a ceremony at The Hague, in the Netherlands, where the court is based. The Palestinians signed the ICC's founding Rome Statute in January, when they also accepted its jurisdiction over alleged crimes committed \"in the occupied Palestinian territory, including East Jerusalem, since June 13, 2014.\" Later that month, the ICC opened a preliminary examination into the situation in Palestinian territories, paving the way for possible war crimes investigations against Israelis. As members of the court, Palestinians may be subject to counter-charges as well. Israel and the United States, neither of which is an ICC member, opposed the Palestinians' efforts to join the body. But Palestinian Foreign Minister Riad al-Malki, speaking at Wednesday's ceremony, said it was a move toward greater justice. \"As Palestine formally becomes a State Party to the Rome Statute today, the world is also a step closer to ending a long era of impunity and injustice,\" he said, according to an ICC news release. \"Indeed, today brings us closer to our shared goals of justice and peace.\" Judge Kuniko Ozaki, a vice president of the ICC, said acceding to the treaty was just the first step for the Palestinians. \"As the Rome Statute today enters into force for the State of Palestine, Palestine acquires all the rights as well as responsibilities that come with being a State Party to the Statute. These are substantive commitments, which cannot be taken lightly,\" she said. Rights group Human Rights Watch welcomed the development. \"Governments seeking to penalize Palestine for joining the ICC should immediately end their pressure, and countries that support universal acceptance of the court's treaty should speak out to welcome its membership,\" said Balkees Jarrah, international justice counsel for the group. \"What's objectionable is the attempts to undermine international justice, not Palestine's decision to join a treaty to which over 100 countries around the world are members.\" In January, when the preliminary ICC examination was opened, Israeli Prime Minister Benjamin Netanyahu described it as an outrage, saying the court was overstepping its boundaries. The United States also said it \"strongly\" disagreed with the court's decision. \"As we have said repeatedly, we do not believe that Palestine is a state and therefore we do not believe that it is eligible to join the ICC,\" the State Department said in a statement. It urged the warring sides to resolve their differences through direct negotiations. \"We will continue to oppose actions against Israel at the ICC as counterproductive to the cause of peace,\" it said. But the ICC begs to differ with the definition of a state for its purposes and refers to the territories as \"Palestine.\" While a preliminary examination is not a formal investigation, it allows the court to review evidence and determine whether to investigate suspects on both sides. Prosecutor Fatou Bensouda said her office would \"conduct its analysis in full independence and impartiality.\" The war between Israel and Hamas militants in Gaza last summer left more than 2,000 people dead. The inquiry will include alleged war crimes committed since June. The International Criminal Court was set up in 2002 to prosecute genocide, crimes against humanity and war crimes. CNN's Vasco Cotovio, Kareem Khadder and Faith Karimi contributed to this report.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    }
   ],
   "source": [
    "# input_ids = dataset[\"test\"][:50]\n",
    "tokenizerInputs =  []\n",
    "inputGroundTruths = []\n",
    "for i in range(50):\n",
    "    tokenizerInputs.append(dataset[\"test\"][i][\"article\"])\n",
    "    inputGroundTruths.append(dataset[\"test\"][i][\"highlights\"])\n",
    "print(tokenizerInputs[0])\n",
    "\n",
    "encoder_input_ids = torch.LongTensor()\n",
    "\n",
    "for i in range(50):\n",
    "    encoder_input_ids = torch.cat((encoder_input_ids, tokenizer2(tokenizerInputs[i], return_tensors=\"pt\", padding='max_length', truncation=True).input_ids))\n",
    "print(tokenizer2.decode(encoder_input_ids[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task 2 \n",
    "outputs2 = []\n",
    "#Greedy Search\n",
    "outputs2.append(model2.generate(encoder_input_ids, num_beams=1, do_sample=False, max_length=max_length, no_repeat_ngram_size=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Beam Search\n",
    "outputs2.append(model2.generate(encoder_input_ids, num_beams=3, early_stopping=True, max_length=max_length, no_repeat_ngram_size=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Top-K Sampling\n",
    "outputs2.append(model2.generate(encoder_input_ids, do_sample=True, top_k=30, max_length=max_length, no_repeat_ngram_size=2))\n",
    "\n",
    "#max_length = 50 with top_k>=40 gave the \"index out of range in self\" error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Top-P Sampling\n",
    "outputs2.append(model2.generate(encoder_input_ids, do_sample=True, top_p=0.8, top_k=0, max_length=max_length, no_repeat_ngram_size=2)) \n",
    "\n",
    "#max_length = 30 with top_p>=0.8 gave the \"index out of range in self\" error\n",
    "#max_length = 50 with top_p>=0.4 gave the \"index out of range in self\" error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1026,  1244,   307,  1744,   284,   711,   374, 34853,   422,   994,\n",
      "           25,   198,   198,   464,   582,  3332,   319,   262,  7624,   290,\n",
      "         2921,   262,   661,   644,   484,  1965,   329,    13,   770,   318])\n",
      "The Palestinian Authority becomes the 123rd member of the International Criminal Court. The move gives the court jurisdiction over alleged crimes in Palestinian territories. Israel and the United States opposed the Palestinians' efforts to join the body. But Palestinian Foreign Minister Riad al-Malki says it is a move toward greater justice.\n",
      "Palestinian Authority becomes 123rd member of the International Criminal Court. The move gives the court jurisdiction over alleged crimes in Palestinian territories. Israel and the United States opposed the Palestinians' efforts to join the body. But Palestinian Foreign Minister Riad al-Malki said it was a move toward greater justice.\n",
      "The Palestinian Authority becomes the 123rd member of the International Criminal Court. The move gives the court jurisdiction over alleged crimes in Palestinian territories. Israel and the United States opposed the Palestinians' efforts to join the body. But Palestinian Foreign Minister Riad al-Malki said it was a move toward greater justice.\n",
      "The Palestinian Authority becomes the 123rd member of the International Criminal Court. The move gives the court jurisdiction over alleged crimes in Palestinian territories. Israel and the United States, neither of which is an ICC member, opposed the Palestinians' efforts to join the body. But Palestinian Foreign Minister Riad al-Malki said it was a move toward greater justice.\n"
     ]
    }
   ],
   "source": [
    "print(outputs[0][0])\n",
    "print(tokenizer2.decode(outputs2[0][0], skip_special_tokens=True))\n",
    "print(tokenizer2.decode(outputs2[1][0], skip_special_tokens=True))\n",
    "print(tokenizer2.decode(outputs2[2][0], skip_special_tokens=True))\n",
    "print(tokenizer2.decode(outputs2[3][0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\zebzi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\zebzi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\zebzi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from evaluate import load\n",
    "meteorScore = load(\"meteor\")\n",
    "bertScore = load(\"bertscore\")\n",
    "rougeScore = load(\"rouge\")\n",
    "perplexityScore = load(\"perplexity\", module_type=\"metric\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.4.fc1.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.shared.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.8.fc1.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.07it/s]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.4.fc1.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.shared.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.8.fc1.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.55it/s]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.4.fc1.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.shared.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.8.fc1.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.20s/it]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.4.fc1.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.shared.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.8.fc1.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.66it/s]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.4.fc1.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.shared.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.8.fc1.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.13s/it]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.4.fc1.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.shared.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.8.fc1.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.43it/s]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.4.fc1.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.shared.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.8.fc1.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.04s/it]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.4.fc1.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.shared.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.8.fc1.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.15it/s]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.4.fc1.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.shared.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.8.fc1.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.13s/it]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.4.fc1.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.shared.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.8.fc1.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.32it/s]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.4.fc1.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.shared.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.8.fc1.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.25s/it]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.4.fc1.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.shared.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.8.fc1.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.39it/s]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.4.fc1.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.shared.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.8.fc1.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.20s/it]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.4.fc1.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.shared.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.8.fc1.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.93it/s]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.4.fc1.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.shared.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.8.fc1.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.12s/it]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.4.fc1.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.shared.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.8.fc1.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.74it/s]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.4.fc1.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.shared.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.8.fc1.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.02s/it]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.4.fc1.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.shared.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.8.fc1.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.32it/s]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.4.fc1.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.shared.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.8.fc1.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.25s/it]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.4.fc1.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.shared.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.8.fc1.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.96it/s]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.4.fc1.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.shared.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.8.fc1.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.01it/s]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.4.fc1.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.shared.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.8.fc1.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.92it/s]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.4.fc1.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.shared.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.8.fc1.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.24s/it]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.4.fc1.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.shared.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.8.fc1.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.76it/s]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.4.fc1.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.shared.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.8.fc1.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.25s/it]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.4.fc1.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.shared.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.8.fc1.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.58it/s]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.4.fc1.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.shared.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.8.fc1.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.46s/it]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.4.fc1.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.shared.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.8.fc1.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.29it/s]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.4.fc1.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.shared.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.8.fc1.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.07s/it]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.4.fc1.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.shared.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.8.fc1.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:00<00:00,  6.29it/s]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.4.fc1.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.shared.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.8.fc1.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.34s/it]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.4.fc1.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.shared.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.8.fc1.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.65it/s]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.4.fc1.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.shared.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.8.fc1.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.24s/it]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.4.fc1.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.shared.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.8.fc1.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.56it/s]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.4.fc1.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.shared.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.8.fc1.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.33s/it]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.4.fc1.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.shared.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.8.fc1.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.94it/s]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.4.fc1.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.shared.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.8.fc1.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.22s/it]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.4.fc1.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.shared.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.8.fc1.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.64it/s]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.4.fc1.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.shared.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.8.fc1.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.17it/s]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.4.fc1.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.shared.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.8.fc1.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.18it/s]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.4.fc1.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.shared.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.8.fc1.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.29s/it]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.4.fc1.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.shared.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.8.fc1.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.19it/s]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.4.fc1.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.shared.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.8.fc1.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.46s/it]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.4.fc1.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.shared.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.8.fc1.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.79it/s]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.4.fc1.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.shared.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.8.fc1.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.09s/it]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.4.fc1.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.shared.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.8.fc1.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.91it/s]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.4.fc1.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.shared.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.8.fc1.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.28s/it]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.4.fc1.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.shared.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.8.fc1.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.50it/s]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.4.fc1.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.shared.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.8.fc1.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.15s/it]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.4.fc1.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.shared.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.8.fc1.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.26it/s]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.4.fc1.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.shared.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.8.fc1.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.43s/it]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.4.fc1.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.shared.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.8.fc1.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.55it/s]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.4.fc1.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.shared.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.8.fc1.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.22s/it]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.4.fc1.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.shared.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.8.fc1.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.53it/s]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.4.fc1.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.shared.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.8.fc1.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.19s/it]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.4.fc1.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.shared.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.8.fc1.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:00<00:00,  6.37it/s]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.4.fc1.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.shared.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.8.fc1.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.11s/it]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.4.fc1.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.shared.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.8.fc1.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.55it/s]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.4.fc1.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.shared.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.8.fc1.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.18s/it]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.4.fc1.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.shared.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.8.fc1.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.78it/s]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.4.fc1.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.shared.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.8.fc1.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.11s/it]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.4.fc1.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.shared.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.8.fc1.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.82it/s]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.4.fc1.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.shared.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.8.fc1.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.35s/it]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.4.fc1.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.shared.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.8.fc1.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.12it/s]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.4.fc1.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.shared.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.8.fc1.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.31s/it]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.4.fc1.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.shared.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.8.fc1.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:00<00:00,  6.16it/s]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.4.fc1.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.shared.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.8.fc1.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.24s/it]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.4.fc1.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.shared.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.8.fc1.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.82it/s]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.4.fc1.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.shared.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.8.fc1.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.16s/it]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.4.fc1.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.shared.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.8.fc1.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.31it/s]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.4.fc1.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.shared.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.8.fc1.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.33s/it]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.4.fc1.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.shared.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.8.fc1.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.56it/s]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.4.fc1.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.shared.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.8.fc1.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.02s/it]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.4.fc1.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.shared.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.8.fc1.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.78it/s]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.4.fc1.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.shared.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.8.fc1.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.22s/it]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.4.fc1.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.shared.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.8.fc1.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.70it/s]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.4.fc1.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.shared.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.8.fc1.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.24s/it]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.4.fc1.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.shared.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.8.fc1.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.50it/s]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.4.fc1.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.shared.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.8.fc1.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.26s/it]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.4.fc1.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.shared.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.8.fc1.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.55it/s]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.4.fc1.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.shared.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.8.fc1.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.04s/it]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.4.fc1.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.shared.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.8.fc1.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:00<00:00,  6.36it/s]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.4.fc1.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.shared.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.8.fc1.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.25s/it]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.4.fc1.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.shared.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.8.fc1.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.78it/s]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.4.fc1.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.shared.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.8.fc1.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.17s/it]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.4.fc1.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.shared.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.8.fc1.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.35it/s]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.4.fc1.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.shared.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.8.fc1.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.17s/it]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.4.fc1.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.shared.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.8.fc1.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.82it/s]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.4.fc1.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.shared.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.8.fc1.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.26s/it]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.4.fc1.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.shared.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.8.fc1.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.08it/s]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.4.fc1.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.shared.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.8.fc1.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.24s/it]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.4.fc1.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.shared.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.8.fc1.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.38it/s]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.4.fc1.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.shared.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.8.fc1.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.09s/it]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.4.fc1.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.shared.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.8.fc1.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.86it/s]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.4.fc1.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.shared.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.8.fc1.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.31s/it]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.4.fc1.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.shared.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.8.fc1.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.75it/s]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.4.fc1.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.shared.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.8.fc1.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.20s/it]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.4.fc1.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.shared.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.8.fc1.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.31it/s]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.4.fc1.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.shared.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.8.fc1.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.01s/it]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.4.fc1.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.shared.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.8.fc1.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.20it/s]\n"
     ]
    }
   ],
   "source": [
    "references = []\n",
    "predictions = []\n",
    "resultsMeteor = []\n",
    "resultsBert = []\n",
    "resultsRouge = []\n",
    "resultsPerplexity = []\n",
    "groundPerplexity = []\n",
    "\n",
    "# references.append([inputGroundTruths[0], inputGroundTruths[0], inputGroundTruths[0], inputGroundTruths[0]])\n",
    "# predictions.append([tokenizer2.decode(outputs2[0][0], skip_special_tokens=True), tokenizer2.decode(outputs2[1][0], skip_special_tokens=True), tokenizer2.decode(outputs2[2][0], skip_special_tokens=True), tokenizer2.decode(outputs2[3][0], skip_special_tokens=True)])\n",
    "# results.append([roguescore.compute(predictions=predictions[0], references=references[0])])\n",
    "# results.append([roguescore.compute(predictions=[tokenizer2.decode(outputs2[0][0], skip_special_tokens=True)], references=[inputGroundTruths[0]])])\n",
    "# results.append([roguescore.compute(predictions=[\"hey\"], references=[[\"hey\"]])])\n",
    "\n",
    "# print(predictions[0])\n",
    "# print(references[0])\n",
    "# print(results)\n",
    "# print(outputs2)\n",
    "with open(\"generatedText12.txt\", \"w\") as f:\n",
    "    for i in range(len(outputs2[0])):\n",
    "        references.append([inputGroundTruths[i], inputGroundTruths[i], inputGroundTruths[i], inputGroundTruths[i]])\n",
    "        predictions.append([tokenizer2.decode(outputs2[0][i], skip_special_tokens=True), tokenizer2.decode(outputs2[1][i], skip_special_tokens=True), tokenizer2.decode(outputs2[2][i], skip_special_tokens=True), tokenizer2.decode(outputs2[3][i], skip_special_tokens=True)])\n",
    "        resultsBert.append([bertScore.compute(predictions=predictions[i], references=references[i], model_type=\"t5-base\")])\n",
    "        resultsRouge.append([rougeScore.compute(predictions=predictions[i], references=references[i], use_aggregator=False)])\n",
    "        resultsPerplexity.append([perplexityScore.compute(predictions=predictions[i], model_id=\"facebook/bart-large-cnn\")])\n",
    "        groundPerplexity.append([perplexityScore.compute(predictions=[references[i][0]], model_id=\"facebook/bart-large-cnn\")])\n",
    "        # print(predictions[i][0])\n",
    "        # print(references[i][0])\n",
    "        temp = []\n",
    "        for j in range(4):\n",
    "            temp.append([meteorScore.compute(predictions=[predictions[i][j]], references=[references[i][j]])])\n",
    "        resultsMeteor.append(temp)\n",
    "\n",
    "        f.write(f\"Ground Truth: {inputGroundTruths[i]} \\n\")\n",
    "        f.write(f\"Greedy Search: {predictions[i][0]} \\n\")\n",
    "        f.write(f\"Beam Search: {predictions[i][1]} \\n\")\n",
    "        f.write(f\"Top-K Sampling: {predictions[i][2]} \\n\")\n",
    "        f.write(f\"Top-P Sampling: {predictions[i][3]} \\n\")\n",
    "        f.write(f\"Meteor Score: {resultsMeteor[i]} \\n\")\n",
    "        f.write(f\"Bert Score: {resultsBert[i]} \\n\")\n",
    "        f.write(f\"Rouge Score: {resultsRouge[i]} \\n\")\n",
    "        f.write(f\"Perplexty: {resultsPerplexity[i]} \\n\")\n",
    "        f.write(f\"Ground Perplexity {groundPerplexity[i]} \\n\")\n",
    "        f.write(\"\\n \\n\")\n",
    "\n",
    "#generatedText is with no capped max_length so it defaulted to 20 I think top_p=0.92 and top_k=50\n",
    "#generatedText2 is with max_length = 30\n",
    "#generatedText3 is also max_length = 30 but top_p=0.7 ~ 6mins of runtime\n",
    "#generatedText4 is at max_length = 40 with top_p=0.7 ~5mins of runtime thanks to CPU acceleration\n",
    "#generatedText5 is at max_length = 50 with top_p=0.3 and top_k=25 ~Xmins of runtime but model_max_length = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open(\"outputs4.csv\", 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Ground Truth\", \"Greedy Search\", \"Beam Search\", \"Top-K Sampling\", \"Top-P Sampling\", \" \", \" \", \"Meteor Score\", \"BERT Precision Score\", \"Rouge1 Score\", \"Perplexity of Predicitons\", \"Perplexity of Ground Truth\"])\n",
    "\n",
    "    for i in range(len(predictions)):\n",
    "        writer.writerow([inputGroundTruths[i], predictions[i][0], predictions[i][1], predictions[i][2], predictions[i][3], \" \", \" \", \"Meteor Greedy: \" + str(resultsMeteor[i][0][0][\"meteor\"]) + \"\\n Meteor Beam: \" + str(resultsMeteor[i][1][0][\"meteor\"]) + \"\\n Meteor Top-K: \" + str(resultsMeteor[i][2][0][\"meteor\"]) + \"\\n Meteor Top-P: \" + str(resultsMeteor[i][3][0][\"meteor\"]), \"BERT Greedy Precision \" + str(resultsBert[i][0][\"precision\"][0]) + \"\\nBERT Beam Precision \" + str(resultsBert[i][0][\"precision\"][1]) + \"\\nBERT Top-K Precision \" + str(resultsBert[i][0][\"precision\"][2]) + \"\\nBERT Top-P Precision \" + str(resultsBert[i][0][\"precision\"][3]),\"Rouge1 Greedy: \" + str(resultsRouge[i][0][\"rouge1\"][0]) + \"\\nRouge1 Beam: \" + str(resultsRouge[i][0][\"rouge1\"][1]) + \"\\nRouge1 Top-K: \" + str(resultsRouge[i][0][\"rouge1\"][2]) + \"\\nRouge1 Top-P: \" + str(resultsRouge[i][0][\"rouge1\"][3]), \"Greedy Preplexity: \" + str(resultsPerplexity[i][0][\"perplexities\"][0]) + \"\\nBeam Preplexity: \" + str(resultsPerplexity[i][0][\"perplexities\"][1]) + \"\\nTop-K Preplexity: \" + str(resultsPerplexity[i][0][\"perplexities\"][2]) + \"\\nTop-P Preplexity: \" + str(resultsPerplexity[i][0][\"perplexities\"][3]), \"Ground Truth Perplexity: \" + str(groundPerplexity[i][0][\"perplexities\"][0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(resultsMeteor[i])\n",
    "    print(resultsBert[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
