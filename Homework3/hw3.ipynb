{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    LogitsProcessorList,\n",
    "    MinLengthLogitsProcessor,\n",
    "    StoppingCriteriaList,\n",
    "    MaxLengthCriteria,\n",
    "    AutoModelForSeq2SeqLM,\n",
    ")\n",
    "import numpy as np\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "# prompt = \"Today I believe we can finally\"\n",
    "prompt = \"It might be possible to\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/docs/transformers/main/en/main_classes/text_generation#transformers.GenerationMixin.greedy_search\n",
    "The above link is used in Task 1 to find & print the logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zebzi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\generation\\utils.py:1186: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"It might be possible to get a better understanding of the nature of the problem, but it's not clear how to do that.\\n\\nThe problem\"]\n",
      "|   651 |  get     | -3.341 | 3.54%\n",
      "|   257 |  a       | -1.936 | 14.43%\n",
      "|  1365 |  better  | -3.269 | 3.80%\n",
      "|  4547 |  understanding | -1.486 | 22.63%\n",
      "|   286 |  of      | -0.151 | 86.03%\n",
      "|   262 |  the     | -1.247 | 28.73%\n",
      "|  3450 |  nature  | -4.258 | 1.41%\n",
      "|   286 |  of      | -0.083 | 92.02%\n",
      "|   262 |  the     | -1.289 | 27.57%\n",
      "|  1917 |  problem | -4.050 | 1.74%\n",
      "|    11 | ,        | -1.583 | 20.54%\n",
      "|   475 |  but     | -0.740 | 47.72%\n",
      "|   340 |  it      | -1.707 | 18.14%\n",
      "|   338 | 's       | -1.193 | 30.33%\n",
      "|   407 |  not     | -1.242 | 28.88%\n",
      "|  1598 |  clear   | -2.580 | 7.58%\n",
      "|   703 |  how     | -1.511 | 22.07%\n",
      "|   284 |  to      | -1.713 | 18.03%\n",
      "|   466 |  do      | -1.116 | 32.77%\n",
      "|   326 |  that    | -1.147 | 31.77%\n",
      "|    13 | .        | -0.663 | 51.54%\n",
      "|   198 | \n",
      "        | -1.139 | 32.01%\n",
      "|   198 | \n",
      "        | -0.002 | 99.84%\n",
      "|   464 | The      | -2.294 | 10.09%\n",
      "|  1917 |  problem | -2.906 | 5.47%\n",
      "Length of the output: 25\n",
      "Perplexity: 5.505372557671196\n",
      "likelihood: 0.7923689491236807\n"
     ]
    }
   ],
   "source": [
    "#Greedy Search TASK 1\n",
    "\n",
    "outputs = model.generate(input_ids, num_beams=1, do_sample=False, max_length=30,return_dict_in_generate=True, output_scores=True)\n",
    "transition_scores = model.compute_transition_scores(outputs.sequences, outputs.scores, normalize_logits=True)\n",
    "print(tokenizer.batch_decode(outputs[0], skip_special_tokens=True))\n",
    "\n",
    "input_length = 1 if model.config.is_encoder_decoder else input_ids.shape[1]\n",
    "generated_tokens = outputs.sequences[:, input_length:]\n",
    "\n",
    "perplexity = 0\n",
    "likelihood = 0\n",
    "\n",
    "for tok, score in zip(generated_tokens[0], transition_scores[0]):\n",
    "    # | token | token string | logits | probability\n",
    "    print(f\"| {tok:5d} | {tokenizer.decode(tok):8s} | {score.detach().numpy():.3f} | {np.exp(score.detach().numpy()):.2%}\")\n",
    "    likelihood += np.log(-1 * score.detach().numpy())\n",
    "    perplexity += np.log(np.exp(score.detach().numpy()))\n",
    "\n",
    "print(f\"Length of the output: {generated_tokens.shape[1]}\")\n",
    "print(f\"Perplexity: {np.exp((-1/ generated_tokens.shape[1]) * perplexity)}\")\n",
    "print(f\"likelihood: {likelihood}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"It might be possible to get a better understanding of how the system works, but it's not going to be easy.\\n\\nIn the meantime,\"]\n",
      "|   651 |  get     | -3.341 | 3.54%\n",
      "|   257 |  a       | -1.936 | 14.43%\n",
      "|  1365 |  better  | -3.269 | 3.80%\n",
      "|  4547 |  understanding | -1.486 | 22.63%\n",
      "|   286 |  of      | -0.151 | 86.02%\n",
      "|   703 |  how     | -1.795 | 16.61%\n",
      "|   262 |  the     | -1.791 | 16.67%\n",
      "|  1080 |  system  | -4.110 | 1.64%\n",
      "|  2499 |  works   | -0.338 | 71.32%\n",
      "|    11 | ,        | -1.423 | 24.09%\n",
      "|   475 |  but     | -0.623 | 53.65%\n",
      "|   340 |  it      | -1.656 | 19.08%\n",
      "|   338 | 's       | -0.925 | 39.67%\n",
      "|   407 |  not     | -1.277 | 27.90%\n",
      "|  1016 |  going   | -2.657 | 7.02%\n",
      "|   284 |  to      | -0.006 | 99.38%\n",
      "|   307 |  be      | -1.001 | 36.75%\n",
      "|  2562 |  easy    | -1.028 | 35.76%\n",
      "|    13 | .        | -0.485 | 61.58%\n",
      "|   198 | \n",
      "        | -1.203 | 30.02%\n",
      "|   198 | \n",
      "        | -0.001 | 99.91%\n",
      "|   818 | In       | -3.445 | 3.19%\n",
      "|   262 |  the     | -1.601 | 20.17%\n",
      "| 14324 |  meantime | -1.308 | 27.05%\n",
      "|    11 | ,        | -0.036 | 96.45%\n",
      "Length of the output: 25\n",
      "Perplexity: 4.37375969279929\n",
      "likelihood: -9.344831745940242\n"
     ]
    }
   ],
   "source": [
    "#Beam Search TASK 1\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    num_beams=3,\n",
    "    early_stopping=True,\n",
    "    max_length=30,\n",
    "    return_dict_in_generate=True,\n",
    "    output_scores=True,\n",
    ")\n",
    "transition_scores = model.compute_transition_scores(\n",
    "    outputs.sequences, outputs.scores, outputs.beam_indices, normalize_logits=False\n",
    ")\n",
    "# If you sum the generated tokens' scores and apply the length penalty, you'll get the sequence scores.\n",
    "# Tip: recomputing the scores is only guaranteed to match with `normalize_logits=False`. Depending on the\n",
    "# use case, you might want to recompute it with `normalize_logits=True`.\n",
    "output_length = input_length + np.sum(transition_scores.numpy() < 0, axis=1)\n",
    "length_penalty = model.generation_config.length_penalty\n",
    "reconstructed_scores = transition_scores.sum(axis=1) / (output_length**length_penalty)\n",
    "\n",
    "print(tokenizer.batch_decode(outputs[0], skip_special_tokens=True))\n",
    "\n",
    "# input_length = 1 if model.config.is_encoder_decoder else input_ids.shape[1]\n",
    "generated_tokens = outputs.sequences[:, input_length:]\n",
    "\n",
    "perplexity = 0\n",
    "likelihood = 0\n",
    "\n",
    "for tok, score in zip(generated_tokens[0], transition_scores[0]):\n",
    "    # | token | token string | logits | probability\n",
    "    print(f\"| {tok:5d} | {tokenizer.decode(tok):8s} | {score.detach().numpy():.3f} | {np.exp(score.detach().numpy()):.2%}\")\n",
    "    likelihood += np.log(-1 * score.detach().numpy())\n",
    "    perplexity += np.log(np.exp(score.detach().numpy()))\n",
    "\n",
    "print(f\"Length of the output: {generated_tokens.shape[1]}\")\n",
    "print(f\"Perplexity: {np.exp((-1/ generated_tokens.shape[1]) * perplexity)}\")\n",
    "print(f\"likelihood: {likelihood}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It might be possible to say that the reason why the Church has more women priests today than ever before is because there are no more women priests, and']\n",
      "|   910 |  say     | -4.189 | 1.52%\n",
      "|   326 |  that    | -0.703 | 49.53%\n",
      "|   262 |  the     | -1.293 | 27.43%\n",
      "|  1738 |  reason  | -4.248 | 1.43%\n",
      "|  1521 |  why     | -1.969 | 13.96%\n",
      "|   262 |  the     | -1.353 | 25.86%\n",
      "|  4564 |  Church  | -4.351 | 1.29%\n",
      "|   468 |  has     | -1.547 | 21.28%\n",
      "|   517 |  more    | -5.477 | 0.42%\n",
      "|  1466 |  women   | -3.038 | 4.79%\n",
      "| 17345 |  priests | -1.697 | 18.33%\n",
      "|  1909 |  today   | -4.329 | 1.32%\n",
      "|   621 |  than    | -0.861 | 42.26%\n",
      "|  1683 |  ever    | -1.569 | 20.83%\n",
      "|   878 |  before  | -0.272 | 76.18%\n",
      "|   318 |  is      | -0.318 | 72.77%\n",
      "|   780 |  because | -0.418 | 65.82%\n",
      "|   612 |  there   | -3.033 | 4.82%\n",
      "|   389 |  are     | -0.820 | 44.05%\n",
      "|   645 |  no      | -4.169 | 1.55%\n",
      "|   517 |  more    | -0.673 | 51.02%\n",
      "|  1466 |  women   | -0.526 | 59.10%\n",
      "| 17345 |  priests | -0.292 | 74.64%\n",
      "|    11 | ,        | -2.690 | 6.79%\n",
      "|   290 |  and     | -1.248 | 28.69%\n",
      "Length of the output: 25\n",
      "Perplexity: 7.71630208432037\n",
      "likelihood: 8.739395656243618\n"
     ]
    }
   ],
   "source": [
    "#Top-K Sampling TASK 1\n",
    "\n",
    "outputs = model.generate(input_ids, do_sample=True, max_length=30, return_dict_in_generate=True, output_scores=True, top_k=50)\n",
    "\n",
    "print(tokenizer.batch_decode(outputs[0], skip_special_tokens=True))\n",
    "\n",
    "input_length = 1 if model.config.is_encoder_decoder else input_ids.shape[1]\n",
    "generated_tokens = outputs.sequences[:, input_length:]\n",
    "\n",
    "transition_scores = model.compute_transition_scores(outputs.sequences, outputs.scores, normalize_logits=True)\n",
    "\n",
    "perplexity = 0\n",
    "likelihood = 0\n",
    "\n",
    "for tok, score in zip(generated_tokens[0], transition_scores[0]):\n",
    "    # | token | token string | logits | probability\n",
    "    print(f\"| {tok:5d} | {tokenizer.decode(tok):8s} | {score.detach().numpy():.3f} | {np.exp(score.detach().numpy()):.2%}\")\n",
    "    likelihood += np.log(-1 * score.detach().numpy())\n",
    "    perplexity += np.log(np.exp(score.detach().numpy()))\n",
    "\n",
    "print(f\"Length of the output: {generated_tokens.shape[1]}\")\n",
    "print(f\"Perplexity: {np.exp((-1/ generated_tokens.shape[1]) * perplexity)}\")\n",
    "print(f\"likelihood: {likelihood}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It might be possible to block the impact of player exclusivity on the user experience by ensuring that the game does not violate the PC platform. This includes']\n",
      "|  2512 |  block   | -6.878 | 0.10%\n",
      "|   262 |  the     | -1.543 | 21.38%\n",
      "|  2928 |  impact  | -6.667 | 0.13%\n",
      "|   286 |  of      | -0.221 | 80.18%\n",
      "|  2137 |  player  | -8.546 | 0.02%\n",
      "| 10293 |  exclus  | -8.081 | 0.03%\n",
      "|  3458 | ivity    | 0.000 | 100.00%\n",
      "|   319 |  on      | -1.772 | 16.99%\n",
      "|   262 |  the     | -1.429 | 23.94%\n",
      "|  2836 |  user    | -6.444 | 0.16%\n",
      "|  1998 |  experience | -0.309 | 73.42%\n",
      "|   416 |  by      | -2.393 | 9.14%\n",
      "| 13359 |  ensuring | -4.696 | 0.91%\n",
      "|   326 |  that    | -0.287 | 75.06%\n",
      "|   262 |  the     | -1.590 | 20.40%\n",
      "|   983 |  game    | -1.451 | 23.44%\n",
      "|   857 |  does    | -2.444 | 8.68%\n",
      "|   407 |  not     | 0.000 | 100.00%\n",
      "| 16967 |  violate | -5.098 | 0.61%\n",
      "|   262 |  the     | -1.334 | 26.35%\n",
      "|  4217 |  PC      | -4.955 | 0.70%\n",
      "|  3859 |  platform | -2.428 | 8.82%\n",
      "|    13 | .        | -1.708 | 18.12%\n",
      "|   770 |  This    | -2.658 | 7.01%\n",
      "|  3407 |  includes | -3.546 | 2.88%\n",
      "Length of the output: 25\n",
      "Perplexity: 21.308511891444834\n",
      "likelihood: -4.340644665295816\n"
     ]
    }
   ],
   "source": [
    "#Top-P Sampling TASK 1 \n",
    "#Need to figure out a good value for top_p\n",
    "#top_p = 4 gave good values but it's supposed to be bounded (0,1)\n",
    "\n",
    "outputs = model.generate(input_ids, top_p = 0.8, top_k=0, do_sample=True, max_length=30, return_dict_in_generate=True, output_scores=True)\n",
    "print(tokenizer.batch_decode(outputs[0], skip_special_tokens=True))\n",
    "\n",
    "input_length = 1 if model.config.is_encoder_decoder else input_ids.shape[1]\n",
    "generated_tokens = outputs.sequences[:, input_length:]\n",
    "\n",
    "transition_scores = model.compute_transition_scores(outputs.sequences, outputs.scores, normalize_logits=True)\n",
    "\n",
    "perplexity = 0\n",
    "likelihood = 0\n",
    "\n",
    "for tok, score in zip(generated_tokens[0], transition_scores[0]):\n",
    "    # | token | token string | logits | probability\n",
    "    print(f\"| {tok:5d} | {tokenizer.decode(tok):8s} | {score.detach().numpy():.3f} | {np.exp(score.detach().numpy()):.2%}\")\n",
    "    likelihood += np.log(-1 * score.detach().numpy() + 0.00001)\n",
    "    perplexity += np.log(np.exp(score.detach().numpy()))\n",
    "\n",
    "print(f\"Length of the output: {generated_tokens.shape[1]}\")\n",
    "print(f\"Perplexity: {np.exp((-1/ generated_tokens.shape[1]) * perplexity)}\")\n",
    "print(f\"likelihood: {likelihood}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**************************************************\n",
    "Task 2 starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset cnn_dailymail (C:/Users/zebzi/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de)\n",
      "100%|██████████| 3/3 [00:00<00:00,  5.03it/s]\n"
     ]
    }
   ],
   "source": [
    "#TASK 2\n",
    "#Load the dataset\n",
    "\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
    "\n",
    "max_length = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TASK 2 (switched from a downstream BERT because things were failing)\n",
    "tokenizer2 = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "model2 = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-large-cnn\")#, pad_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(CNN)The Palestinian Authority officially became the 123rd member of the International Criminal Court on Wednesday, a step that gives the court jurisdiction over alleged crimes in Palestinian territories. The formal accession was marked with a ceremony at The Hague, in the Netherlands, where the court is based. The Palestinians signed the ICC's founding Rome Statute in January, when they also accepted its jurisdiction over alleged crimes committed \"in the occupied Palestinian territory, including East Jerusalem, since June 13, 2014.\" Later that month, the ICC opened a preliminary examination into the situation in Palestinian territories, paving the way for possible war crimes investigations against Israelis. As members of the court, Palestinians may be subject to counter-charges as well. Israel and the United States, neither of which is an ICC member, opposed the Palestinians' efforts to join the body. But Palestinian Foreign Minister Riad al-Malki, speaking at Wednesday's ceremony, said it was a move toward greater justice. \"As Palestine formally becomes a State Party to the Rome Statute today, the world is also a step closer to ending a long era of impunity and injustice,\" he said, according to an ICC news release. \"Indeed, today brings us closer to our shared goals of justice and peace.\" Judge Kuniko Ozaki, a vice president of the ICC, said acceding to the treaty was just the first step for the Palestinians. \"As the Rome Statute today enters into force for the State of Palestine, Palestine acquires all the rights as well as responsibilities that come with being a State Party to the Statute. These are substantive commitments, which cannot be taken lightly,\" she said. Rights group Human Rights Watch welcomed the development. \"Governments seeking to penalize Palestine for joining the ICC should immediately end their pressure, and countries that support universal acceptance of the court's treaty should speak out to welcome its membership,\" said Balkees Jarrah, international justice counsel for the group. \"What's objectionable is the attempts to undermine international justice, not Palestine's decision to join a treaty to which over 100 countries around the world are members.\" In January, when the preliminary ICC examination was opened, Israeli Prime Minister Benjamin Netanyahu described it as an outrage, saying the court was overstepping its boundaries. The United States also said it \"strongly\" disagreed with the court's decision. \"As we have said repeatedly, we do not believe that Palestine is a state and therefore we do not believe that it is eligible to join the ICC,\" the State Department said in a statement. It urged the warring sides to resolve their differences through direct negotiations. \"We will continue to oppose actions against Israel at the ICC as counterproductive to the cause of peace,\" it said. But the ICC begs to differ with the definition of a state for its purposes and refers to the territories as \"Palestine.\" While a preliminary examination is not a formal investigation, it allows the court to review evidence and determine whether to investigate suspects on both sides. Prosecutor Fatou Bensouda said her office would \"conduct its analysis in full independence and impartiality.\" The war between Israel and Hamas militants in Gaza last summer left more than 2,000 people dead. The inquiry will include alleged war crimes committed since June. The International Criminal Court was set up in 2002 to prosecute genocide, crimes against humanity and war crimes. CNN's Vasco Cotovio, Kareem Khadder and Faith Karimi contributed to this report.\n",
      "<s>(CNN)The Palestinian Authority officially became the 123rd member of the International Criminal Court on Wednesday, a step that gives the court jurisdiction over alleged crimes in Palestinian territories. The formal accession was marked with a ceremony at The Hague, in the Netherlands, where the court is based. The Palestinians signed the ICC's founding Rome Statute in January, when they also accepted its jurisdiction over alleged crimes committed \"in the occupied Palestinian territory, including East Jerusalem, since June 13, 2014.\" Later that month, the ICC opened a preliminary examination into the situation in Palestinian territories, paving the way for possible war crimes investigations against Israelis. As members of the court, Palestinians may be subject to counter-charges as well. Israel and the United States, neither of which is an ICC member, opposed the Palestinians' efforts to join the body. But Palestinian Foreign Minister Riad al-Malki, speaking at Wednesday's ceremony, said it was a move toward greater justice. \"As Palestine formally becomes a State Party to the Rome Statute today, the world is also a step closer to ending a long era of impunity and injustice,\" he said, according to an ICC news release. \"Indeed, today brings us closer to our shared goals of justice and peace.\" Judge Kuniko Ozaki, a vice president of the ICC, said acceding to the treaty was just the first step for the Palestinians. \"As the Rome Statute today enters into force for the State of Palestine, Palestine acquires all the rights as well as responsibilities that come with being a State Party to the Statute. These are substantive commitments, which cannot be taken lightly,\" she said. Rights group Human Rights Watch welcomed the development. \"Governments seeking to penalize Palestine for joining the ICC should immediately end their pressure, and countries that support universal acceptance of the court's treaty should speak out to welcome its membership,\" said Balkees Jarrah, international justice counsel for the group. \"What's objectionable is the attempts to undermine international justice, not Palestine's decision to join a treaty to which over 100 countries around the world are members.\" In January, when the preliminary ICC examination was opened, Israeli Prime Minister Benjamin Netanyahu described it as an outrage, saying the court was overstepping its boundaries. The United States also said it \"strongly\" disagreed with the court's decision. \"As we have said repeatedly, we do not believe that Palestine is a state and therefore we do not believe that it is eligible to join the ICC,\" the State Department said in a statement. It urged the warring sides to resolve their differences through direct negotiations. \"We will continue to oppose actions against Israel at the ICC as counterproductive to the cause of peace,\" it said. But the ICC begs to differ with the definition of a state for its purposes and refers to the territories as \"Palestine.\" While a preliminary examination is not a formal investigation, it allows the court to review evidence and determine whether to investigate suspects on both sides. Prosecutor Fatou Bensouda said her office would \"conduct its analysis in full independence and impartiality.\" The war between Israel and Hamas militants in Gaza last summer left more than 2,000 people dead. The inquiry will include alleged war crimes committed since June. The International Criminal Court was set up in 2002 to prosecute genocide, crimes against humanity and war crimes. CNN's Vasco Cotovio, Kareem Khadder and Faith Karimi contributed to this report.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    }
   ],
   "source": [
    "# input_ids = dataset[\"test\"][:50]\n",
    "tokenizerInputs =  []\n",
    "inputGroundTruths = []\n",
    "for i in range(50):\n",
    "    tokenizerInputs.append(dataset[\"test\"][i][\"article\"])\n",
    "    inputGroundTruths.append(dataset[\"test\"][i][\"highlights\"])\n",
    "print(tokenizerInputs[0])\n",
    "\n",
    "encoder_input_ids = torch.LongTensor()\n",
    "\n",
    "for i in range(50):\n",
    "    encoder_input_ids = torch.cat((encoder_input_ids, tokenizer2(tokenizerInputs[i], return_tensors=\"pt\", padding='max_length', truncation=True).input_ids))\n",
    "print(tokenizer2.decode(encoder_input_ids[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task 2 \n",
    "outputs2 = []\n",
    "#Greedy Search\n",
    "outputs2.append(model2.generate(encoder_input_ids, num_beams=1, do_sample=False, max_length=max_length, no_repeat_ngram_size=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Beam Search\n",
    "outputs2.append(model2.generate(encoder_input_ids, num_beams=3, early_stopping=True, max_length=max_length, no_repeat_ngram_size=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Top-K Sampling\n",
    "outputs2.append(model2.generate(encoder_input_ids, do_sample=True, top_k=30, max_length=max_length, no_repeat_ngram_size=2))\n",
    "\n",
    "#max_length = 50 with top_k>=40 gave the \"index out of range in self\" error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Top-P Sampling\n",
    "outputs2.append(model2.generate(encoder_input_ids, do_sample=True, top_p=0.8, top_k=0, max_length=max_length, no_repeat_ngram_size=2)) \n",
    "\n",
    "#max_length = 30 with top_p>=0.8 gave the \"index out of range in self\" error\n",
    "#max_length = 50 with top_p>=0.4 gave the \"index out of range in self\" error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(outputs[0][0])\n",
    "print(tokenizer2.decode(outputs2[0][0], skip_special_tokens=True))\n",
    "print(tokenizer2.decode(outputs2[1][0], skip_special_tokens=True))\n",
    "print(tokenizer2.decode(outputs2[2][0], skip_special_tokens=True))\n",
    "print(tokenizer2.decode(outputs2[3][0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\zebzi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\zebzi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\zebzi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from evaluate import load\n",
    "meteorScore = load(\"meteor\")\n",
    "bertScore = load(\"bertscore\")\n",
    "rougeScore = load(\"rouge\")\n",
    "perplexityScore = load(\"perplexity\", module_type=\"metric\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.shared.weight', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.4.fc1.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.8.fc1.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.4.self_attn.k_proj.weight']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:00<00:00, 62.53it/s]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.shared.weight', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.4.fc1.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.8.fc1.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.4.self_attn.k_proj.weight']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:00<00:00, 16.42it/s]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.shared.weight', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.4.fc1.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.8.fc1.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.4.self_attn.k_proj.weight']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:00<00:00, 63.17it/s]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.shared.weight', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.4.fc1.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.8.fc1.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.4.self_attn.k_proj.weight']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:00<00:00, 58.47it/s]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.shared.weight', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.4.fc1.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.8.fc1.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.4.self_attn.k_proj.weight']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:00<00:00, 61.35it/s]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.shared.weight', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.4.fc1.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.8.fc1.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.4.self_attn.k_proj.weight']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:00<00:00, 59.89it/s]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.shared.weight', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.4.fc1.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.8.fc1.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.4.self_attn.k_proj.weight']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:00<00:00, 14.02it/s]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.shared.weight', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.4.fc1.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.8.fc1.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.4.self_attn.k_proj.weight']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:00<00:00, 57.93it/s]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.shared.weight', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.4.fc1.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.8.fc1.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.4.self_attn.k_proj.weight']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:00<00:00, 59.87it/s]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.shared.weight', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.4.fc1.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.8.fc1.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.4.self_attn.k_proj.weight']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:00<00:00, 70.54it/s]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.shared.weight', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.4.fc1.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.8.fc1.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.4.self_attn.k_proj.weight']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:00<00:00, 61.58it/s]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.shared.weight', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.4.fc1.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.8.fc1.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.4.self_attn.k_proj.weight']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:00<00:00, 62.18it/s]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.shared.weight', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.4.fc1.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.8.fc1.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.4.self_attn.k_proj.weight']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:00<00:00, 17.91it/s]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.shared.weight', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.4.fc1.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.8.fc1.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.4.self_attn.k_proj.weight']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:00<00:00, 45.48it/s]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.shared.weight', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.4.fc1.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.8.fc1.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.4.self_attn.k_proj.weight']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:00<00:00, 32.05it/s]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.shared.weight', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.4.fc1.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.8.fc1.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.4.self_attn.k_proj.weight']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:00<00:00, 40.05it/s]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.shared.weight', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.4.fc1.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.8.fc1.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.4.self_attn.k_proj.weight']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:00<00:00, 37.10it/s]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.shared.weight', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.4.fc1.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.8.fc1.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.4.self_attn.k_proj.weight']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.99it/s]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.shared.weight', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.4.fc1.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.8.fc1.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.4.self_attn.k_proj.weight']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:00<00:00, 62.60it/s]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.shared.weight', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.4.fc1.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.8.fc1.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.4.self_attn.k_proj.weight']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:00<00:00, 63.15it/s]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.shared.weight', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.4.fc1.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.8.fc1.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.4.self_attn.k_proj.weight']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:00<00:00, 59.33it/s]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.shared.weight', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.4.fc1.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.8.fc1.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.4.self_attn.k_proj.weight']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:00<00:00, 59.14it/s]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.shared.weight', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.4.fc1.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.8.fc1.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.4.self_attn.k_proj.weight']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:00<00:00, 58.16it/s]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.shared.weight', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.4.fc1.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.8.fc1.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.4.self_attn.k_proj.weight']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:00<00:00, 60.37it/s]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.shared.weight', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.4.fc1.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.8.fc1.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.4.self_attn.k_proj.weight']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:00<00:00, 63.88it/s]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.shared.weight', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.4.fc1.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.8.fc1.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.4.self_attn.k_proj.weight']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:00<00:00, 57.30it/s]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.shared.weight', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.4.fc1.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.8.fc1.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.4.self_attn.k_proj.weight']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:00<00:00, 59.91it/s]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.shared.weight', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.4.fc1.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.8.fc1.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.4.self_attn.k_proj.weight']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:00<00:00, 66.95it/s]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.shared.weight', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.4.fc1.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.8.fc1.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.4.self_attn.k_proj.weight']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:00<00:00, 56.70it/s]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.shared.weight', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.4.fc1.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.8.fc1.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.4.self_attn.k_proj.weight']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:00<00:00, 63.73it/s]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.shared.weight', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.4.fc1.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.8.fc1.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.4.self_attn.k_proj.weight']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:00<00:00, 14.79it/s]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.shared.weight', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.4.fc1.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.8.fc1.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.4.self_attn.k_proj.weight']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:00<00:00, 63.80it/s]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.shared.weight', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.4.fc1.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.8.fc1.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.4.self_attn.k_proj.weight']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:00<00:00, 65.46it/s]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.shared.weight', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.4.fc1.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.8.fc1.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.4.self_attn.k_proj.weight']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:00<00:00, 64.46it/s]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.shared.weight', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.4.fc1.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.8.fc1.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.4.self_attn.k_proj.weight']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:00<00:00, 59.65it/s]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.shared.weight', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.4.fc1.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.8.fc1.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.4.self_attn.k_proj.weight']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:00<00:00, 58.39it/s]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.shared.weight', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.4.fc1.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.8.fc1.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.4.self_attn.k_proj.weight']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:00<00:00, 65.45it/s]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.shared.weight', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.4.fc1.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.8.fc1.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.4.self_attn.k_proj.weight']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:00<00:00, 60.46it/s]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.shared.weight', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.4.fc1.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.8.fc1.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.4.self_attn.k_proj.weight']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:00<00:00, 58.55it/s]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.shared.weight', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.4.fc1.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.8.fc1.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.4.self_attn.k_proj.weight']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:00<00:00, 59.66it/s]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.shared.weight', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.4.fc1.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.8.fc1.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.4.self_attn.k_proj.weight']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:00<00:00, 68.64it/s]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.shared.weight', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.4.fc1.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.8.fc1.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.4.self_attn.k_proj.weight']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:00<00:00, 61.37it/s]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.shared.weight', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.4.fc1.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.8.fc1.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.4.self_attn.k_proj.weight']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:00<00:00, 46.66it/s]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.shared.weight', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.4.fc1.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.8.fc1.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.4.self_attn.k_proj.weight']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:00<00:00, 61.78it/s]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.shared.weight', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.4.fc1.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.8.fc1.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.4.self_attn.k_proj.weight']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:00<00:00, 44.80it/s]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.shared.weight', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.4.fc1.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.8.fc1.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.4.self_attn.k_proj.weight']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:00<00:00, 50.36it/s]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.shared.weight', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.4.fc1.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.8.fc1.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.4.self_attn.k_proj.weight']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:00<00:00, 53.42it/s]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.shared.weight', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.4.fc1.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.8.fc1.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.4.self_attn.k_proj.weight']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:00<00:00, 28.23it/s]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.shared.weight', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.4.fc1.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.8.fc1.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.4.self_attn.k_proj.weight']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:00<00:00, 53.27it/s]\n",
      "Some weights of the model checkpoint at facebook/bart-large-cnn were not used when initializing BartForCausalLM: ['model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.shared.weight', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.4.fc1.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.8.fc1.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.4.self_attn.k_proj.weight']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:00<00:00, 40.96it/s]\n"
     ]
    }
   ],
   "source": [
    "references = []\n",
    "predictions = []\n",
    "resultsMeteor = []\n",
    "resultsBert = []\n",
    "resultsRouge = []\n",
    "resultsPerplexity = []\n",
    "groundPerplexity = []\n",
    "\n",
    "# references.append([inputGroundTruths[0], inputGroundTruths[0], inputGroundTruths[0], inputGroundTruths[0]])\n",
    "# predictions.append([tokenizer2.decode(outputs2[0][0], skip_special_tokens=True), tokenizer2.decode(outputs2[1][0], skip_special_tokens=True), tokenizer2.decode(outputs2[2][0], skip_special_tokens=True), tokenizer2.decode(outputs2[3][0], skip_special_tokens=True)])\n",
    "# results.append([roguescore.compute(predictions=predictions[0], references=references[0])])\n",
    "# results.append([roguescore.compute(predictions=[tokenizer2.decode(outputs2[0][0], skip_special_tokens=True)], references=[inputGroundTruths[0]])])\n",
    "# results.append([roguescore.compute(predictions=[\"hey\"], references=[[\"hey\"]])])\n",
    "\n",
    "# print(predictions[0])\n",
    "# print(references[0])\n",
    "# print(results)\n",
    "# print(outputs2)\n",
    "with open(\"generatedText12.txt\", \"w\") as f:\n",
    "    for i in range(50):\n",
    "        references.append([inputGroundTruths[i], inputGroundTruths[i], inputGroundTruths[i], inputGroundTruths[i]])\n",
    "        # predictions.append([tokenizer2.decode(outputs2[0][i], skip_special_tokens=True), tokenizer2.decode(outputs2[1][i], skip_special_tokens=True), tokenizer2.decode(outputs2[2][i], skip_special_tokens=True), tokenizer2.decode(outputs2[3][i], skip_special_tokens=True)])\n",
    "        # resultsBert.append([bertScore.compute(predictions=predictions[i], references=references[i], model_type=\"t5-base\")])\n",
    "        # resultsRouge.append([rougeScore.compute(predictions=predictions[i], references=references[i], use_aggregator=False)])\n",
    "        # resultsPerplexity.append([perplexityScore.compute(predictions=predictions[i], model_id=\"facebook/bart-large-cnn\")])\n",
    "        # groundPerplexity.append([perplexityScore.compute(predictions=[references[i][0]], model_id=\"facebook/bart-large-cnn\")])\n",
    "\n",
    "        # resultsBert.append([bertScore.compute(predictions=predictions[i], references=references[i], model_type=\"t5-base\")])\n",
    "        # resultsRouge.append([rougeScore.compute(predictions=predictions[i], references=references[i], use_aggregator=False)])\n",
    "        # resultsPerplexity.append([perplexityScore.compute(predictions=predictions[i], model_id=\"facebook/bart-large-cnn\")])\n",
    "        groundPerplexity.append([perplexityScore.compute(predictions=[references[i][0]], model_id=\"facebook/bart-large-cnn\")])\n",
    "        # print(predictions[i][0])\n",
    "        # print(references[i][0])\n",
    "        # temp = []\n",
    "        # for j in range(4):\n",
    "        #     temp.append([meteorScore.compute(predictions=[predictions[i][j]], references=[references[i][j]])])\n",
    "        # resultsMeteor.append(temp)\n",
    "\n",
    "        # f.write(f\"Ground Truth: {inputGroundTruths[i]} \\n\")\n",
    "        # f.write(f\"Greedy Search: {predictions[i][0]} \\n\")\n",
    "        # f.write(f\"Beam Search: {predictions[i][1]} \\n\")\n",
    "        # f.write(f\"Top-K Sampling: {predictions[i][2]} \\n\")\n",
    "        # f.write(f\"Top-P Sampling: {predictions[i][3]} \\n\")\n",
    "        # f.write(f\"Meteor Score: {resultsMeteor[i]} \\n\")\n",
    "        # f.write(f\"Bert Score: {resultsBert[i]} \\n\")\n",
    "        # f.write(f\"Rouge Score: {resultsRouge[i]} \\n\")\n",
    "        # f.write(f\"Perplexty: {resultsPerplexity[i]} \\n\")\n",
    "        f.write(f\"Ground Perplexity {groundPerplexity[i]} \\n\")\n",
    "        f.write(\"\\n \\n\")\n",
    "\n",
    "#generatedText is with no capped max_length so it defaulted to 20 I think top_p=0.92 and top_k=50\n",
    "#generatedText2 is with max_length = 30\n",
    "#generatedText3 is also max_length = 30 but top_p=0.7 ~ 6mins of runtime\n",
    "#generatedText4 is at max_length = 40 with top_p=0.7 ~5mins of runtime thanks to CPU acceleration\n",
    "#generatedText5 is at max_length = 50 with top_p=0.3 and top_k=25 ~Xmins of runtime but model_max_length = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = 0\n",
    "for i in range(50):\n",
    "    temp += groundPerplexity[i]\n",
    "print(temp / 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open(\"outputs4.csv\", 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Ground Truth\", \"Greedy Search\", \"Beam Search\", \"Top-K Sampling\", \"Top-P Sampling\", \" \", \" \", \"Meteor Score\", \"BERT Precision Score\", \"Rouge1 Score\", \"Perplexity of Predicitons\", \"Perplexity of Ground Truth\"])\n",
    "\n",
    "    for i in range(len(predictions)):\n",
    "        writer.writerow([inputGroundTruths[i], predictions[i][0], predictions[i][1], predictions[i][2], predictions[i][3], \" \", \" \", \"Meteor Greedy: \" + str(resultsMeteor[i][0][0][\"meteor\"]) + \"\\n Meteor Beam: \" + str(resultsMeteor[i][1][0][\"meteor\"]) + \"\\n Meteor Top-K: \" + str(resultsMeteor[i][2][0][\"meteor\"]) + \"\\n Meteor Top-P: \" + str(resultsMeteor[i][3][0][\"meteor\"]), \"BERT Greedy Precision \" + str(resultsBert[i][0][\"precision\"][0]) + \"\\nBERT Beam Precision \" + str(resultsBert[i][0][\"precision\"][1]) + \"\\nBERT Top-K Precision \" + str(resultsBert[i][0][\"precision\"][2]) + \"\\nBERT Top-P Precision \" + str(resultsBert[i][0][\"precision\"][3]),\"Rouge1 Greedy: \" + str(resultsRouge[i][0][\"rouge1\"][0]) + \"\\nRouge1 Beam: \" + str(resultsRouge[i][0][\"rouge1\"][1]) + \"\\nRouge1 Top-K: \" + str(resultsRouge[i][0][\"rouge1\"][2]) + \"\\nRouge1 Top-P: \" + str(resultsRouge[i][0][\"rouge1\"][3]), \"Greedy Preplexity: \" + str(resultsPerplexity[i][0][\"perplexities\"][0]) + \"\\nBeam Preplexity: \" + str(resultsPerplexity[i][0][\"perplexities\"][1]) + \"\\nTop-K Preplexity: \" + str(resultsPerplexity[i][0][\"perplexities\"][2]) + \"\\nTop-P Preplexity: \" + str(resultsPerplexity[i][0][\"perplexities\"][3]), \"Ground Truth Perplexity: \" + str(groundPerplexity[i][0][\"perplexities\"][0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'resultsMeteor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\zebzi\\Documents\\School\\Senior_Year\\CSCI_5541\\Homework3\\hw3.ipynb Cell 18\u001b[0m in \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/zebzi/Documents/School/Senior_Year/CSCI_5541/Homework3/hw3.ipynb#X23sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m top_PMeteor, top_PBert, top_PRouge1, top_PPerplexity \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/zebzi/Documents/School/Senior_Year/CSCI_5541/Homework3/hw3.ipynb#X23sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m50\u001b[39m):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/zebzi/Documents/School/Senior_Year/CSCI_5541/Homework3/hw3.ipynb#X23sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     greedyMeteor \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m resultsMeteor[i][\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mmeteor\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/zebzi/Documents/School/Senior_Year/CSCI_5541/Homework3/hw3.ipynb#X23sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     beamMeteor \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m resultsMeteor[i][\u001b[39m1\u001b[39m][\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mmeteor\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/zebzi/Documents/School/Senior_Year/CSCI_5541/Homework3/hw3.ipynb#X23sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     top_KMeteor \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m resultsMeteor[i][\u001b[39m2\u001b[39m][\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mmeteor\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'resultsMeteor' is not defined"
     ]
    }
   ],
   "source": [
    "#Average the 'automatic' scores by distinct categories\n",
    "\n",
    "greedyMeteor, greedyBert, greedyRouge1, greedyPerplexity = 0, 0, 0, 0\n",
    "beamMeteor, beamBert, beamRouge1, beamPerplexity = 0, 0, 0, 0\n",
    "top_KMeteor, top_KBert, top_KRouge1, top_KPerplexity = 0, 0, 0, 0\n",
    "top_PMeteor, top_PBert, top_PRouge1, top_PPerplexity = 0, 0, 0, 0\n",
    "\n",
    "\n",
    "for i in range(50):\n",
    "    greedyMeteor += resultsMeteor[i][0][0][\"meteor\"]\n",
    "    beamMeteor += resultsMeteor[i][1][0][\"meteor\"]\n",
    "    top_KMeteor += resultsMeteor[i][2][0][\"meteor\"]\n",
    "    top_PMeteor += resultsMeteor[i][3][0][\"meteor\"]\n",
    "\n",
    "    greedyBert += resultsBert[i][0][\"precision\"][0]\n",
    "    beamBert += resultsBert[i][0][\"precision\"][1]\n",
    "    top_KBert += resultsBert[i][0][\"precision\"][2]\n",
    "    top_PBert += resultsBert[i][0][\"precision\"][3]\n",
    "\n",
    "    greedyRouge1 += resultsRouge[i][0][\"rouge1\"][0]\n",
    "    beamRouge1 += resultsRouge[i][0][\"rouge1\"][1]\n",
    "    top_KRouge1 += resultsRouge[i][0][\"rouge1\"][2]\n",
    "    top_PRouge1 += resultsRouge[i][0][\"rouge1\"][3]\n",
    "\n",
    "    greedyPerplexity += resultsPerplexity[i][0][\"perplexities\"][0]\n",
    "    beamPerplexity += resultsPerplexity[i][0][\"perplexities\"][1]\n",
    "    top_KPerplexity += resultsPerplexity[i][0][\"perplexities\"][2]\n",
    "    top_PPerplexity += resultsPerplexity[i][0][\"perplexities\"][3]\n",
    "\n",
    "print(f\"Greedy Meteor Average: {greedyMeteor/50} Greedy Bert Average: {greedyBert/50} Greedy Rouge1 Average: {greedyRouge1/50} Greedy Perplexity Average: {greedyPerplexity/50} \\n\")\n",
    "print(f\"beam Meteor Average: {beamMeteor/50} beam Bert Average: {beamBert/50} beam Rouge1 Average: {beamRouge1/50} beam Perplexity Average: {beamPerplexity/50} \\n\")\n",
    "print(f\"top_K Meteor Average: {top_KMeteor/50} top_K Bert Average: {top_KBert/50} top_K Rouge1 Average: {top_KRouge1/50} top_K Perplexity Average: {top_KPerplexity/50} \\n\")\n",
    "print(f\"top_P Meteor Average: {top_PMeteor/50} top_P Bert Average: {top_PBert/50} top_P Rouge1 Average: {top_PRouge1/50} top_P Perplexity Average: {top_PPerplexity/50} \\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
