{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getDocument import *\n",
    "from PdfUtils import *\n",
    "import json\n",
    "import time\n",
    "from sentence_transformers import SentenceTransformer\n",
    "# import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.perf_counter() # To be used later\n",
    "\n",
    "defaultPaper = get_paper_and_references_embedding_and_titles_from_query(\"BERT Rediscovers the Classical NLP Pipeline\")\n",
    "\n",
    "mainPaperSections = get_text(get_paperId_from_query(defaultPaper[1][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tupleOfResults = get_paper_and_references_embedding_and_titles_from_query(\"BERT Rediscovers the Classical NLP Pipeline\")\n",
    "\n",
    "# embeddings = tupleOfResults[0]\n",
    "# titles = tupleOfResults[1]\n",
    "\n",
    "# print(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newCosineSimilarity = torch.nn.CosineSimilarity(dim=0)\n",
    "# mainPaper = torch.tensor(embeddings[0])\n",
    "# print(f\"Main paper title: {titles[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary for the purpose of not repreating titles\n",
    "title_embedding_dict = {}\n",
    "\n",
    "listOfSearchedPapers = [] # For speed purposes, took 4min 20sec before\n",
    "\n",
    "for embedding, title in zip(defaultPaper[0], defaultPaper[1]):\n",
    "    if title not in listOfSearchedPapers:\n",
    "        listOfSearchedPapers.append(title)\n",
    "        subPaper = get_paper_and_references_embedding_and_titles_from_query(title)\n",
    "        # First depth, all of the main paper's references\n",
    "        if title not in title_embedding_dict:\n",
    "            title_embedding_dict[title] = embedding\n",
    "\n",
    "        # Second depth, all of the referenced paper's references\n",
    "        for sub_embedding, sub_title in zip(subPaper[0], subPaper[1]):\n",
    "            if sub_title not in title_embedding_dict:\n",
    "                title_embedding_dict[sub_title] = sub_embedding\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to return just the first value of the list to be used in list.sort()\n",
    "def getKey(listOfCosAndTitle):\n",
    "    return listOfCosAndTitle[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newCosineSimilarity = torch.nn.CosineSimilarity(dim=0)\n",
    "mainPaperEmbedding = torch.tensor(defaultPaper[0][0])\n",
    "\n",
    "similarityScoresWithTitle = []\n",
    "# Get the cos similarity between the main paper and all of the subpapers\n",
    "for key in title_embedding_dict.keys():\n",
    "    compared_paper = torch.tensor(title_embedding_dict[key])\n",
    "    similarityScoresWithTitle.append([newCosineSimilarity(mainPaperEmbedding, compared_paper), key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GREEN = '\\033[32m'\n",
    "RED = '\\033[31m'\n",
    "RESET = '\\033[0m'\n",
    "\n",
    "similarityScoresWithTitle.sort(reverse=True, key=getKey)\n",
    "for i in similarityScoresWithTitle:\n",
    "    print(f\"Cos similarity score: {i[0]:.3f} and it's {RED + 'not cited in ' if i[1] not in defaultPaper[1] else GREEN + 'cited in     '}{RESET}the main paper with title: {i[1]:20s}\")\n",
    "print(similarityScoresWithTitle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "embeddings = {}\n",
    "# for i in listOfSections:\n",
    "#     embeddings.append(model.encode(i))\n",
    "# print(embeddings)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "embeddings is a dictionary where each key is a paper title, the value is a list of lists\n",
    "each sublist is -> Section Embedding, Index into the section, the section text, the title again(?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try parsing the top 10 things (that will include the main paper) and then see how long it takes to get and parse the pdfs\n",
    "for i in similarityScoresWithTitle[:10]:\n",
    "    listOfSections = get_text(get_paperId_from_query(i[1]))\n",
    "    print(f\"listOfSections is {listOfSections}\")\n",
    "    if(listOfSections):\n",
    "        embeddings[i[1]] = []\n",
    "        for index, value in enumerate(listOfSections):\n",
    "            print(f\"{type(i[1])} and {type(index)} and {type(listOfSections)} and {type(value)}\")\n",
    "            embeddings[i[1]].append((model.encode(value), index, value, i[1]))\n",
    "print(f\"Run time = {time.perf_counter() - start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for index, words in enumerate(mainPaperSections):\n",
    "#     print(f\"Index {index}:\\n{words}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sectionSimilarityScores is a list of lists where each sublist is a similarity score of the section embeddings and then\n",
    "each sublist is -> Section Embedding, Index into the section, the section text, the title again(?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare a section against all the others and return the best 5\n",
    "# TODO Create a general variable for the main paper\n",
    "# Compare the 7th section to all the other sections\n",
    "sectionSimilarityScores = []\n",
    "\n",
    "for key, value in embeddings.items():\n",
    "    if(key == \"BERT Rediscovers the Classical NLP Pipeline\"):\n",
    "        continue\n",
    "    for i in range(len(value)):\n",
    "        # print(value[i])\n",
    "        sectionSimilarityScores.append([newCosineSimilarity(torch.tensor(embeddings[\"BERT Rediscovers the Classical NLP Pipeline\"][7][0]),  torch.tensor(value[i][0])), value[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to return just the first value of the list to be used in list.sort()\n",
    "def getKeySecondSort(listOfCosAndValue):\n",
    "    return listOfCosAndValue[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the sections \n",
    "sectionSimilarityScores.sort(reverse=True, key=getKeySecondSort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(sectionSimilarityScores[1][1][2])\n",
    "for i in sectionSimilarityScores:\n",
    "    print(f\"Score: {i[0]} and text: {i[1][2]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
