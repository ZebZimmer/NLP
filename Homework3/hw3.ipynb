{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    LogitsProcessorList,\n",
    "    MinLengthLogitsProcessor,\n",
    "    StoppingCriteriaList,\n",
    "    MaxLengthCriteria,\n",
    "    AutoModelForSeq2SeqLM,\n",
    ")\n",
    "import numpy as np\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "# prompt = \"Today I believe we can finally\"\n",
    "prompt = \"It might be possible to\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Greedy Search TASK 1\n",
    "\n",
    "outputs = model.generate(input_ids, num_beams=1, do_sample=False, max_length=30,return_dict_in_generate=True, output_scores=True)\n",
    "transition_scores = model.compute_transition_scores(outputs.sequences, outputs.scores, normalize_logits=True)\n",
    "print(tokenizer.batch_decode(outputs[0], skip_special_tokens=True))\n",
    "\n",
    "input_length = 1 if model.config.is_encoder_decoder else input_ids.shape[1]\n",
    "generated_tokens = outputs.sequences[:, input_length:]\n",
    "\n",
    "perplexity = 0\n",
    "likelihood = 0\n",
    "\n",
    "for tok, score in zip(generated_tokens[0], transition_scores[0]):\n",
    "    # | token | token string | logits | probability\n",
    "    print(f\"| {tok:5d} | {tokenizer.decode(tok):8s} | {score.detach().numpy():.3f} | {np.exp(score.detach().numpy()):.2%}\")\n",
    "    likelihood += np.log(-1 * score.detach().numpy())\n",
    "    perplexity += np.log(np.exp(score.detach().numpy()))\n",
    "\n",
    "print(f\"Length of the output: {generated_tokens.shape[1]}\")\n",
    "print(f\"Perplexity: {np.exp((-1/ generated_tokens.shape[1]) * perplexity)}\")\n",
    "print(f\"likelihood: {likelihood}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Beam Search TASK 1\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    num_beams=3,\n",
    "    early_stopping=True,\n",
    "    max_length=30,\n",
    "    return_dict_in_generate=True,\n",
    "    output_scores=True,\n",
    ")\n",
    "transition_scores = model.compute_transition_scores(\n",
    "    outputs.sequences, outputs.scores, outputs.beam_indices, normalize_logits=False\n",
    ")\n",
    "# If you sum the generated tokens' scores and apply the length penalty, you'll get the sequence scores.\n",
    "# Tip: recomputing the scores is only guaranteed to match with `normalize_logits=False`. Depending on the\n",
    "# use case, you might want to recompute it with `normalize_logits=True`.\n",
    "output_length = input_length + np.sum(transition_scores.numpy() < 0, axis=1)\n",
    "length_penalty = model.generation_config.length_penalty\n",
    "reconstructed_scores = transition_scores.sum(axis=1) / (output_length**length_penalty)\n",
    "\n",
    "print(tokenizer.batch_decode(outputs[0], skip_special_tokens=True))\n",
    "\n",
    "# input_length = 1 if model.config.is_encoder_decoder else input_ids.shape[1]\n",
    "generated_tokens = outputs.sequences[:, input_length:]\n",
    "\n",
    "perplexity = 0\n",
    "likelihood = 0\n",
    "\n",
    "for tok, score in zip(generated_tokens[0], transition_scores[0]):\n",
    "    # | token | token string | logits | probability\n",
    "    print(f\"| {tok:5d} | {tokenizer.decode(tok):8s} | {score.detach().numpy():.3f} | {np.exp(score.detach().numpy()):.2%}\")\n",
    "    likelihood += np.log(-1 * score.detach().numpy())\n",
    "    perplexity += np.log(np.exp(score.detach().numpy()))\n",
    "\n",
    "print(f\"Length of the output: {generated_tokens.shape[1]}\")\n",
    "print(f\"Perplexity: {np.exp((-1/ generated_tokens.shape[1]) * perplexity)}\")\n",
    "print(f\"likelihood: {likelihood}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Top-K Sampling TASK 1\n",
    "\n",
    "outputs = model.generate(input_ids, do_sample=True, max_length=30, return_dict_in_generate=True, output_scores=True, top_k=50)\n",
    "\n",
    "print(tokenizer.batch_decode(outputs[0], skip_special_tokens=True))\n",
    "\n",
    "input_length = 1 if model.config.is_encoder_decoder else input_ids.shape[1]\n",
    "generated_tokens = outputs.sequences[:, input_length:]\n",
    "\n",
    "transition_scores = model.compute_transition_scores(outputs.sequences, outputs.scores, normalize_logits=True)\n",
    "\n",
    "perplexity = 0\n",
    "likelihood = 0\n",
    "\n",
    "for tok, score in zip(generated_tokens[0], transition_scores[0]):\n",
    "    # | token | token string | logits | probability\n",
    "    print(f\"| {tok:5d} | {tokenizer.decode(tok):8s} | {score.detach().numpy():.3f} | {np.exp(score.detach().numpy()):.2%}\")\n",
    "    likelihood += np.log(-1 * score.detach().numpy())\n",
    "    perplexity += np.log(np.exp(score.detach().numpy()))\n",
    "\n",
    "print(f\"Length of the output: {generated_tokens.shape[1]}\")\n",
    "print(f\"Perplexity: {np.exp((-1/ generated_tokens.shape[1]) * perplexity)}\")\n",
    "print(f\"likelihood: {likelihood}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Top-P Sampling TASK 1 \n",
    "#Need to figure out a good value for top_p\n",
    "#top_p = 4 gave good values but it's supposed to be bounded (0,1)\n",
    "\n",
    "outputs = model.generate(input_ids, top_p = 0.92, top_k=0, do_sample=True, max_length=30, return_dict_in_generate=True, output_scores=True)\n",
    "print(tokenizer.batch_decode(outputs[0], skip_special_tokens=True))\n",
    "\n",
    "input_length = 1 if model.config.is_encoder_decoder else input_ids.shape[1]\n",
    "generated_tokens = outputs.sequences[:, input_length:]\n",
    "\n",
    "transition_scores = model.compute_transition_scores(outputs.sequences, outputs.scores, normalize_logits=True)\n",
    "\n",
    "perplexity = 0\n",
    "likelihood = 0\n",
    "\n",
    "for tok, score in zip(generated_tokens[0], transition_scores[0]):\n",
    "    # | token | token string | logits | probability\n",
    "    print(f\"| {tok:5d} | {tokenizer.decode(tok):8s} | {score.detach().numpy():.3f} | {np.exp(score.detach().numpy()):.2%}\")\n",
    "    likelihood += np.log(-1 * score.detach().numpy())\n",
    "    perplexity += np.log(np.exp(score.detach().numpy()))\n",
    "\n",
    "print(f\"Length of the output: {generated_tokens.shape[1]}\")\n",
    "print(f\"Perplexity: {np.exp((-1/ generated_tokens.shape[1]) * perplexity)}\")\n",
    "print(f\"likelihood: {likelihood}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**************************************************\n",
    "Task 2 starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TASK 2\n",
    "#Load the dataset\n",
    "\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
    "\n",
    "max_length = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TASK 2 (switched from a downstream BERT because things were failing)\n",
    "tokenizer2 = AutoTokenizer.from_pretrained(\"t5-base\")\n",
    "model2 = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\", pad_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_ids = dataset[\"test\"][:50]\n",
    "tokenizerInputs =  []\n",
    "inputGroundTruths = []\n",
    "for i in range(50):\n",
    "    tokenizerInputs.append(dataset[\"test\"][i][\"article\"])\n",
    "    inputGroundTruths.append(dataset[\"test\"][i][\"highlights\"])\n",
    "print(tokenizerInputs[0])\n",
    "\n",
    "encoder_input_ids = torch.LongTensor()\n",
    "\n",
    "for i in range(50):\n",
    "    encoder_input_ids = torch.cat((encoder_input_ids, tokenizer2(tokenizerInputs[i], return_tensors=\"pt\", padding='max_length', truncation=True).input_ids))\n",
    "print(tokenizer2.decode(encoder_input_ids[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task 2 \n",
    "outputs2 = []\n",
    "#Greedy Search\n",
    "outputs2.append(model2.generate(encoder_input_ids, num_beams=1, do_sample=False, max_length=max_length, no_repeat_ngram_size=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Beam Search\n",
    "outputs2.append(model2.generate(encoder_input_ids, num_beams=3, early_stopping=True, max_length=max_length, no_repeat_ngram_size=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Top-K Sampling\n",
    "outputs2.append(model2.generate(encoder_input_ids, do_sample=True, top_k=30, max_length=max_length, no_repeat_ngram_size=2))\n",
    "\n",
    "#max_length = 50 with top_k>=40 gave the \"index out of range in self\" error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Top-P Sampling\n",
    "outputs2.append(model2.generate(encoder_input_ids, do_sample=True, top_p=0.8, top_k=0, max_length=max_length, no_repeat_ngram_size=2)) \n",
    "\n",
    "#max_length = 30 with top_p>=0.8 gave the \"index out of range in self\" error\n",
    "#max_length = 50 with top_p>=0.4 gave the \"index out of range in self\" error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(outputs[0][0])\n",
    "print(tokenizer2.decode(outputs2[0][0], skip_special_tokens=True))\n",
    "print(tokenizer2.decode(outputs2[1][0], skip_special_tokens=True))\n",
    "print(tokenizer2.decode(outputs2[2][0], skip_special_tokens=True))\n",
    "print(tokenizer2.decode(outputs2[3][0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|██████████| 6.81k/6.81k [00:00<00:00, 6.80MB/s]\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\zebzi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\zebzi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\zebzi\\AppData\\Roaming\\nltk_data...\n"
     ]
    }
   ],
   "source": [
    "from evaluate import load\n",
    "roguescore = load(\"meteor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "references = []\n",
    "predictions = []\n",
    "results = []\n",
    "\n",
    "# references.append([inputGroundTruths[0], inputGroundTruths[0], inputGroundTruths[0], inputGroundTruths[0]])\n",
    "# predictions.append([tokenizer2.decode(outputs2[0][0], skip_special_tokens=True), tokenizer2.decode(outputs2[1][0], skip_special_tokens=True), tokenizer2.decode(outputs2[2][0], skip_special_tokens=True), tokenizer2.decode(outputs2[3][0], skip_special_tokens=True)])\n",
    "# results.append([roguescore.compute(predictions=predictions[0], references=references[0])])\n",
    "# results.append([roguescore.compute(predictions=[tokenizer2.decode(outputs2[0][0], skip_special_tokens=True)], references=[inputGroundTruths[0]])])\n",
    "# results.append([roguescore.compute(predictions=[\"hey\"], references=[[\"hey\"]])])\n",
    "\n",
    "# print(predictions[0])\n",
    "# print(references[0])\n",
    "# print(results)\n",
    "# print(outputs2)\n",
    "with open(\"generatedText10.txt\", \"w\") as f:\n",
    "    for i in range(len(outputs2[0])):\n",
    "        references.append([inputGroundTruths[i], inputGroundTruths[i], inputGroundTruths[i], inputGroundTruths[i]])\n",
    "        predictions.append([tokenizer2.decode(outputs2[0][i], skip_special_tokens=True), tokenizer2.decode(outputs2[1][i], skip_special_tokens=True), tokenizer2.decode(outputs2[2][i], skip_special_tokens=True), tokenizer2.decode(outputs2[3][i], skip_special_tokens=True)])\n",
    "        # print(predictions[i][0])\n",
    "        # print(references[i][0])\n",
    "        temp = []\n",
    "        for j in range(4):\n",
    "            temp.append([roguescore.compute(predictions=[predictions[i][j]], references=[references[i][j]])])\n",
    "        results.append(temp)\n",
    "\n",
    "        f.write(f\"Ground Truth: {inputGroundTruths[i]} \\n\")\n",
    "        f.write(f\"Greedy Search: {predictions[i][0]} \\n\")\n",
    "        f.write(f\"Beam Search: {predictions[i][1]} \\n\")\n",
    "        f.write(f\"Top-K Sampling: {predictions[i][2]} \\n\")\n",
    "        f.write(f\"Top-P Sampling: {predictions[i][3]} \\n\")\n",
    "        f.write(f\"Scores: {results[i]}\")\n",
    "        f.write(\"\\n \\n\")\n",
    "\n",
    "#generatedText is with no capped max_length so it defaulted to 20 I think top_p=0.92 and top_k=50\n",
    "#generatedText2 is with max_length = 30\n",
    "#generatedText3 is also max_length = 30 but top_p=0.7 ~ 6mins of runtime\n",
    "#generatedText4 is at max_length = 40 with top_p=0.7 ~5mins of runtime thanks to CPU acceleration\n",
    "#generatedText5 is at max_length = 50 with top_p=0.3 and top_k=25 ~Xmins of runtime but model_max_length = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'meteor': 0.08474576271186442}], [{'meteor': 0.08450704225352114}], [{'meteor': 0.09749303621169916}], [{'meteor': 0.14044943820224717}]]\n",
      "[[{'meteor': 0.11893087928390034}], [{'meteor': 0.20450957471167402}], [{'meteor': 0.09164969450101833}], [{'meteor': 0.08230452674897121}]]\n",
      "[[{'meteor': 0.11778698588090851}], [{'meteor': 0.08264462809917356}], [{'meteor': 0.08196721311475409}], [{'meteor': 0.05555555555555555}]]\n",
      "[[{'meteor': 0.0}], [{'meteor': 0.0}], [{'meteor': 0.07125890736342043}], [{'meteor': 0.02380952380952381}]]\n",
      "[[{'meteor': 0.08512585812356978}], [{'meteor': 0.0453514739229025}], [{'meteor': 0.12443438914027148}], [{'meteor': 0.0894854586129754}]]\n",
      "[[{'meteor': 0.11673151750972763}], [{'meteor': 0.27542892156862747}], [{'meteor': 0.1372549019607843}], [{'meteor': 0.09727626459143969}]]\n",
      "[[{'meteor': 0.16827783745076977}], [{'meteor': 0.16827783745076977}], [{'meteor': 0.08488351092649449}], [{'meteor': 0.1296743800039055}]]\n",
      "[[{'meteor': 0.16710961789188644}], [{'meteor': 0.14550357192884156}], [{'meteor': 0.10615711252653927}], [{'meteor': 0.11652542372881355}]]\n",
      "[[{'meteor': 0.017605633802816902}], [{'meteor': 0.035211267605633804}], [{'meteor': 0.05190311418685121}], [{'meteor': 0.05190311418685121}]]\n",
      "[[{'meteor': 0.042553191489361694}], [{'meteor': 0.020920502092050212}], [{'meteor': 0.1224489795918367}], [{'meteor': 0.10204081632653061}]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(results[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
