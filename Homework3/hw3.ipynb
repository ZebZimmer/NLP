{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    LogitsProcessorList,\n",
    "    MinLengthLogitsProcessor,\n",
    "    StoppingCriteriaList,\n",
    "    MaxLengthCriteria,\n",
    "    BeamSearchScorer,\n",
    ")\n",
    "import numpy as np\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "stopping_criteria = StoppingCriteriaList([MaxLengthCriteria(max_length=30)])\n",
    "\n",
    "# prompt = \"Today I believe we can finally\"\n",
    "prompt = \"It might be possible to\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "# outputs = model.generate(input_ids, do_sample=False, max_length = 30)\n",
    "# tokenizer.batch_decode(outputs, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"It might be possible to get a better understanding of the nature of the problem, but it's not clear how to do that.\\n\\nThe problem\"]\n",
      "|   651 |  get     | -3.341 | 3.54%\n",
      "|   257 |  a       | -1.936 | 14.43%\n",
      "|  1365 |  better  | -3.269 | 3.80%\n",
      "|  4547 |  understanding | -1.486 | 22.63%\n",
      "|   286 |  of      | -0.151 | 86.03%\n",
      "|   262 |  the     | -1.247 | 28.73%\n",
      "|  3450 |  nature  | -4.258 | 1.41%\n",
      "|   286 |  of      | -0.083 | 92.02%\n",
      "|   262 |  the     | -1.289 | 27.57%\n",
      "|  1917 |  problem | -4.050 | 1.74%\n",
      "|    11 | ,        | -1.583 | 20.54%\n",
      "|   475 |  but     | -0.740 | 47.72%\n",
      "|   340 |  it      | -1.707 | 18.14%\n",
      "|   338 | 's       | -1.193 | 30.33%\n",
      "|   407 |  not     | -1.242 | 28.88%\n",
      "|  1598 |  clear   | -2.580 | 7.58%\n",
      "|   703 |  how     | -1.511 | 22.07%\n",
      "|   284 |  to      | -1.713 | 18.03%\n",
      "|   466 |  do      | -1.116 | 32.77%\n",
      "|   326 |  that    | -1.147 | 31.77%\n",
      "|    13 | .        | -0.663 | 51.54%\n",
      "|   198 | \n",
      "        | -1.139 | 32.01%\n",
      "|   198 | \n",
      "        | -0.002 | 99.84%\n",
      "|   464 | The      | -2.294 | 10.09%\n",
      "|  1917 |  problem | -2.906 | 5.47%\n",
      "Length of the output: 25\n",
      "Perplexity: 5.505365413942294\n",
      "likelihood: 0.7923318616828192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zebzi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\generation\\utils.py:1029: UserWarning: Use of index_put_ on expanded tensors is deprecated. Please clone() the tensor before performing this operation. This also applies to advanced indexing e.g. tensor[indices] = tensor (Triggered internally at ..\\aten\\src\\ATen\\native\\TensorAdvancedIndexing.cpp:634.)\n",
      "  beam_indices[beam_indices_mask] = 0\n",
      "c:\\Users\\zebzi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\generation\\utils.py:1029: UserWarning: Use of masked_fill_ on expanded tensors is deprecated. Please clone() the tensor before performing this operation. This also applies to advanced indexing e.g. tensor[mask] = scalar (Triggered internally at ..\\aten\\src\\ATen\\native\\TensorAdvancedIndexing.cpp:1654.)\n",
      "  beam_indices[beam_indices_mask] = 0\n"
     ]
    }
   ],
   "source": [
    "#Greedy Search\n",
    "\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "model.generation_config.pad_token_id = model.generation_config.eos_token_id\n",
    "logits_processor = LogitsProcessorList([MinLengthLogitsProcessor(10, eos_token_id = model.generation_config.eos_token_id),])\n",
    "outputs = model.greedy_search(input_ids, logits_processor=logits_processor, return_dict_in_generate=True, output_scores=True, stopping_criteria=stopping_criteria)\n",
    "# print(outputs[1])\n",
    "transition_scores = model.compute_transition_scores(outputs.sequences, outputs.scores, normalize_logits=True)\n",
    "print(tokenizer.batch_decode(outputs[0], skip_special_tokens=True))\n",
    "\n",
    "input_length = 1 if model.config.is_encoder_decoder else input_ids.shape[1]\n",
    "generated_tokens = outputs.sequences[:, input_length:]\n",
    "\n",
    "perplexity = 0\n",
    "likelihood = 0\n",
    "\n",
    "for tok, score in zip(generated_tokens[0], transition_scores[0]):\n",
    "    # | token | token string | logits | probability\n",
    "    print(f\"| {tok:5d} | {tokenizer.decode(tok):8s} | {score.detach().numpy():.3f} | {np.exp(score.detach().numpy()):.2%}\")\n",
    "    likelihood += np.log(-1 * score.detach().numpy())\n",
    "    perplexity += np.log(np.exp(score.detach().numpy()))\n",
    "\n",
    "print(f\"Length of the output: {generated_tokens.shape[1]}\")\n",
    "print(f\"Perplexity: {np.exp((-1/ generated_tokens.shape[1]) * perplexity)}\")\n",
    "print(f\"likelihood: {likelihood}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"It might be possible to get a better idea of what is going on, but it's not going to be easy.\\n\", \"It might be possible to get a better idea of what is going on, but it's not going to happen.\\n\\n\", \"It might be possible to get a better idea of what is going on, but it's not going to be easy. The\", \"It might be possible to get a better idea of what is going on, but it's not going to be easy. It\"]\n",
      "|   651 |  get     | -3.341 | 3.54%\n",
      "|   257 |  a       | -1.936 | 14.43%\n",
      "|  1365 |  better  | -3.269 | 3.80%\n",
      "|  2126 |  idea    | -1.894 | 15.04%\n",
      "|   286 |  of      | -0.236 | 79.01%\n",
      "|   644 |  what    | -1.419 | 24.20%\n",
      "|   318 |  is      | -3.010 | 4.93%\n",
      "|  1016 |  going   | -1.086 | 33.77%\n",
      "|   319 |  on      | -0.074 | 92.86%\n",
      "|    11 | ,        | -2.350 | 9.54%\n",
      "|   475 |  but     | -0.540 | 58.28%\n",
      "|   340 |  it      | -1.867 | 15.46%\n",
      "|   338 | 's       | -1.378 | 25.20%\n",
      "|   407 |  not     | -1.351 | 25.90%\n",
      "|  1016 |  going   | -2.077 | 12.53%\n",
      "|   284 |  to      | -0.013 | 98.71%\n",
      "|   307 |  be      | -0.929 | 39.51%\n",
      "|  2562 |  easy    | -1.329 | 26.46%\n",
      "|    13 | .        | -0.558 | 57.22%\n",
      "|   198 | \n",
      "        | -1.331 | 26.42%\n",
      "Length of the output: 20\n",
      "Perplexity: 4.478586661831552\n",
      "likelihood: -1.0414466998686494\n"
     ]
    }
   ],
   "source": [
    "#Beam Search\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens=20,\n",
    "    num_beams=4,\n",
    "    num_return_sequences=4,\n",
    "    return_dict_in_generate=True,\n",
    "    output_scores=True,\n",
    ")\n",
    "transition_scores = model.compute_transition_scores(\n",
    "    outputs.sequences, outputs.scores, outputs.beam_indices, normalize_logits=False\n",
    ")\n",
    "# If you sum the generated tokens' scores and apply the length penalty, you'll get the sequence scores.\n",
    "# Tip: recomputing the scores is only guaranteed to match with `normalize_logits=False`. Depending on the\n",
    "# use case, you might want to recompute it with `normalize_logits=True`.\n",
    "output_length = input_length + np.sum(transition_scores.numpy() < 0, axis=1)\n",
    "length_penalty = model.generation_config.length_penalty\n",
    "reconstructed_scores = transition_scores.sum(axis=1) / (output_length**length_penalty)\n",
    "\n",
    "print(tokenizer.batch_decode(outputs[0], skip_special_tokens=True))\n",
    "\n",
    "# input_length = 1 if model.config.is_encoder_decoder else input_ids.shape[1]\n",
    "generated_tokens = outputs.sequences[:, input_length:]\n",
    "\n",
    "perplexity = 0\n",
    "likelihood = 0\n",
    "\n",
    "for tok, score in zip(generated_tokens[0], transition_scores[0]):\n",
    "    # | token | token string | logits | probability\n",
    "    print(f\"| {tok:5d} | {tokenizer.decode(tok):8s} | {score.detach().numpy():.3f} | {np.exp(score.detach().numpy()):.2%}\")\n",
    "    likelihood += np.log(-1 * score.detach().numpy())\n",
    "    perplexity += np.log(np.exp(score.detach().numpy()))\n",
    "\n",
    "print(f\"Length of the output: {generated_tokens.shape[1]}\")\n",
    "print(f\"Perplexity: {np.exp((-1/ generated_tokens.shape[1]) * perplexity)}\")\n",
    "print(f\"likelihood: {likelihood}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"It might be possible to get a better understanding of the nature of the problem, but it's not clear how to do that.\\n\\nThe problem\"]\n",
      "|   651 |  get     | -3.341 | 3.54%\n",
      "|   257 |  a       | -1.936 | 14.43%\n",
      "|  1365 |  better  | -3.269 | 3.80%\n",
      "|  4547 |  understanding | -1.486 | 22.63%\n",
      "|   286 |  of      | -0.151 | 86.03%\n",
      "|   262 |  the     | -1.247 | 28.73%\n",
      "|  3450 |  nature  | -4.258 | 1.41%\n",
      "|   286 |  of      | -0.083 | 92.02%\n",
      "|   262 |  the     | -1.289 | 27.57%\n",
      "|  1917 |  problem | -4.050 | 1.74%\n",
      "|    11 | ,        | -1.583 | 20.54%\n",
      "|   475 |  but     | -0.740 | 47.72%\n",
      "|   340 |  it      | -1.707 | 18.14%\n",
      "|   338 | 's       | -1.193 | 30.33%\n",
      "|   407 |  not     | -1.242 | 28.88%\n",
      "|  1598 |  clear   | -2.580 | 7.58%\n",
      "|   703 |  how     | -1.511 | 22.07%\n",
      "|   284 |  to      | -1.713 | 18.03%\n",
      "|   466 |  do      | -1.116 | 32.77%\n",
      "|   326 |  that    | -1.147 | 31.77%\n",
      "|    13 | .        | -0.663 | 51.54%\n",
      "|   198 | \n",
      "        | -1.139 | 32.01%\n",
      "|   198 | \n",
      "        | -0.002 | 99.84%\n",
      "|   464 | The      | -2.294 | 10.09%\n",
      "|  1917 |  problem | -2.906 | 5.47%\n",
      "Length of the output: 25\n",
      "Perplexity: 5.505372557671196\n",
      "likelihood: 0.7923689491236807\n"
     ]
    }
   ],
   "source": [
    "#Top_k Sampling\n",
    "\n",
    "outputs = model.contrastive_search(input_ids, do_sample=True, max_length=20, stopping_criteria=stopping_criteria, return_dict_in_generate=True, output_scores=True)\n",
    "\n",
    "print(tokenizer.batch_decode(outputs[0], skip_special_tokens=True))\n",
    "\n",
    "input_length = 1 if model.config.is_encoder_decoder else input_ids.shape[1]\n",
    "generated_tokens = outputs.sequences[:, input_length:]\n",
    "\n",
    "transition_scores = model.compute_transition_scores(outputs.sequences, outputs.scores, normalize_logits=True)\n",
    "\n",
    "perplexity = 0\n",
    "likelihood = 0\n",
    "\n",
    "for tok, score in zip(generated_tokens[0], transition_scores[0]):\n",
    "    # | token | token string | logits | probability\n",
    "    print(f\"| {tok:5d} | {tokenizer.decode(tok):8s} | {score.detach().numpy():.3f} | {np.exp(score.detach().numpy()):.2%}\")\n",
    "    likelihood += np.log(-1 * score.detach().numpy())\n",
    "    perplexity += np.log(np.exp(score.detach().numpy()))\n",
    "\n",
    "print(f\"Length of the output: {generated_tokens.shape[1]}\")\n",
    "print(f\"Perplexity: {np.exp((-1/ generated_tokens.shape[1]) * perplexity)}\")\n",
    "print(f\"likelihood: {likelihood}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|  1486 |  design  | -4.740 | 0.87%\n",
      "|   290 |  and     | -3.173 | 4.19%\n",
      "|  1382 |  build   | -0.995 | 36.96%\n",
      "|  3341 |  systems | -4.780 | 0.84%\n",
      "|  1262 |  using   | -3.235 | 3.93%\n",
      "|  4683 |  existing | -2.848 | 5.80%\n",
      "|   513 |  3       | -5.035 | 0.65%\n",
      "|    35 | D        | -0.109 | 89.72%\n",
      "|  4981 |  models  | -2.218 | 10.89%\n",
      "|    11 | ,        | -1.177 | 30.81%\n",
      "|   290 |  and     | -3.198 | 4.09%\n",
      "|  3863 |  maybe   | -4.084 | 1.68%\n",
      "|   779 |  use     | -3.466 | 3.12%\n",
      "|  4683 |  existing | -2.760 | 6.33%\n",
      "|  3788 |  software | -3.622 | 2.67%\n",
      "|   284 |  to      | -0.816 | 44.21%\n",
      "|   787 |  make    | -2.445 | 8.67%\n",
      "|   606 |  them    | -1.699 | 18.29%\n",
      "|   517 |  more    | -1.629 | 19.61%\n",
      "| 12846 |  flexible | -3.002 | 4.97%\n",
      "|   290 |  and     | -1.523 | 21.81%\n",
      "|  6068 |  adapt   | -2.681 | 6.85%\n",
      "|   540 | able     | -0.018 | 98.19%\n",
      "|    13 | .        | -0.542 | 58.16%\n",
      "|   314 |  I       | -3.616 | 2.69%\n",
      "Length of the output: 25\n",
      "Perplexity: 12.635014748241938\n",
      "likelihood: 13.84558696688466\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(input_ids, top_p = 4, do_sample=True, max_length=30, return_dict_in_generate=True, output_scores=True)\n",
    "print(tokenizer.batch_decode(outputs[0], skip_special_tokens=True))\n",
    "\n",
    "input_length = 1 if model.config.is_encoder_decoder else input_ids.shape[1]\n",
    "generated_tokens = outputs.sequences[:, input_length:]\n",
    "\n",
    "transition_scores = model.compute_transition_scores(outputs.sequences, outputs.scores, normalize_logits=True)\n",
    "\n",
    "perplexity = 0\n",
    "likelihood = 0\n",
    "\n",
    "for tok, score in zip(generated_tokens[0], transition_scores[0]):\n",
    "    # | token | token string | logits | probability\n",
    "    print(f\"| {tok:5d} | {tokenizer.decode(tok):8s} | {score.detach().numpy():.3f} | {np.exp(score.detach().numpy()):.2%}\")\n",
    "    likelihood += np.log(-1 * score.detach().numpy())\n",
    "    perplexity += np.log(np.exp(score.detach().numpy()))\n",
    "\n",
    "print(f\"Length of the output: {generated_tokens.shape[1]}\")\n",
    "print(f\"Perplexity: {np.exp((-1/ generated_tokens.shape[1]) * perplexity)}\")\n",
    "print(f\"likelihood: {likelihood}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
