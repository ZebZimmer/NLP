{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zebzi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    LogitsProcessorList,\n",
    "    MinLengthLogitsProcessor,\n",
    "    StoppingCriteriaList,\n",
    "    MaxLengthCriteria,\n",
    "    AutoModelForSeq2SeqLM,\n",
    ")\n",
    "import numpy as np\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "# prompt = \"Today I believe we can finally\"\n",
    "prompt = \"It might be possible to\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zebzi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\generation\\utils.py:1186: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"It might be possible to get a better understanding of the nature of the problem, but it's not clear how to do that.\\n\\nThe problem\"]\n",
      "|   651 |  get     | -3.341 | 3.54%\n",
      "|   257 |  a       | -1.936 | 14.43%\n",
      "|  1365 |  better  | -3.269 | 3.80%\n",
      "|  4547 |  understanding | -1.486 | 22.63%\n",
      "|   286 |  of      | -0.151 | 86.03%\n",
      "|   262 |  the     | -1.247 | 28.73%\n",
      "|  3450 |  nature  | -4.258 | 1.41%\n",
      "|   286 |  of      | -0.083 | 92.02%\n",
      "|   262 |  the     | -1.289 | 27.57%\n",
      "|  1917 |  problem | -4.050 | 1.74%\n",
      "|    11 | ,        | -1.583 | 20.54%\n",
      "|   475 |  but     | -0.740 | 47.72%\n",
      "|   340 |  it      | -1.707 | 18.14%\n",
      "|   338 | 's       | -1.193 | 30.33%\n",
      "|   407 |  not     | -1.242 | 28.88%\n",
      "|  1598 |  clear   | -2.580 | 7.58%\n",
      "|   703 |  how     | -1.511 | 22.07%\n",
      "|   284 |  to      | -1.713 | 18.03%\n",
      "|   466 |  do      | -1.116 | 32.77%\n",
      "|   326 |  that    | -1.147 | 31.77%\n",
      "|    13 | .        | -0.663 | 51.54%\n",
      "|   198 | \n",
      "        | -1.139 | 32.01%\n",
      "|   198 | \n",
      "        | -0.002 | 99.84%\n",
      "|   464 | The      | -2.294 | 10.09%\n",
      "|  1917 |  problem | -2.906 | 5.47%\n",
      "Length of the output: 25\n",
      "Perplexity: 5.505372557671196\n",
      "likelihood: 0.7923689491236807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zebzi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\generation\\utils.py:1029: UserWarning: Use of index_put_ on expanded tensors is deprecated. Please clone() the tensor before performing this operation. This also applies to advanced indexing e.g. tensor[indices] = tensor (Triggered internally at ..\\aten\\src\\ATen\\native\\TensorAdvancedIndexing.cpp:634.)\n",
      "  beam_indices[beam_indices_mask] = 0\n",
      "c:\\Users\\zebzi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\generation\\utils.py:1029: UserWarning: Use of masked_fill_ on expanded tensors is deprecated. Please clone() the tensor before performing this operation. This also applies to advanced indexing e.g. tensor[mask] = scalar (Triggered internally at ..\\aten\\src\\ATen\\native\\TensorAdvancedIndexing.cpp:1654.)\n",
      "  beam_indices[beam_indices_mask] = 0\n"
     ]
    }
   ],
   "source": [
    "#Greedy Search TASK 1\n",
    "\n",
    "outputs = model.generate(input_ids, num_beams=1, do_sample=False, max_length=30,return_dict_in_generate=True, output_scores=True)\n",
    "transition_scores = model.compute_transition_scores(outputs.sequences, outputs.scores, normalize_logits=True)\n",
    "print(tokenizer.batch_decode(outputs[0], skip_special_tokens=True))\n",
    "\n",
    "input_length = 1 if model.config.is_encoder_decoder else input_ids.shape[1]\n",
    "generated_tokens = outputs.sequences[:, input_length:]\n",
    "\n",
    "perplexity = 0\n",
    "likelihood = 0\n",
    "\n",
    "for tok, score in zip(generated_tokens[0], transition_scores[0]):\n",
    "    # | token | token string | logits | probability\n",
    "    print(f\"| {tok:5d} | {tokenizer.decode(tok):8s} | {score.detach().numpy():.3f} | {np.exp(score.detach().numpy()):.2%}\")\n",
    "    likelihood += np.log(-1 * score.detach().numpy())\n",
    "    perplexity += np.log(np.exp(score.detach().numpy()))\n",
    "\n",
    "print(f\"Length of the output: {generated_tokens.shape[1]}\")\n",
    "print(f\"Perplexity: {np.exp((-1/ generated_tokens.shape[1]) * perplexity)}\")\n",
    "print(f\"likelihood: {likelihood}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"It might be possible to get a better understanding of how the system works, but it's not going to be easy.\\n\\nIn the meantime,\"]\n",
      "|   651 |  get     | -3.341 | 3.54%\n",
      "|   257 |  a       | -1.936 | 14.43%\n",
      "|  1365 |  better  | -3.269 | 3.80%\n",
      "|  4547 |  understanding | -1.486 | 22.63%\n",
      "|   286 |  of      | -0.151 | 86.02%\n",
      "|   703 |  how     | -1.795 | 16.61%\n",
      "|   262 |  the     | -1.791 | 16.67%\n",
      "|  1080 |  system  | -4.110 | 1.64%\n",
      "|  2499 |  works   | -0.338 | 71.32%\n",
      "|    11 | ,        | -1.423 | 24.09%\n",
      "|   475 |  but     | -0.623 | 53.65%\n",
      "|   340 |  it      | -1.656 | 19.08%\n",
      "|   338 | 's       | -0.925 | 39.67%\n",
      "|   407 |  not     | -1.277 | 27.90%\n",
      "|  1016 |  going   | -2.657 | 7.02%\n",
      "|   284 |  to      | -0.006 | 99.38%\n",
      "|   307 |  be      | -1.001 | 36.75%\n",
      "|  2562 |  easy    | -1.028 | 35.76%\n",
      "|    13 | .        | -0.485 | 61.58%\n",
      "|   198 | \n",
      "        | -1.203 | 30.02%\n",
      "|   198 | \n",
      "        | -0.001 | 99.91%\n",
      "|   818 | In       | -3.445 | 3.19%\n",
      "|   262 |  the     | -1.601 | 20.17%\n",
      "| 14324 |  meantime | -1.308 | 27.05%\n",
      "|    11 | ,        | -0.036 | 96.45%\n",
      "Length of the output: 25\n",
      "Perplexity: 4.37375969279929\n",
      "likelihood: -9.344831745940242\n"
     ]
    }
   ],
   "source": [
    "#Beam Search TASK 1\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    num_beams=3,\n",
    "    early_stopping=True,\n",
    "    max_length=30,\n",
    "    return_dict_in_generate=True,\n",
    "    output_scores=True,\n",
    ")\n",
    "transition_scores = model.compute_transition_scores(\n",
    "    outputs.sequences, outputs.scores, outputs.beam_indices, normalize_logits=False\n",
    ")\n",
    "# If you sum the generated tokens' scores and apply the length penalty, you'll get the sequence scores.\n",
    "# Tip: recomputing the scores is only guaranteed to match with `normalize_logits=False`. Depending on the\n",
    "# use case, you might want to recompute it with `normalize_logits=True`.\n",
    "output_length = input_length + np.sum(transition_scores.numpy() < 0, axis=1)\n",
    "length_penalty = model.generation_config.length_penalty\n",
    "reconstructed_scores = transition_scores.sum(axis=1) / (output_length**length_penalty)\n",
    "\n",
    "print(tokenizer.batch_decode(outputs[0], skip_special_tokens=True))\n",
    "\n",
    "# input_length = 1 if model.config.is_encoder_decoder else input_ids.shape[1]\n",
    "generated_tokens = outputs.sequences[:, input_length:]\n",
    "\n",
    "perplexity = 0\n",
    "likelihood = 0\n",
    "\n",
    "for tok, score in zip(generated_tokens[0], transition_scores[0]):\n",
    "    # | token | token string | logits | probability\n",
    "    print(f\"| {tok:5d} | {tokenizer.decode(tok):8s} | {score.detach().numpy():.3f} | {np.exp(score.detach().numpy()):.2%}\")\n",
    "    likelihood += np.log(-1 * score.detach().numpy())\n",
    "    perplexity += np.log(np.exp(score.detach().numpy()))\n",
    "\n",
    "print(f\"Length of the output: {generated_tokens.shape[1]}\")\n",
    "print(f\"Perplexity: {np.exp((-1/ generated_tokens.shape[1]) * perplexity)}\")\n",
    "print(f\"likelihood: {likelihood}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"It might be possible to make them fit the bill.\\n\\nI don't know if you are familiar with the concept of a machine as part of\"]\n",
      "|   787 |  make    | -2.534 | 7.93%\n",
      "|   606 |  them    | -4.176 | 1.54%\n",
      "|  4197 |  fit     | -4.550 | 1.06%\n",
      "|   262 |  the     | -2.868 | 5.68%\n",
      "|  2855 |  bill    | -2.032 | 13.10%\n",
      "|    13 | .        | -1.738 | 17.58%\n",
      "|   198 | \n",
      "        | -1.354 | 25.81%\n",
      "|   198 | \n",
      "        | -0.001 | 99.94%\n",
      "|    40 | I        | -4.203 | 1.50%\n",
      "|   836 |  don     | -2.558 | 7.74%\n",
      "|   470 | 't       | -0.001 | 99.95%\n",
      "|   760 |  know    | -1.129 | 32.34%\n",
      "|   611 |  if      | -1.584 | 20.51%\n",
      "|   345 |  you     | -2.672 | 6.91%\n",
      "|   389 |  are     | -3.962 | 1.90%\n",
      "|  5385 |  familiar | -1.937 | 14.42%\n",
      "|   351 |  with    | -0.018 | 98.25%\n",
      "|   262 |  the     | -0.682 | 50.56%\n",
      "|  3721 |  concept | -2.215 | 10.91%\n",
      "|   286 |  of      | -0.337 | 71.42%\n",
      "|   257 |  a       | -1.128 | 32.36%\n",
      "|  4572 |  machine | -4.832 | 0.80%\n",
      "|   355 |  as      | -4.114 | 1.63%\n",
      "|   636 |  part    | -4.549 | 1.06%\n",
      "|   286 |  of      | -0.011 | 98.91%\n",
      "Length of the output: 25\n",
      "Perplexity: 9.092421268389595\n",
      "likelihood: -7.12822581776544\n"
     ]
    }
   ],
   "source": [
    "#Top-K Sampling TASK 1\n",
    "\n",
    "outputs = model.generate(input_ids, do_sample=True, max_length=30, return_dict_in_generate=True, output_scores=True, top_k=50)\n",
    "\n",
    "print(tokenizer.batch_decode(outputs[0], skip_special_tokens=True))\n",
    "\n",
    "input_length = 1 if model.config.is_encoder_decoder else input_ids.shape[1]\n",
    "generated_tokens = outputs.sequences[:, input_length:]\n",
    "\n",
    "transition_scores = model.compute_transition_scores(outputs.sequences, outputs.scores, normalize_logits=True)\n",
    "\n",
    "perplexity = 0\n",
    "likelihood = 0\n",
    "\n",
    "for tok, score in zip(generated_tokens[0], transition_scores[0]):\n",
    "    # | token | token string | logits | probability\n",
    "    print(f\"| {tok:5d} | {tokenizer.decode(tok):8s} | {score.detach().numpy():.3f} | {np.exp(score.detach().numpy()):.2%}\")\n",
    "    likelihood += np.log(-1 * score.detach().numpy())\n",
    "    perplexity += np.log(np.exp(score.detach().numpy()))\n",
    "\n",
    "print(f\"Length of the output: {generated_tokens.shape[1]}\")\n",
    "print(f\"Perplexity: {np.exp((-1/ generated_tokens.shape[1]) * perplexity)}\")\n",
    "print(f\"likelihood: {likelihood}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It might be possible to explain it by having servants work on the code.\\n\\nWhy does it take 2 hours to code a Feather4U in']\n",
      "|  4727 |  explain | -5.907 | 0.27%\n",
      "|   340 |  it      | -3.166 | 4.22%\n",
      "|   416 |  by      | -1.353 | 25.86%\n",
      "|  1719 |  having  | -4.316 | 1.34%\n",
      "| 17523 |  servants | -9.796 | 0.01%\n",
      "|   670 |  work    | -4.432 | 1.19%\n",
      "|   319 |  on      | -1.751 | 17.36%\n",
      "|   262 |  the     | -1.252 | 28.59%\n",
      "|  2438 |  code    | -6.577 | 0.14%\n",
      "|    13 | .        | -1.566 | 20.90%\n",
      "|   198 | \n",
      "        | -1.844 | 15.82%\n",
      "|   198 | \n",
      "        | 0.000 | 100.00%\n",
      "|  5195 | Why      | -4.929 | 0.72%\n",
      "|   857 |  does    | -2.846 | 5.81%\n",
      "|   340 |  it      | -1.883 | 15.21%\n",
      "|  1011 |  take    | -2.318 | 9.85%\n",
      "|   362 |  2       | -5.077 | 0.62%\n",
      "|  2250 |  hours   | -1.571 | 20.79%\n",
      "|   284 |  to      | -0.645 | 52.46%\n",
      "|  2438 |  code    | -4.366 | 1.27%\n",
      "|   257 |  a       | -1.374 | 25.31%\n",
      "| 34501 |  Feather | -9.831 | 0.01%\n",
      "|    19 | 4        | -8.790 | 0.02%\n",
      "|    52 | U        | -4.515 | 1.09%\n",
      "|   287 |  in      | -5.075 | 0.63%\n",
      "Length of the output: 25\n",
      "Perplexity: 45.02579320804757\n",
      "likelihood: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zebzi\\AppData\\Local\\Temp\\ipykernel_27316\\2684038281.py:19: RuntimeWarning: divide by zero encountered in log\n",
      "  likelihood += np.log(-1 * score.detach().numpy())\n"
     ]
    }
   ],
   "source": [
    "#Top-P Sampling TASK 1 \n",
    "#Need to figure out a good value for top_p\n",
    "#top_p = 4 gave good values but it's supposed to be bounded (0,1)\n",
    "\n",
    "outputs = model.generate(input_ids, top_p = 0.92, top_k=0, do_sample=True, max_length=30, return_dict_in_generate=True, output_scores=True)\n",
    "print(tokenizer.batch_decode(outputs[0], skip_special_tokens=True))\n",
    "\n",
    "input_length = 1 if model.config.is_encoder_decoder else input_ids.shape[1]\n",
    "generated_tokens = outputs.sequences[:, input_length:]\n",
    "\n",
    "transition_scores = model.compute_transition_scores(outputs.sequences, outputs.scores, normalize_logits=True)\n",
    "\n",
    "perplexity = 0\n",
    "likelihood = 0\n",
    "\n",
    "for tok, score in zip(generated_tokens[0], transition_scores[0]):\n",
    "    # | token | token string | logits | probability\n",
    "    print(f\"| {tok:5d} | {tokenizer.decode(tok):8s} | {score.detach().numpy():.3f} | {np.exp(score.detach().numpy()):.2%}\")\n",
    "    likelihood += np.log(-1 * score.detach().numpy())\n",
    "    perplexity += np.log(np.exp(score.detach().numpy()))\n",
    "\n",
    "print(f\"Length of the output: {generated_tokens.shape[1]}\")\n",
    "print(f\"Perplexity: {np.exp((-1/ generated_tokens.shape[1]) * perplexity)}\")\n",
    "print(f\"likelihood: {likelihood}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**************************************************\n",
    "Task 2 starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset cnn_dailymail (C:/Users/zebzi/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de)\n",
      "100%|██████████| 3/3 [00:00<00:00, 10.57it/s]\n"
     ]
    }
   ],
   "source": [
    "#TASK 2\n",
    "#Load the dataset\n",
    "\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
    "\n",
    "max_length = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zebzi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#TASK 2 (switched from a downstream BERT because things were failing)\n",
    "from evaluate import load\n",
    "bertscore = load(\"bertscore\")\n",
    "\n",
    "tokenizer2 = AutoTokenizer.from_pretrained(\"t5-base\")\n",
    "model2 = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\", pad_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(CNN)The Palestinian Authority officially became the 123rd member of the International Criminal Court on Wednesday, a step that gives the court jurisdiction over alleged crimes in Palestinian territories. The formal accession was marked with a ceremony at The Hague, in the Netherlands, where the court is based. The Palestinians signed the ICC's founding Rome Statute in January, when they also accepted its jurisdiction over alleged crimes committed \"in the occupied Palestinian territory, including East Jerusalem, since June 13, 2014.\" Later that month, the ICC opened a preliminary examination into the situation in Palestinian territories, paving the way for possible war crimes investigations against Israelis. As members of the court, Palestinians may be subject to counter-charges as well. Israel and the United States, neither of which is an ICC member, opposed the Palestinians' efforts to join the body. But Palestinian Foreign Minister Riad al-Malki, speaking at Wednesday's ceremony, said it was a move toward greater justice. \"As Palestine formally becomes a State Party to the Rome Statute today, the world is also a step closer to ending a long era of impunity and injustice,\" he said, according to an ICC news release. \"Indeed, today brings us closer to our shared goals of justice and peace.\" Judge Kuniko Ozaki, a vice president of the ICC, said acceding to the treaty was just the first step for the Palestinians. \"As the Rome Statute today enters into force for the State of Palestine, Palestine acquires all the rights as well as responsibilities that come with being a State Party to the Statute. These are substantive commitments, which cannot be taken lightly,\" she said. Rights group Human Rights Watch welcomed the development. \"Governments seeking to penalize Palestine for joining the ICC should immediately end their pressure, and countries that support universal acceptance of the court's treaty should speak out to welcome its membership,\" said Balkees Jarrah, international justice counsel for the group. \"What's objectionable is the attempts to undermine international justice, not Palestine's decision to join a treaty to which over 100 countries around the world are members.\" In January, when the preliminary ICC examination was opened, Israeli Prime Minister Benjamin Netanyahu described it as an outrage, saying the court was overstepping its boundaries. The United States also said it \"strongly\" disagreed with the court's decision. \"As we have said repeatedly, we do not believe that Palestine is a state and therefore we do not believe that it is eligible to join the ICC,\" the State Department said in a statement. It urged the warring sides to resolve their differences through direct negotiations. \"We will continue to oppose actions against Israel at the ICC as counterproductive to the cause of peace,\" it said. But the ICC begs to differ with the definition of a state for its purposes and refers to the territories as \"Palestine.\" While a preliminary examination is not a formal investigation, it allows the court to review evidence and determine whether to investigate suspects on both sides. Prosecutor Fatou Bensouda said her office would \"conduct its analysis in full independence and impartiality.\" The war between Israel and Hamas militants in Gaza last summer left more than 2,000 people dead. The inquiry will include alleged war crimes committed since June. The International Criminal Court was set up in 2002 to prosecute genocide, crimes against humanity and war crimes. CNN's Vasco Cotovio, Kareem Khadder and Faith Karimi contributed to this report.\n",
      "(CNN)The Palestinian Authority officially became the 123rd member of the International Criminal Court on Wednesday, a step that gives the court jurisdiction over alleged crimes in Palestinian territories. The formal accession was marked with a ceremony at The Hague, in the Netherlands, where the court is based. The Palestinians signed the ICC's founding Rome Statute in January, when they also accepted its jurisdiction over alleged crimes committed \"in the occupied Palestinian territory, including East Jerusalem, since June 13, 2014.\" Later that month, the ICC opened a preliminary examination into the situation in Palestinian territories, paving the way for possible war crimes investigations against Israelis. As members of the court, Palestinians may be subject to counter-charges as well. Israel and the United States, neither of which is an ICC member, opposed the Palestinians' efforts to join the body. But Palestinian Foreign Minister Riad al-Malki, speaking at Wednesday's ceremony, said it was a move toward greater justice. \"As Palestine formally becomes a State Party to the Rome Statute today, the world is also a step closer to ending a long era of impunity and injustice,\" he said, according to an ICC news release. \"Indeed, today brings us closer to our shared goals of justice and peace.\" Judge Kuniko Ozaki, a vice president of the ICC, said acceding to the treaty was just the first step for the Palestinians. \"As the Rome Statute today enters into force for the State of Palestine, Palestine acquires all the rights as well as responsibilities that come with being a State Party to the Statute. These are substantive commitments, which cannot be taken lightly,\" she said. Rights group Human Rights Watch welcomed the development. \"Governments seeking to penalize Palestine for joining the ICC should immediately end their pressure, and countries that support universal acceptance of the court's treaty should speak out to welcome its membership,\" said Balkees Jarrah, international justice counsel for the group. \"What's objectionable is the attempts to undermine international justice, not Palestine's decision to join a treaty to which over 100 countries around the world are members.\" In January, when the preliminary ICC examination was opened, Israeli Prime Minister Benjamin Netanyahu described it as an outrage, saying the court was overstepping its boundaries.</s>\n"
     ]
    }
   ],
   "source": [
    "# input_ids = dataset[\"test\"][:50]\n",
    "tokenizerInputs =  []\n",
    "inputGroundTruths = []\n",
    "for i in range(50):\n",
    "    tokenizerInputs.append(dataset[\"test\"][i][\"article\"])\n",
    "    inputGroundTruths.append(dataset[\"test\"][i][\"highlights\"])\n",
    "print(tokenizerInputs[0])\n",
    "\n",
    "encoder_input_ids = torch.LongTensor()\n",
    "\n",
    "for i in range(50):\n",
    "    encoder_input_ids = torch.cat((encoder_input_ids, tokenizer2(tokenizerInputs[i], return_tensors=\"pt\", padding='max_length', truncation=True).input_ids))\n",
    "print(tokenizer2.decode(encoder_input_ids[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zebzi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\generation\\utils.py:1186: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Task 2 \n",
    "outputs2 = []\n",
    "#Greedy Search\n",
    "outputs2.append(model2.generate(encoder_input_ids, num_beams=1, do_sample=False, max_length=max_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Beam Search\n",
    "outputs2.append(model2.generate(encoder_input_ids, num_beams=3, early_stopping=True, max_length=max_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Top-K Sampling\n",
    "outputs2.append(model2.generate(encoder_input_ids, do_sample=True, top_k=50, max_length=max_length))\n",
    "\n",
    "#max_length = 50 with top_k>=40 gave the \"index out of range in self\" error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Top-P Sampling\n",
    "outputs2.append(model2.generate(encoder_input_ids, do_sample=True, top_p=0.92, top_k=0, max_length=max_length)) \n",
    "\n",
    "#max_length = 30 with top_p>=0.8 gave the \"index out of range in self\" error\n",
    "#max_length = 50 with top_p>=0.4 gave the \"index out of range in self\" error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the Palestinians became the 123rd member of the international criminal court on Wednesday \n",
      "the Palestinian Authority officially became the 123rd member of the international criminal court on Wednesday\n",
      "the transition to international law took place at the Hague on Wednesday. a move\n",
      "it \"brings us closer to our shared goals of justice and peace\" formally\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer2.batch_decode(outputs2[0], skip_special_tokens=True)[0])\n",
    "print(tokenizer2.batch_decode(outputs2[1], skip_special_tokens=True)[0])\n",
    "print(tokenizer2.batch_decode(outputs2[2], skip_special_tokens=True)[0])\n",
    "print(tokenizer2.batch_decode(outputs2[3], skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████| 482/482 [00:00<00:00, 284kB/s]\n",
      "c:\\Users\\zebzi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:129: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\zebzi\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading (…)olve/main/vocab.json: 100%|██████████| 899k/899k [00:01<00:00, 503kB/s]\n",
      "Downloading (…)olve/main/merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 487kB/s]\n",
      "Downloading (…)\"pytorch_model.bin\";: 100%|██████████| 1.43G/1.43G [13:01<00:00, 1.82MB/s]\n"
     ]
    }
   ],
   "source": [
    "references = []\n",
    "predictions = []\n",
    "results = []\n",
    "\n",
    "with open(\"generatedText6.txt\", \"w\") as f:\n",
    "    for i in range(len(outputs2[0])):\n",
    "        references.append([inputGroundTruths[i], inputGroundTruths[i], inputGroundTruths[i], inputGroundTruths[i]])\n",
    "        predictions.append([tokenizer2.batch_decode(outputs2[0], skip_special_tokens=True)[i], tokenizer2.batch_decode(outputs2[1], skip_special_tokens=True)[i], tokenizer2.batch_decode(outputs2[2], skip_special_tokens=True)[i], tokenizer2.batch_decode(outputs2[3], skip_special_tokens=True)[i]])\n",
    "        results.append([bertscore.compute(predictions=predictions[i], references=references[i], lang=\"en\")])\n",
    "\n",
    "        f.write(f\"Ground Truth: {inputGroundTruths[i]} \\n\")\n",
    "        f.write(f\"Greedy Search: {tokenizer2.batch_decode(outputs2[0], skip_special_tokens=True)[i]} \\n\")\n",
    "        f.write(f\"Beam Search: {tokenizer2.batch_decode(outputs2[1], skip_special_tokens=True)[i]} \\n\")\n",
    "        f.write(f\"Top-K Sampling: {tokenizer2.batch_decode(outputs2[2], skip_special_tokens=True)[i]} \\n\")\n",
    "        f.write(f\"Top-P Sampling: {tokenizer2.batch_decode(outputs2[3], skip_special_tokens=True)[i]} \\n\")\n",
    "        f.write(\"\\n \\n\")\n",
    "\n",
    "#generatedText is with no capped max_length so it defaulted to 20 I think top_p=0.92 and top_k=50\n",
    "#generatedText2 is with max_length = 30\n",
    "#generatedText3 is also max_length = 30 but top_p=0.7 ~ 6mins of runtime\n",
    "#generatedText4 is at max_length = 40 with top_p=0.7 ~5mins of runtime thanks to CPU acceleration\n",
    "#generatedText5 is at max_length = 50 with top_p=0.3 and top_k=25 ~Xmins of runtime but model_max_length = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'precision': [0.8502079248428345, 0.8599529266357422, 0.8599265217781067, 0.8428065180778503], 'recall': [0.8301665782928467, 0.8464600443840027, 0.8243862986564636, 0.8299463391304016], 'f1': [0.8400676846504211, 0.8531530499458313, 0.8417814373970032, 0.8363269567489624], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.26.1)'}]\n",
      "[{'precision': [0.8497933149337769, 0.8756840825080872, 0.8550580143928528, 0.849199652671814], 'recall': [0.8177963495254517, 0.832705020904541, 0.8195192813873291, 0.8176928758621216], 'f1': [0.8334878087043762, 0.8536539673805237, 0.8369115591049194, 0.8331485390663147], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.26.1)'}]\n",
      "[{'precision': [0.8543307185173035, 0.8487553596496582, 0.8374242782592773, 0.8458214402198792], 'recall': [0.8133378624916077, 0.830542802810669, 0.8214253187179565, 0.8297159671783447], 'f1': [0.8333304524421692, 0.8395503759384155, 0.8293476700782776, 0.8376913070678711], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.26.1)'}]\n",
      "[{'precision': [0.7285739183425903, 0.7294161319732666, 0.7682983875274658, 0.8260198831558228], 'recall': [0.7813332080841064, 0.7778809070587158, 0.7832638025283813, 0.807387113571167], 'f1': [0.754031777381897, 0.7528693675994873, 0.7757089138031006, 0.8165972232818604], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.26.1)'}]\n",
      "[{'precision': [0.7415476441383362, 0.7217028141021729, 0.7521421909332275, 0.8216397762298584], 'recall': [0.7863083481788635, 0.7871159911155701, 0.7690068483352661, 0.8093950152397156], 'f1': [0.7632723450660706, 0.7529914379119873, 0.760481059551239, 0.8154714107513428], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.26.1)'}]\n",
      "[{'precision': [0.8544648885726929, 0.8851975202560425, 0.845255434513092, 0.8789201974868774], 'recall': [0.8307133316993713, 0.8406747579574585, 0.8293774127960205, 0.8382112383842468], 'f1': [0.8424217104911804, 0.8623618483543396, 0.8372411727905273, 0.858083188533783], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.26.1)'}]\n",
      "[{'precision': [0.8793066740036011, 0.8793066740036011, 0.8315795063972473, 0.8624203205108643], 'recall': [0.8222671151161194, 0.8222671151161194, 0.7972549200057983, 0.8273137807846069], 'f1': [0.8498308658599854, 0.8498308658599854, 0.8140555024147034, 0.844502329826355], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.26.1)'}]\n",
      "[{'precision': [0.8992487192153931, 0.8908983469009399, 0.8915755748748779, 0.87859046459198], 'recall': [0.851578950881958, 0.848084568977356, 0.8522383570671082, 0.8372063040733337], 'f1': [0.8747648596763611, 0.8689643740653992, 0.8714632391929626, 0.8573993444442749], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.26.1)'}]\n",
      "[{'precision': [0.7254838347434998, 0.733814537525177, 0.739755392074585, 0.7923809289932251], 'recall': [0.7763985991477966, 0.7784170508384705, 0.7776917219161987, 0.7833358645439148], 'f1': [0.7500781416893005, 0.7554579973220825, 0.7582493424415588, 0.787832498550415], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.26.1)'}]\n",
      "[{'precision': [0.7222504615783691, 0.7222504615783691, 0.789600133895874, 0.7919732332229614], 'recall': [0.7862586379051208, 0.7862586379051208, 0.7847388982772827, 0.7939815521240234], 'f1': [0.7528965473175049, 0.7528965473175049, 0.7871620059013367, 0.7929761409759521], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.26.1)'}]\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(results[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
