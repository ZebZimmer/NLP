{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zebzi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#Import here\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "from getDocument import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer, BertModel\n",
    "\n",
    "model = BertModel.from_pretrained(\"allenai/scibert_scivocab_uncased\", is_decoder=True) #add_cross_attention\n",
    "tokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased', padding=True, truncation=True) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is an example of running Torch.nn.CosineSimilarity() on single sentences. It works somewhat as text1 and text2 have a higher similarity than 1/3 and 2/3.  Torch.nn.CosineSimilarity(dim=2) returns a vector of size [1, 14] which I then sum to get the similarity *THIS MAY NOT BE THE CORRECT WAY TO DO IT* but right now I don't have any better guidance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded_input1: tensor([[  102,   425,   482,   241,  4254,   147, 13780,   190,   111,   697,\n",
      "           205,   103,     0,     0,     0,     0,     0,     0]])\n",
      "encoded_input2: tensor([[ 102,  111, 4417,  241, 2992,  190,  425, 2019,  205,  103,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0]])\n",
      "encoded_input3: tensor([[  102,  9704,   121,   111,  7713,   165, 11035,   603,   256,  2505,\n",
      "           112,  8591,   205,   103,     0,     0,     0,     0]])\n",
      "decode of encoded_input1: [CLS] no one was allowed to interfere with the work. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "decode of encoded_input2: [CLS] the job was done with no interaction. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "decode of encoded_inpu32: [CLS] walking in the park is fun when it's warm. [SEP] [PAD] [PAD] [PAD] [PAD]\n",
      "Sentence 1 & 2 cosine similarity score: 0.6555863618850708\n",
      "Sentence 1 & 3 cosine similarity score: 0.5731323957443237\n",
      "Sentence 2 & 3 cosine similarity score: 0.5880245566368103\n"
     ]
    }
   ],
   "source": [
    "# Test of sentences to gather understanding of what cosine similarity is\n",
    "# https://pytorch.org/docs/stable/generated/torch.nn.CosineSimilarity.html\n",
    "\n",
    "text1 = \"No one was allowed to interfere with the work.\"\n",
    "text2 = \"The job was done with no interaction.\"\n",
    "text3 = \"Walking in the park is fun when it's warm.\"\n",
    "\n",
    "MAX_LENGTH = 18\n",
    "\n",
    "# Need to do the padding buisness because nn.CosineSimilarity requires tensors of equal length\n",
    "encoded_input1 = tokenizer(text1, return_tensors='pt', max_length=MAX_LENGTH, padding='max_length')\n",
    "encoded_input2 = tokenizer(text2, return_tensors='pt', max_length=MAX_LENGTH, padding='max_length')\n",
    "encoded_input3 = tokenizer(text3, return_tensors='pt', max_length=MAX_LENGTH, padding='max_length')\n",
    "\n",
    "output1 = model(**encoded_input1, return_dict=False)\n",
    "output2 = model(**encoded_input2, return_dict=False)\n",
    "output3 = model(**encoded_input3, return_dict=False)\n",
    "\n",
    "print(f\"encoded_input1: {encoded_input1['input_ids']}\")\n",
    "print(f\"encoded_input2: {encoded_input2['input_ids']}\")\n",
    "print(f\"encoded_input3: {encoded_input3['input_ids']}\")\n",
    "print(f\"decode of encoded_input1: {tokenizer.decode(encoded_input1['input_ids'][0])}\")\n",
    "print(f\"decode of encoded_input2: {tokenizer.decode(encoded_input2['input_ids'][0])}\")\n",
    "print(f\"decode of encoded_inpu32: {tokenizer.decode(encoded_input3['input_ids'][0])}\")\n",
    "\n",
    "# print(output1[0].shape)\n",
    "# print(output2[0].shape)\n",
    "# print(output3[0].shape)\n",
    "\n",
    "# cosine_scores = util.cos_sim(encoded_input1[\"input_ids\"], encoded_input2[\"input_ids\"])\n",
    "cosine_scores_DIM2 = torch.nn.CosineSimilarity(dim=2)\n",
    "# cosine_scores2 = torch.nn.CosineSimilarity(dim=1)\n",
    "# cosine_scores3 = torch.nn.CosineSimilarity(dim=2)\n",
    "\n",
    "score1 = cosine_scores_DIM2(output1[0], output2[0])\n",
    "score2 = cosine_scores_DIM2(output1[0], output3[0])\n",
    "score3 = cosine_scores_DIM2(output2[0], output3[0])\n",
    "# score4 = cosine_scores4(output1[0], output2[0])\n",
    "\n",
    "# print(f\"score1.shape: {score1.shape}\")\n",
    "# print(f\"score2.shape: {score2.shape}\")\n",
    "# print(f\"score3.shape: {score3.shape}\")\n",
    "\n",
    "# print(f\"sum of score3: {score3[0].sum().item()} and score3[0].size()[0]: {score3[0].size()[0]}\")\n",
    "print(f\"Sentence 1 & 2 cosine similarity score: {score1.mean().item()}\")\n",
    "print(f\"Sentence 1 & 3 cosine similarity score: {score2.mean().item()}\")\n",
    "print(f\"Sentence 2 & 3 cosine similarity score: {score3.mean().item()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With max_length=14 the scores are\n",
    "1/2 -> 0.582\n",
    "1/3 -> 0.475\n",
    "2/3 -> 0.490\n",
    "\n",
    "With max_length = 18 the scores are\n",
    "1/2 -> 0.656\n",
    "1/3 -> 0.573\n",
    "2/3 -> 0.588\n",
    "\n",
    "seeming to suggest that unnecessary padding will hurt the accuracy (as expected)\n",
    "_____________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decode of encoded_input1: [CLS] pre - trained text encoders have rapidly advanced the state of the art on many nlptasks. we focus on one such model, bert, and aim to quantify where linguistic information is captured within the network. we find that the model represents the steps of the traditional nlp pipeline in an interpretable and localizable way, and that the regions responsible for each step appear in the expected sequence : pos tagging, parsing, ner, semantic roles, then coreference. qualitative analysis reveals that the model can and often does adjust this pipeline dynamically, revising lower - level decisions on the basis of disambiguating information from higher - level representations. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "decode of encoded_input2: [CLS] we present a set of experiments to demonstrate that deep recurrent neural networks ( rnns ) learn internal representations that capture soft hierarchical notions of syntax from highly varied supervision. we consider four syntax tasks at different depths of the parse tree ; for each word, we predict its part of speech as well as the first ( parent ), second ( grandparent ) and third level ( great - grandparent ) constituent labels that appear above it. these predictions are made from representations produced at different depths in networks that are pretrained with one of four objectives : dependency parsing, semantic role labeling, machine translation, or language modeling. in every case, we find a correspondence between network depth and syntactic depth, suggesting that a soft syntactic hierarchy emerges. this effect is robust across all conditions, indicating that the models encode significant amounts of syntax even in the absence of an explicit syntactic training supervision. [SEP] [PAD]\n",
      "decode of encoded_inpu32: [CLS] language technology has become pervasive in everyday life. neural networks are a key component in this technology thanks to their ability to model large amounts of data. contrary to traditional systems, models based on deep neural networks ( a. k. a. deep learning ) can be trained in an end - to - end fashion on input - output pairs, such as a sentence in one language and its translation in another language, or a speech utterance and its transcription. the end - to - end training paradigm simplifies the engineering process while giving the model flexibility to optimize for the desired task. this, however, often comes at the expense of model interpretability : understanding the role of different parts of the deep neural network is difficult, and such models are sometimes perceived as'black - box ', hindering research efforts and limiting their utility to society. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "torch.Size([1, 175, 768])\n",
      "torch.Size([1, 175, 768])\n",
      "torch.Size([1, 175, 768])\n",
      "Sentence 1 & 2 cosine similarity score: 0.19116070866584778\n",
      "Sentence 1 & 3 cosine similarity score: 0.21889880299568176\n",
      "Sentence 2 & 3 cosine similarity score: 0.2011607140302658\n"
     ]
    }
   ],
   "source": [
    "paragraph1 = \"Pre-trained text encoders have rapidly advanced the state of the art on many NLPtasks. We focus on one such model, BERT, and aim to quantify where linguistic information is captured within the network. We find that the model represents the steps of the traditional NLP pipeline in an interpretable and localizable way, and that the regions responsible for each step appear in the expected sequence: POS tagging, parsing, NER, semantic roles, then coreference. Qualitative analysis reveals that the model can and often does adjust this pipeline dynamically, revising lower-level decisions on the basis of disambiguating information from higher-level representations.\"\n",
    "paragraph2 = \"We present a set of experiments to demonstrate that deep recurrent neural networks (RNNs) learn internal representations that capture soft hierarchical notions of syntax from highly varied supervision. We consider four syntax tasks at different depths of the parse tree; for each word, we predict its part of speech as well as the first (parent), second (grandparent) and third level (great-grandparent) constituent labels that appear above it. These predictions are made from representations produced at different depths in networks that are pretrained with one of four objectives: dependency parsing, semantic role labeling, machine translation, or language modeling. In every case, we find a correspondence between network depth and syntactic depth, suggesting that a soft syntactic hierarchy emerges. This effect is robust across all conditions, indicating that the models encode significant amounts of syntax even in the absence of an explicit syntactic training supervision.\"\n",
    "paragraph3 = \"Language  technology  has  become  pervasive  in  everyday  life. Neural  networks  are  a key component  in  this technology  thanks  to  their ability  to model  large  amounts  of data.  Contrary  to traditional  systems,  models  based  on  deep  neural  networks  (a.k.a. deep  learning) can  be  trained  in  an  end-to-end  fashion  on  input-output  pairs,  such  as  a  sentence  in  one language  and  its translation  in  another  language,  or  a  speech  utterance  and  its  transcription.  The end-to-end  training  paradigm simplifies  the  engineering process  while  giving  the model  flexibility  to  optimize  for  the  desired  task. This,  however,  often  comes  at  the  expense  of model  interpretability:  understanding  the role  of different parts  of the  deep  neural network  is  difficult,  and  such  models  are  sometimes  perceived  as  'black-box',  hindering research  efforts  and  limiting their  utility to  society.\"\n",
    "\n",
    "MAX_LENGTH = 175\n",
    "\n",
    "# Need to do the padding buisness because nn.CosineSimilarity requires tensors of equal length\n",
    "encoded_long_input1 = tokenizer(paragraph1, return_tensors='pt', max_length=MAX_LENGTH, padding='max_length')\n",
    "encoded_long_input2 = tokenizer(paragraph2, return_tensors='pt', max_length=MAX_LENGTH, padding='max_length')\n",
    "encoded_long_input3 = tokenizer(paragraph3, return_tensors='pt', max_length=MAX_LENGTH, padding='max_length')\n",
    "\n",
    "output_long_1 = model(**encoded_long_input1, return_dict=False)\n",
    "output_long_2 = model(**encoded_long_input2, return_dict=False)\n",
    "output_long_3 = model(**encoded_long_input3, return_dict=False)\n",
    "\n",
    "# print(f\"encoded_input1: {encoded_long_input1['input_ids']}\")\n",
    "# print(f\"encoded_input2: {encoded_long_input2['input_ids']}\")\n",
    "# print(f\"encoded_input3: {encoded_long_input3['input_ids']}\")\n",
    "print(f\"decode of encoded_input1: {tokenizer.decode(encoded_long_input1['input_ids'][0])}\")\n",
    "print(f\"decode of encoded_input2: {tokenizer.decode(encoded_long_input2['input_ids'][0])}\")\n",
    "print(f\"decode of encoded_inpu32: {tokenizer.decode(encoded_long_input3['input_ids'][0])}\")\n",
    "\n",
    "print(output_long_1[0].shape)\n",
    "print(output_long_2[0].shape)\n",
    "print(output_long_3[0].shape)\n",
    "\n",
    "cosine_scores_DIM2 = torch.nn.CosineSimilarity(dim=0)\n",
    "\n",
    "score_long_1 = cosine_scores_DIM2(output_long_1[0], output_long_2[0])\n",
    "score_long_2 = cosine_scores_DIM2(output_long_1[0], output_long_3[0])\n",
    "score_long_3 = cosine_scores_DIM2(output_long_2[0], output_long_3[0])\n",
    "\n",
    "print(f\"Sentence 1 & 2 cosine similarity score: {score_long_1.mean().item()}\")\n",
    "print(f\"Sentence 1 & 3 cosine similarity score: {score_long_2.mean().item()}\")\n",
    "print(f\"Sentence 2 & 3 cosine similarity score: {score_long_3.mean().item()}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text came from the BERT&ClassicNLP folder with 1: main paper, 2: source1, 3: source2\n",
    "using their abstracts, source 3 is supposed to be a semi-black sheep source as it's a thesis abstract and not a normal academic paper one\n",
    "\n",
    "With max_length=175 & dim=2:\n",
    "Sentence 1 & 2 cosine similarity score: 0.296\n",
    "Sentence 1 & 3 cosine similarity score: 0.364\n",
    "Sentence 2 & 3 cosine similarity score: 0.319\n",
    "\n",
    "With max_length=175 & dim=1:\n",
    "Sentence 1 & 2 cosine similarity score: 0.224\n",
    "Sentence 1 & 3 cosine similarity score: 0.248\n",
    "Sentence 2 & 3 cosine similarity score: 0.237\n",
    "\n",
    "With max_length=175 & dim=0:\n",
    "Sentence 1 & 2 cosine similarity score: 0.191\n",
    "Sentence 1 & 3 cosine similarity score: 0.219\n",
    "Sentence 2 & 3 cosine similarity score: 0.201\n",
    "\n",
    "______________________________________________________________________________________________________________________"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the tuple of SPECTER embeddings of a main paper and all of its references\n",
    "\n",
    "Both indices of the tuple are lists, 0 is embeddings and 1 is references\n",
    "\n",
    "The 0 index of each list is of the main paper\n",
    "\n",
    "The each embedding is just a list of numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BERT Rediscovers the Classical NLP Pipeline', 'What do you learn from context? Probing for sentence structure in contextualized word representations', 'Multi-Task Deep Neural Networks for Natural Language Understanding', 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', 'Language Models are Unsupervised Multitask Learners', 'Dissecting Contextual Word Embeddings: Architecture and Representation', 'Targeted Syntactic Evaluation of Language Models', 'Deep RNNs Encode Soft Hierarchical Syntax', 'What you can cram into a single $&!#* vector: Probing sentence embeddings for linguistic properties', 'Collecting Diverse Natural Language Inference Problems for Sentence Representation Evaluation', 'Deep Contextualized Word Representations', 'On internal language representations in deep learning: an analysis of machine translation and speech recognition', 'Improving Language Understanding by Generative Pre-Training', 'Attention is All you Need', 'Semantic Proto-Role Labeling', 'Does String-Based Neural MT Learn Source Syntax?', 'Semantic Proto-Roles', 'The Stanford CoreNLP Natural Language Processing Toolkit', 'A Gold Standard Dependency Corpus for English', 'Distributed Representations of Words and Phrases and their Compositionality', 'SemEval-2010 Task 8: Multi-Way Classification of Semantic Relations between Pairs of Nominals']\n"
     ]
    }
   ],
   "source": [
    "tupleOfResults = get_paper_and_references_embedding_and_titles_from_query(\"BERT Rediscovers the Classical NLP Pipeline\")\n",
    "\n",
    "embeddings = tupleOfResults[0]\n",
    "titles = tupleOfResults[1]\n",
    "\n",
    "print(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main paper title: BERT Rediscovers the Classical NLP Pipeline\n",
      "Main Paper with embeddings[1]: 0.751 and the reference paper: What do you learn from context? Probing for sentence structure in contextualized word representations\n",
      "Main Paper with embeddings[2]: 0.701 and the reference paper: Multi-Task Deep Neural Networks for Natural Language Understanding\n",
      "Main Paper with embeddings[3]: 0.735 and the reference paper: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\n",
      "Main Paper with embeddings[4]: 0.747 and the reference paper: Language Models are Unsupervised Multitask Learners\n",
      "Main Paper with embeddings[5]: 0.758 and the reference paper: Dissecting Contextual Word Embeddings: Architecture and Representation\n",
      "Main Paper with embeddings[6]: 0.670 and the reference paper: Targeted Syntactic Evaluation of Language Models\n",
      "Main Paper with embeddings[7]: 0.735 and the reference paper: Deep RNNs Encode Soft Hierarchical Syntax\n",
      "Main Paper with embeddings[8]: 0.684 and the reference paper: What you can cram into a single $&!#* vector: Probing sentence embeddings for linguistic properties\n",
      "Main Paper with embeddings[9]: 0.735 and the reference paper: Collecting Diverse Natural Language Inference Problems for Sentence Representation Evaluation\n",
      "Main Paper with embeddings[10]: 0.755 and the reference paper: Deep Contextualized Word Representations\n",
      "Main Paper with embeddings[11]: 0.646 and the reference paper: On internal language representations in deep learning: an analysis of machine translation and speech recognition\n",
      "Main Paper with embeddings[12]: 0.725 and the reference paper: Improving Language Understanding by Generative Pre-Training\n",
      "Main Paper with embeddings[13]: 0.659 and the reference paper: Attention is All you Need\n",
      "Main Paper with embeddings[14]: 0.695 and the reference paper: Semantic Proto-Role Labeling\n",
      "Main Paper with embeddings[15]: 0.653 and the reference paper: Does String-Based Neural MT Learn Source Syntax?\n",
      "Main Paper with embeddings[16]: 0.519 and the reference paper: Semantic Proto-Roles\n",
      "Main Paper with embeddings[17]: 0.688 and the reference paper: The Stanford CoreNLP Natural Language Processing Toolkit\n",
      "Main Paper with embeddings[18]: 0.617 and the reference paper: A Gold Standard Dependency Corpus for English\n",
      "Main Paper with embeddings[19]: 0.645 and the reference paper: Distributed Representations of Words and Phrases and their Compositionality\n",
      "Main Paper with embeddings[20]: 0.622 and the reference paper: SemEval-2010 Task 8: Multi-Way Classification of Semantic Relations between Pairs of Nominals\n"
     ]
    }
   ],
   "source": [
    "# Test the Cosine Similarity by first creating tensors and comparing the results\n",
    "newCosineSimilarity = torch.nn.CosineSimilarity(dim=0)\n",
    "mainPaper = torch.tensor(embeddings[0])\n",
    "print(f\"Main paper title: {titles[0]}\")\n",
    "for i in range(1, len(embeddings)):\n",
    "    referencePaper = torch.tensor(embeddings[i])\n",
    "    print(f\"Main Paper with embeddings[{i}]: {newCosineSimilarity(mainPaper, referencePaper):.3f} and the reference paper: {titles[i]}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With dim=0: (Which is the only allowed cosine similarity )\n",
    "Main Paper with embeddings[1]: 0.7507858276367188\n",
    "Main Paper with embeddings[2]: 0.7007443308830261\n",
    "Main Paper with embeddings[3]: 0.734921395778656\n",
    "Main Paper with embeddings[4]: 0.7470535039901733\n",
    "Main Paper with embeddings[5]: 0.7582955956459045\n",
    "Main Paper with embeddings[6]: 0.6695845127105713\n",
    "Main Paper with embeddings[7]: 0.7345977425575256\n",
    "Main Paper with embeddings[8]: 0.6843916773796082\n",
    "Main Paper with embeddings[9]: 0.7346400618553162\n",
    "Main Paper with embeddings[10]: 0.7553737759590149\n",
    "Main Paper with embeddings[11]: 0.6457442045211792\n",
    "Main Paper with embeddings[12]: 0.7245481610298157\n",
    "Main Paper with embeddings[13]: 0.6589515209197998\n",
    "Main Paper with embeddings[14]: 0.6954355835914612\n",
    "Main Paper with embeddings[15]: 0.6530366539955139\n",
    "Main Paper with embeddings[16]: 0.5189672708511353\n",
    "Main Paper with embeddings[17]: 0.6884840726852417\n",
    "Main Paper with embeddings[18]: 0.616716206073761\n",
    "Main Paper with embeddings[19]: 0.644980251789093\n",
    "Main Paper with embeddings[20]: 0.6220344305038452"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
